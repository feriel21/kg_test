[
    {
        "element_id": "13ac7334fc4af870dc38dcd80262ea0e",
        "text": "SOME METHODS FOR\nCLASSIFICATION AND ANALYSIS\nOF MULTIVARIATE OBSERVATIONS",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                77.69996643066406,
                88.11519622802734,
                391.4328918457031,
                158.67214965820312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "29bb2077481f5fda2fd2343a2581b232",
        "text": "J. MACQUEEN\nUNIVERSITY OF CALIFORNIA, Los ANGELES",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                138.3599853515625,
                166.53726196289062,
                330.9601135253906,
                194.63914489746094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "25faf24eca8823a408a6a60672bdf8dd",
        "text": "1. Introduction",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                61.5,
                203.1768798828125,
                128.4594268798828,
                218.91427612304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "243fd708f507a520d548659965c97060",
        "text": "The main purpose of this paper is to describe a process for partitioning an\nN-dimensional population into k sets on the basis of a sample. The process,\nwhich is called 'k-means,' appears to give partitions which are reasonably\nefficient in the sense of within-class variance. That is, if p is the probability mass\nfunction for the population, S = {S1, S2,\n- * *, Sk} is a partition of EN, and ui,\ni = 1, 2,\n*\n-\n, k, is the conditional mean of p over the set Si, then W2(S) =\nff=ISi\nf\nz - u42 dp(z) tends to be low for the partitions S generated by the\nmethod. We say 'tends to be low,' primarily because of intuitive considerations,\ncorroborated to some extent by mathematical analysis and practical computa-\ntional experience. Also, the k-means procedure is easily programmed and is\ncomputationally economical, so that it is feasible to process very large samples\non a digital computer. Possible applications include methods for similarity\ngrouping, nonlinear prediction, approximating multivariate distributions, and\nnonparametric tests for independence among several variables.\nIn addition to suggesting practical classification methods, the study of k-means\nhas proved to be theoretically interesting. The k-means concept represents a\ngeneralization of the ordinary sample mean, and one is naturally led to study the\npertinent asymptotic behavior, the object being to establish some sort of law of\nlarge numbers for the k-means. This problem is sufficiently interesting, in fact,\nfor us to devote a good portion of this paper to it. The k-means are defined in\nsection 2.1, and the main results which have been obtained on the asymptotic\nbehavior are given there. The rest of section 2 is devoted to the proofs of these\nresults. Section 3 describes several specific possible applications, and reports\nsome preliminary results from computer experiments conducted to explore the\npossibilities inherent in the k-means idea. The extension to general metric spaces\nis indicated briefly in section 4.\nThe original point of departure for the work described here was a series of\nproblems in optimal classification (MacQueen [9]) which represented special",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                61.139923095703125,
                220.990478515625,
                411.5985412597656,
                562.12451171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b0a9019977e5fc1935f0014af95274cb",
        "text": "This work was supported by the Western Management Science Institute under a grant from\nthe Ford Foundation, and by the Office of Naval Research under Contract No. 233(75), Task\nNo. 047-041.\n281",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.2999267578125,
                563.6876831054688,
                411.77874755859375,
                606.6354370117188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ad919a7716a8595c139e91922f92e3da",
        "text": "282\nFIFTH BERKELEY SYMPOSIUM: MAC QIEEN",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                97.62000274658203,
                52.9943962097168,
                372.5411071777344,
                66.78358459472656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ba9f80abf54c19059441ca60c02edff8",
        "text": "cases of the problem of optimal information structures as formulated by\nMarschak [11], [12]. (For an interesting treatment of a closely related problem,\nsee Blackwell [1].) In one instance the problem of finding optimal information\nstructures reduces to finding a partition S = {Sl, S2,\n* * *, Sk} of EN which will\nminimize W2(S) as defined above. In this special model, individual A observes a\nrandom point z E EN, which has a known distribution p, and communicates to\nindividual B what he has seen by transmitting one of k messages. Individual B\ninterprets the message by acting as if the observed point z is equal to a certain\npoint £ to be chosen according to the message received. There is a loss propor-\ntional to the squared error Iz - g12 resulting from this choice. The object is to\nminimize expected loss. The expected loss becomes W2(S), where the i-th message\nis transmitted if z E Si, since the best way for B to interpret the information is\nto choose the conditional mean of p on the set associated with the message\nreceived. The mean, of course, minimizes the squared error. Thus the problem\nis to locate a partition minimizing w2(S). This problem was also studied by\nFisher [5], who gives references to earlier related works.\nThe k-means process was originally devised in an attempt to find a feasible\nmethod of computing such an optimal partition. In general, the k-means pro-\ncedure will not converge to an optimal partition, although there are special cases\nwhere it will. Examples of both situations are given in section 2.3. So far as the\nauthor knows, there is no feasible, general method which always yields an optimal\npartition. Cox [2] has solved the problem explicitly for the normal distribution\nin one dimension, with k = 2, 3,\n-\n- *, 6, and a computational method for finite\nsamples in one dimension has been proposed by Fisher [5]. A closely related\nmethod for obtaining reasonably efficient 'similarity groups' has been described\nby Ward [15]. Also, a simple and elegant method which would appear to yield\npartitions with low within-class variance, was noticed by Edward Forgy [7] and\nRobert Jennrich, independently of one another, and communicated to the writer\nsometime in 1963. This procedure does not appear to be known to workers in\ntaxonomy and grouping, and is therefore described in section 3. For a thorough\nconsideration of the biological taxonomy problem and a discussion of a variety\nof related classification methods, the reader is referred to the interesting book\nby Sokal and Sneath [14]. (See Note added in proof of this paper.)\nSebestyen [13] has described a procedure called \"adaptive sample set con-\nstruction,\" which involves the use of what amounts to the k-means process.\nThis is the earliest explicit use of the process with which the author is familiar.\nAlthough arrived at in ignorance of Sebestyen's work, the suggestions we make\nin sections 3.1, 3.2, and 3.3, are anticipated in Sebestyen's monograph.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                97.260498046875,
                69.41116333007812,
                445.97998046875,
                533.1974487304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "394f27ddbf91a3dc98a5bc8215ef3bba",
        "text": "2. K-means; asymptotic behavior",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                98.0406494140625,
                547.1246337890625,
                246.47821044921875,
                564.5310668945312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "99a4a2fe3f430a7a10d185deaa0d6ff5",
        "text": "2.1. Preliminaries.\nLet zi, Z2,\n-\n-\n- be a random sequence of points (vectors) in\nEN, each point being selected independently of the preceding ones using a fixed\nprobability measure p. Thus P[zi e A] = p(A) and P [zn+l e A Izi, Z2,\n*\n-\n, Zn] =",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                97.98065185546875,
                564.2758178710938,
                445.4407958984375,
                606.8129272460938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "58daf2ffe644e915ee29e3df5a830ed7",
        "text": "MULTIVARIATE OBSERVATIONS\n283",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                161.5800018310547,
                52.972782135009766,
                406.38055419921875,
                65.59825897216797
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2f7932821a8b49bd4f70e6b0476bd251",
        "text": "p(A), n = 1, 2, *-- , for A any measurable set in EN. Relative to a given\nk-tuple X = (X1, X2,\n*\n, Xk), xi E EN, i = 1, 2,\n* * *, k, we define a minimum\ndistance partition S(x) = {SI(X), S2(X),\n*\n, Sk(X)} of EN, by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                58.920013427734375,
                68.44356536865234,
                406.9209899902344,
                110.52130126953125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dd589534b091231484a54d60706477e4",
        "text": "(2.1)\nSI(x) = TI(x), S2(x) = T2(x)S1(x), *-,",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                59.520050048828125,
                106.66593170166016,
                275.2207336425781,
                126.96475982666016
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "80ca35f307520155a22603776eef68f8",
        "text": "Sk(X) = Tk(X)S(X)S2(X)\n*.*.* Sk-l(X),\nwhere",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                59.520050048828125,
                124.07877349853516,
                261.8402404785156,
                151.14923095703125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dd8c399f9298579a7b5d73a21404e85a",
        "text": "(2.2)\nTi(x) = { E:EEN, I|-xil < 1 -xjl, j = 1, 2,\n,k}.\nThe set Si(x) contains the points in EN nearest to xi, with tied points being as-\nsigned arbitrarily to the set of lower index. Note that with this convention con-\ncerning tied points,\nif xi = xj and i < j then Sj(x) = 0. Sample k-means\n=\n2\nk\n* *\nxx),\neE EN, i = 1,\n*\n, k, with associated integer weights",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                59.580047607421875,
                147.091552734375,
                406.74072265625,
                219.4224853515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f4612a10fbf5e3f860294dae25bdd7fd",
        "text": "l\n\"2\nk\n**W8)are now defined as follows: x1 = Zi,\n= 1,i = 1, 2,\n, k, and",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                74.45996856689453,
                215.11679077148438,
                406.7408752441406,
                232.27609252929688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e2b5d4a00dd29c9484d20858c30a5f16",
        "text": "for n =\n1, 2,\n*\n, if Zk+n e Stxi\n= (Wt4t + Zn+k)/(Wl\n+ 1), Wt+l - Wt + 1,\nand xj\"+' = x4n, wi +1 = win for j $ i, where Sn = {Sn, Sn,\n* *Sk} is the mini-\nmum distance partition relative to x\".\nStated informally, the k-means procedure consists of simply starting with k\ngroups each of which consists of a single random point, and thereafter adding\neach new point to the group whose mean the new point is nearest. After a point",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                59.400054931640625,
                227.19082641601562,
                407.09954833984375,
                303.789306640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9d19d3cb5cb9ebf4e20f5e09d150b3ca",
        "text": "is added to a group, the mean of that group is adjusted in order to take account of\nthe new point. Thus at each stage the k-means are, in fact, the means of the groups\nthey represent (hence the term k-means).\nIn studying the asymptotic behavior of the k-means, we make the convenient\nassumptions, (i) p is absolutely continuous with respect to Lebesgue measure\non EN, and (ii) p(R) = 1 for a closed and bounded convex set R C EN, and\np(A) > 0 for every open set A C R. For a given k-tuple x = (xl, x2,\n,k)-\nsuch an entity being referred to hereafter as a k-point-let",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                59.160064697265625,
                298.9176025390625,
                407.7610168457031,
                400.44927978515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9a9298890889b4b9ca3f199fe466c524",
        "text": "k\nW(X) = ii js IZ- Xi2 dp(z),",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                168.6000518798828,
                401.5307922363281,
                297.8389892578125,
                422.3979797363281
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c9f1463710992fff5c936843a6c09622",
        "text": "(2.3)k\nV(X) = i.\nIz|-ui(x)12 dp(z),",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                61.02009582519531,
                421.80877685546875,
                303.7812805175781,
                451.15386962890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d0588eb3a853abcdca88494d564af476",
        "text": "where S = {S1, S2,\n*\n, Sk} is the minimum distance partition relative to x, and\nui(x) =\nsiz dp(z)/p(Si)\nor ui(x) = xi, according\nto whether p(Si) > 0\nor\np(Si) = 0. If xi = ui(x), i = 1, 2,\n*\n, k we say the k-point x is unbiased.\nThe principal result is as follows.\nTHEOREM 1.\nThe sequence of random variables W(xl), W(x2), *-- converges",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                59.880035400390625,
                454.7451477050781,
                407.8200378417969,
                521.0209350585938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "da6b0dd64829365f91a2f2d44dd2d49b",
        "text": "a.s. and W. = lim\",. W(xn) is a.s. equal to V(x) for some x in the class of k-points\nX = (Xl X222,\n*\n, Xk) which are unbiased, and have the property that xi # xj if i $ j.\nIn lieu of a satisfactory strong law of large numbers for k-means, we obtain the\nfollowing theorem.\nTHEOREM 2.\nLet Un' = U,(Xn) and pn = p(S,(xn)); then",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                59.9400634765625,
                514.872802734375,
                407.8215026855469,
                583.8568725585938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "830ad6fe19d071c08b06c4bf94b6b955",
        "text": "(2.4)\nE (\nEptixt - u )/m-O\nas\nm-> oo.\nn=l\ni=l\nII\na.s.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                61.7401123046875,
                581.1519165039062,
                332.70111083984375,
                607.9110717773438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "069bf00abee196422c73fc8283e751b5",
        "text": "284\nFIFTH BERKELEY SYMPOSIUM: MAC QUEEN",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                93.18000030517578,
                58.53998565673828,
                368.7599792480469,
                71.47826385498047
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ebeca9e179660a7ea48eeaf070c1ce4d",
        "text": "2.2. Proofs.\nThe system of k-points forms a complete metric space if the dis-\ntance p(x, y) between the k-points x = (xl, x2,\n* * *, Xk) and y = (YI, Y2,\n* * *, yk) '",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                94.07998657226562,
                74.19717407226562,
                443.1601257324219,
                104.52469635009766
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7a536b1a09d0e305e40e32cfc198d1e7",
        "text": "is defined by p(x, y) = Sk- 1xi - yil. We designate this space by M and inter-\npret continuity, limits, convergence, neighborhoods, and so on, in the usual way\nwith respect to the metric topology of M. Of course, every bounded sequence of\nk-points contains a convergent subsequence.\nCertain difficulties encountered in the proof of theorem 1 are caused by the\npossibility of the limit of a convergent sequence of k-points having some of its\nconstituent points equal to each other. With the end in view of circumventing\nthese difficulties, suppose that for a given k-point x = (X1, X2,\n*\n- *, Xk), xi e R,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                94.139892578125,
                97.7259292602539,
                444.17767333984375,
                200.6526336669922
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "44883bc9c46b77fa43a49d1d733e68a3",
        "text": "i = 1, 2,\n* *, k, we have xi = xj for a certain pair i, j, i < j, and xi = xi 5= xm\nfor m 5$ i, j. The points xi and xj being distinct in this way, and considering\nassumption (ii), we necessarily have p(Si(x)) > 0, for Si(x) certainly contains\nan open subset of R. The convention concerning tied points means p(Sj(x)) = 0.\nNow if {yn} = {(yl, y2,\n*\n, yk)}\nis a sequence of k-points satisfying y' e R,\nand yi $ YJ if i $ j, n = 1, 2,\n*\n, and the sequence yn approached x, then y?\nand yj approach xi = x;, and hence each other; they also approach the boundaries\nof S,(yn) and Sj(yn) in the vicinity of xi. The conditional means u1(yn) and uj(yn),\nhowever, must remain in the interior of the sets S,(yn) and Sj(yn) respectively,\nand thus tend to become separated from the corresponding points y' and yJ. In\nfact, for each sufficiently large n, the distance of U,(yn) from the boundary of\nSi(yn) or the distance of u,(yn) from the boundary of Sj(yn), will exceed a certain\npositive number. For as n tends to infinity, p(Si(yn)) + p(Sj(yn)) will approach\np(Si(x)) > 0-a simple continuity argument based on the absolute continuity\nof p will establish this-and for each sufficiently large n, at least one of the proba-\nbilities p(S1(yn)) or p(Sj(yn)) will be positive by a definite amount, say 6. But in\nview of the boundedness of R, a convex set of p measure at least a > 0 cannot\nhave its conditional mean arbitrarily near its boundary. This line of reasoning,\nwhich extends immediately to the case where some three or more members of",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                94.55987548828125,
                193.41957092285156,
                444.2406005859375,
                432.1428527832031
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8655c48dbaebbb5486ef00d136255477",
        "text": "(xI, x2,\n* * *, Xk) are equal, gives us the following lemma.\nLEMMA 1.\nLet x = (X1, X2,\n*\n, Xk) be the limit of a convergent sequence of\nk-points {yl} = {(yi, Y2, *.**, yk)} satisfying y? E R, y' F6 yj if i $ j, n = 1, 2,\nIf xi = xj for some i $ j, then lim infn 1t p(Si(yn))|yt-Uj(yn)\n> 0.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.29983520507812,
                426.89117431640625,
                444.360107421875,
                482.41156005859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b20bc005bfff4cc8f6d3da66a79087d0",
        "text": "Hence, if limn~g ,\np(Si(y\")) Iyk -u,(y8) = 0, each member of the k-tuple",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.4798583984375,
                470.578369140625,
                443.6406555175781,
                497.7580871582031
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "564ebf172008028bd40efdcb04fe9436",
        "text": "(X1, X2,\n-\n- *\nXk) is distinct from the others.\nWe remark that if each member of the k-tuple x = (xI, x2,\n*\n, Xk) is distinct\nfrom the others, then 7r(y) = (p(SI(y)), p(S2(y)), *-\n, p(Sk(y))), regarded as a\nmapping of M onto Ek, is continuous at x-this follows directly from the absolute\ncontinuity of p. Similarly, u(y) = (u1(y), u2(y),\n* * *, uk(y)) regarded as a map-\nping from M onto M is continuous at x-because of the absolute continuity of p\nand the boundness of R (finiteness of f z dp(z) would do). Putting this remark\ntogether with lemma 1, we get lemma 2.\nLEMMA 2.\nLet x = (X1, X2, ...*, Xk) be the limit of a convergent sequence of\nk-points {yn} = {(yl, y2,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.23989868164062,
                486.4164123535156,
                445.26025390625,
                613.7233276367188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7c65352abcc48d6bb3a7345e4725e54d",
        "text": "- *\n, yk)} satisfying yt c R, y? $ y7 if i $ j, n = 1, 2,",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                211.43997192382812,
                592.6135864257812,
                443.9992370605469,
                613.5150146484375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "83ede76e02bbba7b2d191e95c8538ac1",
        "text": "MULTIVARIATE OBSERVATIONS\n285",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                166.44000244140625,
                52.196372985839844,
                411.5989990234375,
                67.55907440185547
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d25f5919972f36fd1a32b9fc35cfa6f8",
        "text": "*--.If imn\nt=l p(S(y))y\n- Ui(yn)I = 0, then ,t=l p(Si(x))xi -tul(xn)[\n= 0 and each point xi in the k-tuple (X1, X2,\n*\n, Xk) is distinct from the others.\nLemmas 1 and 2 above are primarily technical in nature. The heart of the\nproofs of theorems 1 and 2 is the following application of martingale theory.\nLEMMA 3.\nLet t1, t2,\n*\n, and ti, t2y ...\n, be given sequences of random variables,\nand for each n = 1, 2,\n, let t,n and tn be measurable with respect to j. where\n#1 C /2 C\n... is a monotone increasing sequence of a-fields (belonging to the under-\nlying probability space). Suppose each of the following conditions holds a.s.:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.12005615234375,
                69.0459976196289,
                412.1409606933594,
                170.28924560546875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7fd8b63dd2d20913f9635f4f195758e7",
        "text": "(i)\nItni < K < oo, (ii) n> 0,\nn < -, (iii) E(tn+1113n) < tn+ t.. Then the se-\nquences of random variables\nt1, t2,\nand\nso, s1,\n2,\n--\n, where\nso = 0 and",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.4801025390625,
                163.95957946777344,
                411.7801208496094,
                194.687744140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2386a0929038a382f0b175bf18c56d96",
        "text": "Sn = Et= 1 (ti - E(t+±i!10), n = 1, 2,\n, both converge a.s.\nPROOF.\nLet yn = tn + sn_- so that the yn form a martingale sequence. Let c\nbe a positive number and consider the sequence {f9} obtained by stopping yn\n(see Doob\n[3],\np. 300)\nat the\nfirst n for which yn < -c. From\n(iii) we\nsee that yn > -\n=\ni - K, and since yn- Yn-1 > 2K, we have\nyn >\nmax (- Ft'=l\n- K, -(c + 2K)). The sequence {y} is a martingale, so that\nEyn = E91, n = 1, 2,\n*\n, and being bounded from below with Elgil . K, cer-\ntainly supn EI9nI < oo. The martingale theorem ([3], p. 319) shows 9n converges\na.s. But Yn = yn on the set Ac where\ni> -c-K, i = 1, 2,***, and\n(ii) implies P[A,] -+ 1 as c -oo . Thus {yn} converge a.s. This means Sn = yn+1\n-tn+i\nis\na.s. bounded. Using\n(iii) we can write -Sn = Et=l {\nA\nwhere Ai 2 0. But since Sn and E l {i are a.s. bounded, E, Ai converges a.s., Sn\nconverges a.s., and finally, so does tn. This completes the proof.\nTurning now to the proof of theorem 1, let (On stand for the sequence z1, Z2, ***\nZn-l+k, and let Al be the event [Zn+k e Sn]. Since Sn+1 is the minimum distance\npartition relative to xn+±, we have",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                62.760009765625,
                186.40316772460938,
                411.6605529785156,
                387.7813720703125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "58488f61258a08e0c743cf200dc04c63",
        "text": "(2.5)\nE[W(Xn+l)Ilwn] = E [f,\nXZ\n|- x+,I2 dp(Z)Icon]",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.660003662109375,
                389.08795166015625,
                322.506103515625,
                417.0569763183594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0e73e1e68f27e411a7cf2e393f9dc1ab",
        "text": "k f\nz -xn+y2 dp(z)lW1n]",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                202.62001037597656,
                420.0475158691406,
                318.00335693359375,
                446.69573974609375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9eba592079a7b8b03f284907b800c943",
        "text": "S<E [i",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                172.1400146484375,
                425.8951416015625,
                200.2790985107422,
                446.5587463378906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4ca478a79568f31bf91a51d909e3510a",
        "text": "k\n-k\n1",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                187.61997985839844,
                451.5859375,
                351.1801452636719,
                464.1861572265625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d6fd40907e64a3bf5b111c4934c892dc",
        "text": "= E\n- Xn+112 dp(z)IAn X\nP\nj=1 E Ji\nZ\n=Wj\ni",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                181.3199920654297,
                451.9183349609375,
                360.8399658203125,
                481.6394348144531
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7c71f79f2a82ebae7a2a378a071c8086",
        "text": "If Zn+k E 87, x' = xi for i Fd j. Thus we obtain",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                62.82000732421875,
                482.2775573730469,
                277.4990539550781,
                500.7511291503906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b20b70b789ff5a7537f761aeba6a8f10",
        "text": "(2.6)\nE[W(Xn+l),.n]\n< W(xn) - E (f XZ-n42 dp(z))p\n2",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.360015869140625,
                502.7419128417969,
                354.1800231933594,
                533.9154663085938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "eb2424e6dc7078bf6e6bafa151b13f42",
        "text": "+ E E [f|s\nIz- 4+112 dp(z)#A7, iWn] pj.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                180.54002380371094,
                535.0451049804688,
                359.46038818359375,
                561.6511840820312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "767080236f40f59f4713755bb8f34242",
        "text": "Several\napplications\nof the\nrelation\nfA Iz -X12 dp(z) = fA Iz - U12 dp(z) +\np(A)lx - u12, where JA (u - z) dp(z) = 0, enables us to write the last term in\n(2.6) as",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                62.400054931640625,
                562.1323852539062,
                410.5203552246094,
                603.94091796875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "df43be5f211702652d1ab8a67b9555ba",
        "text": "286\nFIFTH BERKELEY SYMPOSIUM: MAC QUEEN",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                94.9800033569336,
                51.45718002319336,
                368.7601623535156,
                67.56928253173828
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0a993af13caf346db45e2738047fdc36",
        "text": "k\n(2.7)\nW[f~,,~Iz - x1j2 dp(z) pi - (pi)2 xJ -U[2\n6=1\nSi",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                95.58001708984375,
                67.615234375,
                326.4598693847656,
                100.58882904052734
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a5c0f1fa731a15191e054898c8ab275f",
        "text": "+ (p7)2|XJ - u712(Wj/(Wn + 1))2 + fs\nIZ - Uj12 dp(z) pj /(wj + 1)21.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                141.00003051757812,
                97.66559600830078,
                442.0811462402344,
                123.80643463134766
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3e06f7055d8e517a95bc2d5e1a2388fd",
        "text": "Combining this with (2.6), we get",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                95.46002197265625,
                122.86360931396484,
                242.7604522705078,
                138.8459014892578
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c507cf2cc99659f359a000ae1d777782",
        "text": "k\n(2.8)\nE(W(xn+l) IWn] < W(xn) - E 4X -uYl2(p7)2(2wj + l)/(wj + 1)2\nj=1",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                95.8800048828125,
                140.830810546875,
                423.4801025390625,
                168.42013549804688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bdbc5a8b8f718af0109f4cfe9199aaee",
        "text": "k\n+\n7\n2*,j(pjn)2/(Wn + 1)2,\n6=1\nwhere anj = fS, IZ -U712 dp(z)/pj7.\nSince we are assuming p(R) = 1, certainly W(xn) is a.s. bounded, as is\nn,j.\nWe now show that",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                95.64004516601562,
                169.14434814453125,
                442.3807678222656,
                237.81414794921875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c3224b8da0b9e390901e364b31de7036",
        "text": "(2.9)\nE (pfn)2/(w7\" + 1)2\nn",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.90005493164062,
                235.23675537109375,
                308.8190612792969,
                260.0208435058594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1e205bf4346c2fe9097346d2bfda5824",
        "text": "converges a.s. for each j = 1, 2,\n* * *, k, thereby showing that",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.12008666992188,
                260.6299743652344,
                364.2612609863281,
                276.8741760253906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5c434fa3bf1609d0f86a0bbbf82e59d5",
        "text": "(2.10)\nE\n[o j(p7)2/(wn + 1)2])\nn\nj=1\nconverges\na.s. Then lemma 3 can be applied with tn= W(xn) and\n,n=\nEk1 2f,j(p7)2/(W7 + 1)2.\nIt suffices to consider the convergence of",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.54010772705078,
                275.5767822265625,
                441.9007873535156,
                345.84588623046875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a7a50a7604494548e467d09feb65b0e3",
        "text": "(2.11)\nE (pj)2/[Q3 + 1 + Wn)( 3 + 1 + W+1)]\nn>2\nwith A > 0, since this implies convergence of (2.9). Also, this is convenient, for\nE(I,lWn) = pj where I7 is the characteristic function of the event [zn+k E 87],\nand on noting that wj -+ 1 +\n,t= 1 j, an application of theorem 1 in [4],\np. 274, says that for any positive numbers a and j3,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.48016357421875,
                343.7615661621094,
                444.00067138671875,
                420.99420166015625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "29b43452620af16c734bfc1a2ea0db6e",
        "text": "(2.12)\nP\nd + 1 +wj +1 > 1 + ,pj-a a,\nvjf for all n = 1, 2,**I\n_L=l i=l-\n> 1-(1 + a/3)',",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                97.98008728027344,
                423.98919677734375,
                443.0996398925781,
                466.11187744140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "445c6ebb514c5a9864b4898fff5635b2",
        "text": "where vt = pJ - (p5)2 is the conditional variance of It given ci. We take a = 1,\nand thus with probability at least 1 - (1 + f)-I the series (2.11) is dominated by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                97.26010131835938,
                465.29876708984375,
                444.7200927734375,
                496.7809753417969
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "013910f3dcd3e169e578c6bdb31ef075",
        "text": "(2.13)\nE (p\")2/[(1 +\n(pj)2)2\n+ E (pf)2]\n(2.13)\nn>2\nIL\\+",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                99.06011962890625,
                505.5959777832031,
                370.2004699707031,
                523.0733642578125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d2aeab66d32109b17419e11afd19b43d",
        "text": "= E 4[J&+ E (pf))12 (1 +\nn (P)2)]'\nn>2L/-",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                162.4201202392578,
                517.6603393554688,
                388.979248046875,
                555.81298828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d2528e2b44f4552b6c6988f06c74f09e",
        "text": "which clearly converges.\nThe choice of\n3 being arbitrary, we have shown that (2.9) converges a.s.\nApplication of lemma 3 as indicated above proves W(xn) converges a.s.\nTo identify the limit W.O, note that with tn and\n,n taken as above, lemma 3",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                97.80013275146484,
                557.763916015625,
                445.4404296875,
                610.1062622070312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8da0e78d92748c2afef39410d2fe7e0a",
        "text": "MULTIVARIATE OBSERVATIONS\n287",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                166.44000244140625,
                51.24677276611328,
                411.6000061035156,
                63.88656997680664
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0d9896835c5c82d890b6854a6b0b8780",
        "text": "entails a.s. convergence of En[W(xn) - E[W(xn+')ico.]], and hence (2.8) implies\na.s. convergence of",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                64.26004028320312,
                63.36235809326172,
                411.420654296875,
                95.734130859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c31e1099e50eaabb0b172fcd3638ec00",
        "text": "(2.14)\nE (EXn - u. j2(pn)2(2W7 + l)/(w7 + 1)2)",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                65.10003662109375,
                98.61553192138672,
                330.9595031738281,
                120.61384582519531
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fb20e80a219e7abf0f315e9b812e5df0",
        "text": "Since (2.14) dominates ,n\nPlp7Xn - uj)/kn, the latter converges a.s.,\nand a little consideration makes it clear that",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                64.74005126953125,
                120.12754821777344,
                410.1595153808594,
                151.41412353515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dfe6418d6602b61091745e44fae9f4dc",
        "text": "k\nk\n(2.15)\nE pnlXn - ujll = E p(Sj(xn))IXJ - uj(xn)l\nj=1\nj=1",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                65.40005493164062,
                153.07073974609375,
                330.11785888671875,
                179.85293579101562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "130abad11c11e73fadc71454c81999f9",
        "text": "converges to zero on a subsequence {xn } and that this subsequence has itself a\nconvergent subsequence, say {xn'}. Let x = (x,, x2, ... ,)Xk) = limt,.\nxnv. Since\nW(X) = V(X) + Ek=, p(Sj(X))IXj-U(X)2 and in particular,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                64.50003051757812,
                179.28396606445312,
                411.0600280761719,
                223.12557983398438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a3f20b926342daab40f501e1c5b209a0",
        "text": "(2.16)\nW(x,) = V(xn) + E p(Sj(xn))Jxjn - u(xj7)12,\nj=1\nwe have only to show",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                64.19996643066406,
                222.150390625,
                344.0966491699219,
                265.9892883300781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d0a627fd4a336bb1ea6bc8e2406544eb",
        "text": "(a) limt,. W(xn') = WO, = W(x), and\n(b) limn+0 Y,=,I p(Sj(xnt))lX7' - u(x, )12 = 0 = Fl=I p(Sj(x)) xj - uj(x)I2.\nThen W(x) = V(x) and x is a.s. unbiased. (Obviously,\n_t=1 pilail = 0 if and\nonly if Y_it=, pi4aij2 = 0, where pi > 0.)\nWe show that (a) is true by establishing the continuity of W(x). We have",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.95989990234375,
                264.83880615234375,
                410.880615234375,
                334.20928955078125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f856c16473ecc2af806f12d9a073fe27",
        "text": "k\n(2.17)\nW(x) < E ] Z(z - XjI2 dp(z)",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                64.73991394042969,
                337.7840270996094,
                260.9415588378906,
                360.1520080566406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e199d4dd7358dce3e6a976eb3192fa45",
        "text": "k\nk\n-E\nSi(\nz-yJ2 ± E [p(Sj(y))IXj yj- 2\nj=1\n~~~j=1\n+ 2lxj -yjl Ls(y) !z - xjl dp(z)],",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                151.85989379882812,
                365.89080810546875,
                347.0400390625,
                418.8627014160156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f3224385fa7cac679ab397f755fce0a3",
        "text": "with the last inequality following easily from the triangle inequality. Thus\nW(x) < W(y) + o(p(x, y)), and similarly, W(y) < W(x) + o(p(x, y)).\nTo establish (b), lemma 2 can be applied with {yn} and {x-} identified, for",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.95989990234375,
                417.9908142089844,
                410.8794860839844,
                458.6810302734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c89c74c85ddfac6c3104a1e27f9f7595",
        "text": "a.s. xi' $4\nXj' for i 5- j, n = 1, 2,\n*\n- -\n. It remains to remark that lemma 2 also\nimplies a.s. xi #d xj for i $- j. The proof of theorem 1 is complete.\nTheorem 2 follows from the a.s. convergence of 57n(Ft=1 p Xtn- ui)/nk\nupon applying an elementary result (c.f. Halmos [8], theorem C, p. 203), which\nsays that if E an/n converges,\n_i. 1 ai/n --\n0.\n2.3. Remarks.\nIn a number of cases covered by theorem 1, all the unbiased\nk-points have the same value of W. In this situation, theorem\n1 implies\nEk= 1 p1\ni\nUt\nconverges a.s. to zero. An example is provided by the uniform\ndistribution over a disk in E2. If k = 2, the unbiased k-point (xl, x2) with xi $d x2\nconsist of the family of points xi and x2 opposite one another on a diameter, and\nat a certain fixed distance from the center of the disk. (There is one unbiased\nk-point with xi = x2, both xl and x2 being at the center of the disk in this case.)",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.059906005859375,
                453.6983947753906,
                410.69879150390625,
                603.2244262695312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b7ad19cebecbd478fbc78930b7d38a5c",
        "text": "288\nFIFTH BERKELEY SYMPOSIUM: MAC QUEEN",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.42000579833984,
                53.34040069580078,
                370.7411804199219,
                66.2051010131836
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b1b79fc495098821dc232fc4cde42825",
        "text": "The k-means thus converge to some such relative position, but theorem 1 does\nnot quite permit us to eliminate the interesting possibility that the two means\noscillate slowly but indefinitely around the center.\nTheorem 1 provides for a.s. convergence of EF=1 pilxi - uil to zero in a\nslightly broader class of situations. This is where the unbiased k-points x =",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.41995239257812,
                68.2520523071289,
                444.35992431640625,
                134.50100708007812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d4616c6425776a21f4b2d5ee48d99b61",
        "text": "(xI, X2,\n* * *, Xk) with xi $ xi for i #= j, are all stable in the sense that for each\nsuch x, W(y) 2 W(x) (and hence V(y) > V(x)) for all y in a neighborhood of x.\nIn this case, each such x falls in one of finitely many equivalence classes such\nthat W is constant on each class. This is illustrated by the above example, where\nthere is only a single equivalence class. If each of the equivalence classes contains\nonly a single point, theorem 1 implies a.s. convergence of xn to one of those points.\nThere are unbiased k-points which are not stable. Take a distribution on E2\nwhich has sharp peaks of probability at each corner of a square, and is symmetric\nabout both diagonals. With k = 2, the two constituent points can by sym-\nmetrically located on a diagonal so that the boundary of the associated minimum\ndistance partition coincides with the other diagonal. With some adjustment, such\na k-point can be made to be unbiased, and if the probability is sufficiently con-\ncentrated at the corners of the square, any small movement of the two points off\nthe diagonal in opposite directions, results in a decrease in W(x). It seems likely\nthat the k-means cannot converge to such a configuration.\nFor an example where the k-means converge with positive probability to a\npoint x for which V(x) is not a minimum, take equal probabilities at the corner\npoints of a rectangle which is just slightly longer on one side than the other.\nNumber with 1 the corner points, and 2 at the end points of one of the short\nedges, and 3 and 4, at the end points of the other short edge, with 1 opposite 3\non the long edge. Take k = 2. If the first four points fall at the corner points",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                96.89996337890625,
                127.29441833496094,
                445.6211242675781,
                391.5375061035156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9244acafa37786dc66efcdbb3ad43161",
        "text": "1, 2, 3, 4 in that order, the two means at this stage are directly opposite one\nanother at the middle of the long edges. New points falling at 1 and 3 will always\nbe nearer the first mean, and points falling at 2 and 4 will always be nearer the\nsecond mean, unless one of the means has an excursion too near one of the corner\npoints. By the strong law of large numbers there is positive probability this will\nnot happen, and hence with positive probability the two means will converge to\nthe midpoints of the long edges. The corresponding partition clearly does not\nhave minimum within-class variance.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                97.68002319335938,
                386.9375,
                446.5200500488281,
                489.189208984375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f3b1c9fb1d806078aad0367c7aecb09c",
        "text": "3. Applications",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                98.28011322021484,
                503.4906921386719,
                164.4623260498047,
                519.97265625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "54c7dce879f5e1dfb8c123ea01b42845",
        "text": "3.1. Similarity grouping: coarsening and refining.\nPerhaps the most obvious\napplication of the k-means process is to the problem of \"similarity grouping\" or\n\"clustering.\" The point of view taken in this application is not to find some\nunique, definitive grouping, but rather to simply aid the investigator in obtaining\nqualitative and quantitative understanding of large amounts of N-dimensional\ndata by providing him with reasonably good similarity groups. The method\nshould be used in close interaction with theory and intuition. Consequently, the",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                98.04010009765625,
                520.677490234375,
                447.4164733886719,
                609.4326171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6d0f733000b49801be71803b102801be",
        "text": "MULTIVARIATE OBSERVATIONS\n289",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                166.0800018310547,
                55.03437423706055,
                411.77935791015625,
                68.52357482910156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c48b9c817a8d7c83cc97124a4466afc9",
        "text": "computer program actually prepared for this purpose involved several modifi-\ncations of the k-means process, modifications which appear to be helpful in this\nsense.\nFirst, the program involves two parameters: C for 'coarsening,' and R for\n'refinement.' The program starts with a user specified value of k, and takes the\nfirst k points in the sample as initial means. The k-means process is started, each\nsubsequent sample point being assigned to the nearest mean, the new mean\ncomputed, and so on, except that after each new point is added, and for the\ninitial means as well, the program determines the pair of means which are\nnearest to each other among all pairs. If the distance between the members of\nthis pair is less than C, they are averaged together, using their respective weights,\nto form a single mean. The nearest pair is again determined, their separation\ncompared with C, and so on, until all the means are separated by an amount of\nC or more. Thus k is reduced and the partition defined by the means is coarsened.\nIn addition, as each new point is processed and its distance from the nearest of\nthe current means determined, this distance is compared with R. If the new\npoint is found to be further than Rf from the nearest mean, it is left by\nitself as the seed point for a new mean. Thus k is increased and the partition is\nrefined. Ordinarily we take C < R. After the entire sample is processed in this\nway, the program goes back and reclassifies all the points on the basis of nearness\nto the final means. The points thus associated with each mean constitutes the\nfinal grouping. The program prints out the points in each group along with as\nmany as 18 characters of identifying information which may be supplied with\neach point. The distance of each point from its nearest mean, the distances\nbetween the means, the average for each group, of the squared distance of the\npoints in each group from their respective defining means, and the grand average\nof these quantities over groups, are all printed out. The latter quantity, which is\nnot quite the within-group variance, is called the within-class variation for pur-\nposes of the discussion below. If requested, the program determines frequencies\nof occurrence within each group of the values of discrete variables associated\nwith each point. Up to twelve variables, with ten values for each variable, can\nbe supplied. This makes it convenient to determine whether or not the groups\nfinally obtained are related to other attributes of interest. (Copies of this\nexperimental program are available from the author on request.)\nThe program has been applied with some success to several samples of real\ndata, including a sample of five dimensional observations on the students'\nenvironment in 70 U.S. colleges, a sample of twenty semantic differential\nmeasurements on each of 360 common words, a sample of fifteen dimensional\nobservations on 760 documents, and a sample of fifteen physiological observations\non each of 560 human subjects. While analysis of this data is still continuing,\nand will be reported in detail elsewhere, the meaningfulness of the groups ob-\ntained is suggested by their obvious pertinence to other identifiable properties\nof the objects classified. This was apparent on inspection. For example, one\ngroup of colleges contained Reed, Swarthmore, Antioch, Oberlin, and Bryn",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                62.0400390625,
                71.82394409179688,
                411.0610046386719,
                606.1959838867188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a34f1f8e3ee3c7cc3ca888eea1f30974",
        "text": "290\nFIFTH BERKELEY SYMPOSIUM: MAC QUEEN",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                98.04000091552734,
                55.61396408081055,
                372.9595031738281,
                68.72846221923828
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bd9ad87c930f0dbee7f142ae8e43315c",
        "text": "Mawr. Another group contained the Universities of Michigan, Minnesota,\nArkansas, and Illinois, Cornell, Georgia Tech, and Purdue. Selecting at random\na half-dozen words from several groups obtained from the semantic differential\ndata, we find in one group the words calm, dusky, lake, peace, sleep, and white;\nin another group the words beggar, deformed, frigid, lagging, low; and in another\ngroup the words statue, sunlight, time, trees, truthful, wise.\nWhen the sample points are rearranged in a new random order, there is some\nvariation in the grouping which is obtained. However, this has not appeared to\nbe a serious concern. In fact, when there are well separated clusters, as de-\ntermined by inspection of the between-mean distances in relation to the within-\nclass variation, repeated runs give virtually identical groupings. Minor shifts are\ndue to the unavoidable difficulty that some points are located between clusters.\nA degree of stability with respect to the random order in which the points are\nprocessed is also indicated by a tendency for the within-class variation to be\nsimilar in repeated runs. Thus when a sample of 250 points in five dimensions\nwith k = 18, was run three times, each time with the points in a different random\norder, the within-class variation (see above) changed over the three runs by at\nmost 7%. A certain amount of stability is to be expected simply because the\nwithin-class variation is the mean of k dependent random variables having the\nproperty that when one goes up the others generally go down. We can reasonably\nexpect the within-class stability to generally increase with k and the sample size.\nActually, it will usually be desirable to make several runs, with different values\nof C and R, and possibly adding, deleting, or rescaling variables, and so on, in\nan effort to understand the basic structure of the data. Thus any instabilities due\nto random ordering of the sample will be quickly noted. Being able to make\nnumerous classifications cheaply and thereby look at the data from a variety of\ndifferent perspectives is an important advantage.\nAnother general feature of the k-means procedure which is to be expected on\nintuitive grounds, and has been noted in practice, is a tendency for the means and\nthe associated partition to avoid having the extreme of only one or two points in\na set. In fact, there is an appreciable tendency for the frequency to be evenly\nsplit over groups. If there are a few relatively large groups, these tend to have\nrelatively low within-class variation, as would be expected from a tendency for\nthe procedure to approximate minimum variance partitions.\nRunning times of the above program on the IBM 7094 vary with C, R, the\nnumber of dimensions, and the number of points. A conservative estimate for\n20-dimensional data, with C and R set so that k stays in the vicinity of 20, is one\nminute for two hundred sample points. Most of this computation time results\nfrom the coarsening and refining procedure and the auxiliary features. A limited\namount of experience indicates the undecorated k-means procedure with k = 20\nwill process five hundred points in 20 dimensions in something like 10 seconds.\n3.2. Relevant classifications.\nSuppose it is desired to develop a classification\nscheme on the basis of a sample, so that knowing the classification of a new point,\nit will be possible to predict a given dependent variable. The values of the de-",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                97.79998779296875,
                72.4035873413086,
                449.3995056152344,
                612.9544067382812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bc61143520b8255eaedfc043157b2372",
        "text": "MULTIVARIATE OBSERVATIONS\n291",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                164.27999877929688,
                51.10761642456055,
                408.3592834472656,
                64.47191619873047
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c1e73984bc122565ae3b6791696c2c5d",
        "text": "pendent variable are known for the sample. One way to do this, closely related\nto a procedure proposed by Fix and Hodges [6], is illustrated by the following\ncomputer experiment. A sample of 250 four-dimensional random vectors was\nprepared, with the values on each dimension being independently and uniformly\ndistributed on the integers 1 through 10. Two of the dimensions were then arbi-\ntrarily selected, and if with respect to these two dimensions a point was either\n'high' (above 5) on both or 'low' (5 or less) on both, it was called an A; otherwise,\nit was called a B. This gave 121 A's and 129 B's which were related to the selected\ndimensions in a strongly interactive fashion. The k-means with k = 8 were then\nobtained for the A's and B's separately. Finally, using the resulting 16 (four-\ndimensional) means, a prediction, A or B, was made for each of a new sample of\n250 points on the basis of whether or not each point was nearest to an A mean\nor a B mean. These predictions turned out to be 87% correct.\nAs this example shows, the method is potentially capable of taking advantage\nof a highly nonlinear relationship. Also, the method has something to recommend\nit from the point of view of simplicity, and can easily be applied in many di-\nmensions and to more than two-valued dependent variables.\n3.3. Approximating a general distribution.\nSuppose it is desired to approxi-\nmate a distribution on the basis of a sample of points. First the sample points are\nprocessed using the k-means concept or some other method which gives a\nminimum distance partition of the sample points. The approximation, involving\na familiar technique, consists of simply fitting a joint normal distribution to the\npoints in each group, and taking as the approximation the probability combi-\nnation of these distributions, with the probabilities proportional to the number\nof points in each group.\nHaving fitted a mixture of normals in this way, it is computationally easy (on a\ncomputer) to do two types of analysis. One is predicting unknown coordinates of\na new point given the remaining coordinates. This may be done by using the\nregression function determined on the assumption that the fitted mixture is the\ntrue distribution. Another possible application is a kind of nonlinear discriminant\nanalysis. A mixture of k normals is fitted in the above fashion to two samples\nrepresenting two given different populations; one can then easily compute the\nappropriate likelihood ratios for deciding to which population a new point\nbelongs. This method avoids certain difficulties encountered in ordinary discrimi-\nnant analysis, such as when the two populations are each composed of several\ndistinct subgroups, but with some of the subgroups from one population actually\nbetween the subgroups of the other. Typically in this situation, one or several\nof the k-means will be centered in each of the subgroups-provided k is large\nenough-and the fitted normals then provide a reasonable approximation to the\nmixture.\nTo illustrate the application of the regression technique, consider the artificial\nsample of four-dimensional A's and B's described in the preceding section. On\na fifth dimension, the A's were arbitrarily given a value of 10, and the B's a value\nof 0. The k-means procedure with k = 16 was used to partition the combined",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                62.279998779296875,
                67.48360443115234,
                410.1622009277344,
                601.3462524414062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9d04737003bf63161530be7b7b89bb8a",
        "text": "292\nFIFTH BERKELEY SYMPOSIUM: MAC QUEEN",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                103.62000274658203,
                52.1607780456543,
                377.58062744140625,
                65.40017700195312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2bb1c69898747ab8e93077f2bbc8690b",
        "text": "sample of 250 five-dimensional points. Then the mixture of 16 normal distri-\nbutions was determined as described above for this sample. The second sample\nof 250 points was prepared similarly, and predictions were made for the fifth\ndimension on the basis of the original four. The standard error of estimate on\nthe new sample was 2.8. If, in terms of the original A-B classification, we had\ncalled a point on A if the predicted value exceeded 5, and a B otherwise, 96%\nof the designations would have been correct on the new sample. The mean of the\npredictions for the A's was 10.3, and for B's, 1.3.\nConsidering the rather complex and highly nonlinear relationship involved\nin the above sample, it is doubtful that any conventional technique would do\nas well. In the few instances which were tested, the method performed nearly\nas well as linear regression on normally distributed samples, provided k was not\ntoo large. This is not surprising inasmuch as with k = 1 the method is linear\nregression. In determining the choice of k, one procedure is to increase k as long\nas the error of estimate drops. Since this will probably result in \"over fitting\"\nthe sample, a cross validation group is essential.\n3.4. A scrambled dimension test for independence among several variables.\nAs a\ngeneral test for relationship among variables in a sample of N-dimensional ob-\nservations, we propose proceeding as follows. First, the sample points are grouped\ninto a minimum distance partition using k-means, and the within-class variance",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                103.61996459960938,
                68.32357025146484,
                452.40234375,
                314.94915771484375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6b868d3903344dce166cee295c910e3d",
        "text": "is determined. Then the relation among the variables is destroyed by randomly\nassociating the values in each dimension; that is, a sample is prepared in which\nthe variables are unrelated, but which has exactly the same marginal distri-\nbutions as the original sample. A minimum distance partition and the associated\nwithin-class variance is now determined for this sample. Intuition and inspection\nof a few obvious examples suggest that on the average this \"scrambling\" will\ntend to increase the within-class variance, more or less regardless of whatever\ntype of relation might have existed among the variables, and thus comparison\nof the two variances would reveal whether or not any such relation existed.\nTo illustrate this method, a sample of 150 points was prepared in which points\nwere distributed uniformly outside a square 60 units on a side, but inside a\nsurrounding square 100 units on a side. This gave a sample which involves\nessentially a zero correlation coefficient, and yet a substantial degree of relation-\nship which could not be detected by any conventional quantitative technique\nknown to the author (although it could be detected immediately by visual\ninspection). The above procedure was carried out using k-means with k = 12.\nAs was expected, the variance after scrambling was increased by a factor of 1.6.\nThe within-class variances were not only larger in the scrambled data, but were\napparently more variable. This procedure was also applied\nto\nthe\nfive-\ndimensional sample described in the preceding section. Using k = 6, 12, and 18,\nthe within-class variance increased after scrambling by the factors 1.40, 1.55,\nand 1.39, respectively.\nA statistical test for nonindependence can be constructed by simply repeating\nthe scrambling and partitioning a number of times, thus obtaining empirically a",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                104.52008056640625,
                310.3302917480469,
                453.71978759765625,
                604.9427490234375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4f2304d33d8630d335065a3b5cbf011f",
        "text": "MULTIVARIATE OBSERVATIONS\n293",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                159.77999877929688,
                53.58802795410156,
                404.760009765625,
                67.39490509033203
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b9ebeda2d0e2f52a736d09799af7b4f7",
        "text": "sample from the conditional distribution of the within-class variance unlder the\nhypothesis that the variables are unrelated and given the marginal values of the\nsample. Under the hypothesis of independence, the unscrambled variance should\nhave the same (conditional) distribution as the scrambled variance. In fact, the\nrank of the unscrambled variance in this empirical distribution should be\nequally likely to take on any of the possible values 1, 2,\n* * *, n + 1, where n is\nthe number of scrambled samples taken, regardless of the marginal distributions\nin the underlying population. Thus the rank can be used in a nonparametric\ntest of the hypothesis of independence. For example, if the unscrambled variance",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                57.540008544921875,
                70.27041625976562,
                405.300537109375,
                182.82928466796875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e3cec822cfd50a0eda54b7a8da8e13d3",
        "text": "is the lowest in 19 values of the scrambled variance, we can reject the hypothesis\nof independence with a Type I error of .05.\nA computer program was not available to do the scrambling, and its being\ninconvenient to set up large numbers of scrambled samples using punched cards,\nfurther testing of this method was not undertaken. It is estimated, however, that\nan efficient computer program would easily permit this test to be applied at, say,\nthe .01 level, on large samples in many dimensions.\nThe power of this procedure remains to be seen. On the encouraging side is\nthe related conjecture, that for fixed marginal distributions, the within-class\nvariance for the optimal partition as defined in section 1 is maximal when the\njoint distribution is actually the product of the marginals. If this is true (and",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                57.839996337890625,
                178.23721313476562,
                405.6593017578125,
                315.8243103027344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9f65a39a664f2ff3d01efa0096673438",
        "text": "it seems likely that it is, at least for a large class of reasonable distributions),\nthen we reason that since the k-means process tends to give a good partition, this\ndifference will be preserved in the scrambled and unscrambled variances, par-\nticularly for large samples. Variation in the within-class variance due to the\nrandom order in which the points are processed, can be reduced by taking\nseveral random orders, and averaging their result. If this is done for the\nscrambled runs as well, the Type I error is preserved, while the power is increased\nsomewhat.\n3.5. Distance-based classification trees. The k-means concept provides a number\nof simple procedures for developing lexigraphic classification systems (filing\nsystems, index systems, and so on) for a large sample of points. To illustrate, we\ndescribe briefly a procedure which results in the within-group variance of each of\nthe groups at the most refined level of classification being no more than a specified\nnumber, say R. The sample k-means are first determined with a selected value of\nk, for example, k = 2. If the variance of any of the groups of points nearest to\nthese means is less than R, these groups are not subclassified further. The remain-\ning groups are each processed in the same way, that is, k-means are determined\nfor each of them, and then for the points nearest each of these, and so on. This",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                58.079833984375,
                310.7171630859375,
                406.3791809082031,
                532.212646484375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e1d848816949c86ad57e87dc34f2cd24",
        "text": "is continued until only groups with within-group variance less than R remain.\nThus for each mean at the first level, there is associated several means at the\nsecond level, and so on. Once the means at each level are determined from the\nsample in this fashion, the classification of a new point is defined by the rule:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                58.079833984375,
                527.0771484375,
                406.018798828125,
                580.45263671875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6edfd3bb0f7a1874fbd7b56185d7bb14",
        "text": "first, see which one of the first level k-means the point is nearest; then see which\none of the second-level k-means associated with that mean the point is nearest,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                58.2598876953125,
                575.0439453125,
                406.379638671875,
                603.9443359375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "10087a070c3565f46b3ab628746c7c3c",
        "text": "294\nFIFTH BERKELEY SYMPOSIUM: MAC QUEEN",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                98.76000213623047,
                51.28120422363281,
                372.71923828125,
                64.89530181884766
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d332cb83bf976800cebe31b5f7b77a1e",
        "text": "and so on; finally the point is assigned to a group which in the determining sample\nhas variance no more than R.\nThis procedure has some promising features. First, the amount of computation\nrequired to determine the index is approximately linear in the sample size and\nthe number of levels. The procedure can be implemented easily on the computer.\nAt each stage during the construction of the classification tree, we are employing\na powerful heuristic, which consists simply of putting points which are near to\neach other in the same group. Each of the means at each level is a fair repre-\nsentation of its group, and can be used for certain other purposes, for instance,\nto compare other properties of the points as a function of their classification.\n3.6. A two-step improvement procedure.\nThe method of obtaining partitions\nwith low within-class variance which was suggested by Forgy and Jennrich (see\nsection 1.1) works as follows. Starting with an arbitrary partition into k sets, the\nmeans of the points in each set are first computed. Then a new partition of the\npoints is formed by the rule of putting the points into groups of the basis of\nnearness to the first set of means. The average squared distance of the points in\nthe new partition from the first set of means (that is, from their nearest means)",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                98.03997802734375,
                68.0031967163086,
                446.88031005859375,
                277.6010437011719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "830e69064bcc5fe1b1807629fbc376c9",
        "text": "is obviously less than the within-class variance of the first partition. But the\naverage within-class variance of the new partition is even lower, for the variance\nof the squared distance of the points in each group from their respective means,\nand the mean, of course, is that point which minimizes the average squared\ndistance from itself. Thus the new partition has lower variance. Computationally,\nthe two steps of the method are (1) compute the means of the points in each\nset in the initial partition and (2) reclassify the points on the basis of nearness\nto these means, thus forming a new partition. This can be iterated and the series\nof the partitions thus produced have decreasing within-class variances and will\nconverge in a finite number of steps.\nFor a given sample, one cycle of this method requires about as much compu-\ntation as the k-means. The final partition obtained will depend on the initial\npartition, much as the partition produced by k-means will depend on random\nvariation in the order in which the points are processed. Nevertheless, the\nprocedure has much to recommend it. By making repeated runs with different\ninitial starting points, it would seem likely that one would actually obtain the\nsample partition with minimum within-class variance.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                99.30001831054688,
                272.90325927734375,
                448.14019775390625,
                482.6878356933594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ca27034aee7d9ba75e25d875a6cd11e8",
        "text": "4. General metric spaces",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                100.55987548828125,
                497.5636291503906,
                211.38169860839844,
                513.6658935546875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0f2122bfd61d3254c07b861ea7de82e4",
        "text": "It may be something more than a mere mathematical exercise to attempt to\nextend the idea of k-means to general metric spaces. Metric spaces other than\nEuclidian ones do occur in practice. One prominent example is the space of\nbinary sequences of fixed length under Hamming distance.\nAn immediate difficulty in making such an extension is the notion of mean",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                100.91986083984375,
                513.9575805664062,
                449.038818359375,
                579.552734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "664cca3c77497ea529874bfa1328b57a",
        "text": "itself. The arithmetic operations defining the mean in Euclidian space may not\nbe available. However, with the communication problem of section 1 in mind,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                101.099853515625,
                574.3980102539062,
                449.04010009765625,
                603.2776489257812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "67a4a88ea97ea4b012af68045a2d0a22",
        "text": "MULTIVARIATE OBSERVATIONS\n295",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                165.72000122070312,
                51.73316192626953,
                411.0599670410156,
                64.15825653076172
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f7bd53cb8cbd1f94c4221f09ac3ad1e2",
        "text": "one thinks of the problem of representing a population by a point, the goal being\nto have low average error in some sense. Thus we are led to proceed rather\nnaturally as follows.\nLet M be a compact metric space with distance p, let 5Y be the o--algebra of\nsubsets of M, and let p be a probability measure on 5. For the measure p,\na centroid of order r > 0 is any point in the set er of points x* such that\nf pr(x*, z) dp(z) = inf. f pr(x, z) dp(z). The quantity f pr(x*, z) dp(z) is the r-th\nmoment of p. The compactness and the continuity of p guarantee that Ce\nis\nnonempty. For finite samples, sample centroids are defined analogously, each\npoint in the sample being treated as having measure 1/n where n is the sample\nsize; namely, for a sample of size n, the sample centroid is defined up to an equiva-\nlence class e\nwhich consists of all those points An such that F_-= I Pr(n\nZi) =\ninf. X?.. 1 pr(x, zi), where zl, Z2,\n. ..\n, Zn is the sample.\nNote that with M the real line, and p ordinary distance, r = 2 yields the\nordinary mean, and r = 1 yields the family of medians. As r tends to co, the\nelements of C,. will tend to have (in a manner which can easily be made precise)\nthe property that they are centers for a spherical covering of the space with\nminimal radius. In particular, on the line, the centroid will tend to the mid-range.\nAs r tends to zero, one obtains what may with some justification be called a mode,\nfor on a compact set, pr(x, y) is approximately 1 for small r, except where x and y\nare very near, so that minimizing f pr(x, y) dp(y) with respect to x, involves\nattempting to locate x so that there is a large amount of probability in its\nimmediate vicinity. (This relationship can also be made precise.)\nWe note that the optimum communication problem mentioned in section 1.1\nnow takes the following general form. Find a partition S = {Si, S2,\n* *\nSk}\nwhich minimizes w =\n_f-\nfsi pl(xi, y) dp(y), where x4 is the centroid of order\nr with respect to the (conditional) distribution on Si. If there is any mass in\na set Si nearer to xj than to xi, j s! i, then w can be reduced by modifying\nSi and Si so as to reassign this mass to Sj. It follows that in minimizing w we\ncan restrict attention to partitions which are minimum distance partitions,\nanalogous to those defined in section 2, that is, partitions of the form S(x) =\n{S.(x), S2(x),\n* * *, Sk(x)} where x = (xI, x2,\n* * *, Xk) is a k-tuple of points in M,\nand Si(x) is a set of points at least as near xi (in terms of p) as to xj if j # i.\nIn keeping with the terminology of section 2, we may say that a k-tuple, or\n\"k-point,\" x = (xl, x2,\n* * *, Xk) is unbiased if xi, i = 1, 2,\n*\n, k, belongs to the\nclass of points which are centroids within Si(x).\nIt is now clear how to extend the concept of k-means to metric spaces; the\nnotion of centroid replaces the more special concept of mean. The\nfirst\n'k-centroid'\n(xl, xl,\n* * *, xk) consists of the first k points in the sample, and\nthereafter as each new point is considered, the nearest of the centroids is de-\ntermined. The new point is assigned to the corresponding group and the centroid\nof that group modified accordingly, and so on.\nIt would seem reasonable to suppose that the obvious extension of theorem 1\nwould hold. That is, under independent sampling, Ek\n1 fs.(Xt)pt(z4) dp(z) will",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.29998779296875,
                67.11715698242188,
                411.4200134277344,
                603.452880859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b5c4e36bc945dc8334ec5fff2904b8b3",
        "text": "296\nFIFTH BERKELEY SYMPOSIUM: MAC QUEEN",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                99.66000366210938,
                49.00117492675781,
                374.3393249511719,
                62.615272521972656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e1c11dda1aa8e26e094f052da036876f",
        "text": "converge a.s., and the convergent subsequences of the sequence of sample\nk-centroids will have their limits in the class of unbiased k-points. This is true,\nat any rate, for k = 1 and r = 1, for\nif\nzl, Z2, .--\n, zn are independent,\nEs=i p(Zz, y)/n is the mean of independent, identically distributed random\nvariables, which because M is compact, are uniformly bounded in y. It fol-\nlows (cf. Parzen [13]) that Et\n, p(Zi, y)/n converges a.s. to f p(z, y) dp(z)\nuniformly in y. By definition of the sample centroid, we have E_t 1 p(Zi, x*)/n >\nEJ-1 p(Zi, In)/n; hence, fp(z, x*) dp(z) 2 lim sup Et' I p(Zi, 1n)/n with probability\n1.\nOn\nthe\nother\nhand,\nfrom\nthe\ntriangle\ninequality,\nF,. i p(zi, y)/n <\nF-J-\np(zi, 1.)/n + p(£, Y). Using this inequality on a convergent subsequence\nIlni -tn2, ...\n, chosen so that",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                98.760009765625,
                65.81759643554688,
                448.07696533203125,
                204.14181518554688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6d5a1060c9bede4a895f3704c5ad51ed",
        "text": "ne\n~~~~~~~n\n(4.1)\nlim E p(zi, I.)/n, = lim inf L p(Zi, &n)/n,\nt-*O ij1\ni=1\nwe see that with probability 1,",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                99.41998291015625,
                200.47560119628906,
                367.56103515625,
                246.55271911621094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "68a4aaa140abc0eb6502aa396614c943",
        "text": "(4.2)\nf p(z, x*) dp(z) < f p(z, y) dp(z) < lim inf L_ p(zi, x4)/n,",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                100.01998901367188,
                239.54843139648438,
                399.53912353515625,
                276.8371276855469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3f5c281a3ca998fa1ca58b09c7c3a53b",
        "text": "where y = limi,\n£.\nProvided the necessary computations can be accomplished, the methods\nsuggested in sections 3.1, 3.2, 3.4, 3.5, and 3.6 can all be extended to general\nmetric spaces in a quite straightforward fashion.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                99.23995971679688,
                272.3796081542969,
                448.49835205078125,
                326.3775939941406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4f35374a06a322e2c3aba95ff1edc510",
        "text": "ACKNOWLEDGMENTS",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                215.5800018310547,
                340.4172058105469,
                331.7961120605469,
                356.529296875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "362ccd6782e597274fcce7aa8bb2404c",
        "text": "The author is especially indebted to Tom Ferguson, Edward Forgy, and\nRobert Jennrich, for many valuable discussions of the problems to which the\nabove results pertain. Richard Tenney and Sonya Baumstein provided the\nessential programming support, for which the author is very grateful. Computing\nfacilities were provided by the Western Data Processing Center.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                99.42007446289062,
                358.0508117675781,
                448.6798095703125,
                422.8678283691406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "18b9dbcf5c2426923ddba35a1a053dfa",
        "text": "K\nK\nK\nK\nK",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                204.78009033203125,
                432.8192138671875,
                337.8601379394531,
                457.1098327636719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "71a3e5b4452e1c312c644ce15b5b0fe1",
        "text": "Note added in proof.\nThe author recently learned that C. S. Wallace of the\nUniversity of Sidney and G. H. Ball of the Stanford Research Institute have\nindependently used this method as a part of a more complex procedure. Ball has\ndescribed his method, and reviewed earlier literature, in the interesting paper\n\"Data analysis in the social sciences: What about the details?\", Proceedings of\nthe Fall Joint Computer Conference, Washington, D.C., Spartan Books, 1965.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                99.4801025390625,
                454.8780212402344,
                450.11962890625,
                532.0394897460938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b34fd97c3996758e150009939769bb88",
        "text": "REFERENCES",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                244.56005859375,
                546.8544311523438,
                305.1026611328125,
                560.3436279296875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "aa904ea54b00423c435dfc8878698dc8",
        "text": "[1] DAVID BLACKWELL, \"Comparison of experiments,\" Proceedings of the Second Berkeley\nSymposium on Mathematical Statistics and Probability, Berkeley and Los Angeles, Uni-\nversity of California Press, 1951, pp. 93-102.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                106.26005554199219,
                564.1216430664062,
                449.7608642578125,
                598.3836059570312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fa1f7bb795b4fad16177cd43444c5839",
        "text": "MULTIVARIATE OBSERVATIONS\n297",
        "type": "Title",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                165.72000122070312,
                56.909568786621094,
                411.4198913574219,
                72.14736938476562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "53cf5985ffd8132148f6780d049e6bdc",
        "text": "[2] D. R. Cox, \"Note on grouping,\" J. Amer. Statist. Assoc., Vol. 52 (1957), pp. 543-547.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                68.82000732421875,
                77.03400421142578,
                411.4191589355469,
                90.90357971191406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c412ac0e327f04b91675aea4dc8c1e56",
        "text": "[3] J. L. DOOB, Stochastic Processes, New York, Wiley, 1953.\n[4] L. E. DUBINS and L. J. SAVAGE, \"A Tchebycheff-like inequality for stochastic processes,\"\nProc. Nat. Acad. Sci. U.S.A., Vol. 53 (1965), pp. 274-275.\n[5] W. D. FISHER, \"On grouping for maximum homogeneity,\" J. Amer. Statist. Assoc., Vol.\n53 (1958), pp. 789-798.\n[6] EVELYN Fix and J. L. HODGES, JR., \"Discriminatory Analysis,\" USAF Project Report,\nSchool of Aviation Medicine, Project Number 21-49-004, No. 4 (1951).\n[7] EDWARD FORGY, \"Cluster analysis of multivariate data: efficiency vs. interpretability of\nclassifications,\" abstract, Biometrics, Vol. 21 (1965), p. 768.\n[8] PAUL R. HALMOS, Measure Theory, New York, Van Nostrand, 1950.\n[9] J. MACQUEEN, \"The classification problem,\" Western Management Science Institute\nWorking Paper No. 5, 1962.\n[10]\n, \"On convergence of k-means and partitions with minimum average variance,\"\nabstract, Ann. Math. Statist., Vol. 36 (1965), p. 1084.\n[11] JACOB MARSCHAK, \"Towards an economic theory of organization and information,\"\nDecision Processes, edited by R. M. Thrall, C. H. Coombs, and R. C. Davis, New York,\nWiley, 1954.\n[12]\n,\"Remarks on the economics of information,\" Proceedings of the scientific program\nfollowing the dedication of the Western Data Processing Center, University of California,\nLos Angeles, January 29-30, 1959.\n[13] EMANUEL PARZEN, \"On uniform convergence of families of sequences of random vari-\nables,\" Univ. California Publ. Statist., Vol. 2, No. 2 (1954), pp. 23-54.\n[14] GEORGE S. SEBESTYEN, Decision Making Process in Pattern Recognition, New York,\nMacmillan, 1962.\n[15] ROBERT R. SOKAL and PETER H. SNEATH, Principles of Numerical Taxonomy, San\nFrancisco, Freeman, 1963.\n[16] JOE WARD, \"Hierarchical grouping to optimize an objective function,\" J. Amer. Statist.\nAssoc., Vol. 58 (1963), pp. 236-244.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1200512992.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                64.44001770019531,
                87.58760070800781,
                412.14068603515625,
                372.9388122558594
            ],
            "is_full_width": false
        }
    }
]