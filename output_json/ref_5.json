[
    {
        "element_id": "963191467055d264670e7987465e5c2a",
        "text": "Mythos",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                95.48049926757812,
                79.50788879394531,
                294.77752685546875,
                166.056884765625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4d8bdc291831d933d8acec21a6b52d05",
        "text": "of Model \nInterpretability",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                174.13119506835938,
                144.44839477539062,
                364.27520751953125,
                210.45639038085938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6ef4cff25cdd38342a548206a964ea3e",
        "text": "S",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                292.3639831542969,
                180.96051025390625,
                403.5884704589844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0bc70f03da456edc24e6f227712480c6",
        "text": "upervised machine-learning models boast \nremarkable predictive capabilities. But can you \ntrust your model? Will it work in deployment? \nWhat else can it tell you about the world? \nModels should be not only good, but also \ninterpretable, yet the task of interpretation appears \nunderspecified. The academic literature has provided \ndiverse and sometimes non-overlapping motivations for \ninterpretability and has offered myriad techniques for \nrendering interpretable models. Despite this ambiguity, \nmany authors proclaim their models to be interpretable \naxiomatically, absent further argument. Problematically, \nit is not clear what common properties unite these \ntechniques.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                311.323974609375,
                381.5806884765625,
                506.2284851074219
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4a8acc6d1b15cd5c548f393d7e755349",
        "text": "The",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                214.8804931640625,
                71.54840087890625,
                260.9284973144531,
                110.55640411376953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ebce0db462dcb8d8a60b70ed380ffe92",
        "text": "In machine learning, the \nconcept of interpretability is \nboth important and slippery.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                220.24020385742188,
                218.27699279785156,
                352.1201171875,
                262.73699951171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5b25f41826dc0d86933171ccc1f4d8f6",
        "text": "ZACHARY C. LIPTON",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                220.24029541015625,
                281.8074951171875,
                301.0242614746094,
                293.0574951171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "acb220a11451f445a88220a6bab9f7a5",
        "text": "papers addressing interpretability, finding them to be \ndiverse and occasionally discordant. Then, it explores \nmodel properties and techniques thought to confer \ninterpretability, identifying transparency to humans and \npost hoc explanations as competing concepts. Throughout, \nthe feasibility and desirability of different notions of \ninterpretability are discussed. The article questions the \noft-made assertions that linear models are interpretable \nand that deep neural networks are not.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                384.7558898925781,
                221.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "63cb970e23fa3406338483ae841def1f",
        "text": "INTRODUCTION \nUntil recently, humans had a monopoly on agency in \nsociety. If you applied for a job, loan, or bail, a human \ndecided your fate. If you went to the hospital, a human \nwould attempt to categorize your malady and recommend \ntreatment. For consequential decisions such as these, you \nmight demand an explanation from the decision-making \nagent.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                237.1925048828125,
                382.2467346191406,
                347.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2cf4e0ed2d923352be22e6822ea2092f",
        "text": "If your loan application is denied, for example, you \nmight want to understand the agent’s reasoning in a bid to \nstrengthen your next application. If the decision was based \non a flawed premise, you might contest this premise in the \nhope of overturning the decision. In the hospital, a doctor’s \nexplanation might educate you about your condition.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                348.7239990234375,
                384.0208740234375,
                431.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a07300f930f2e0e992788a77d917ecb7",
        "text": "In societal contexts, the reasons for a decision often \nmatter. For example, intentionally causing death (murder) \nvs. unintentionally (manslaughter) are distinct crimes. \nSimilarly, a hiring decision being based (directly or \nindirectly) on a protected characteristic such as race has a \nbearing on its legality. However, today’s predictive models \nare not capable of reasoning at all.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                432.7239990234375,
                382.83453369140625,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3542a1788bc0d3199bc95726db8ef385",
        "text": "Over the past 20 years, rapid progress in ML (machine \nlearning) has led to the deployment of automatic decision \nprocesses. Most ML-based decision making in practical use \nworks in the following way: the ML algorithm is trained \nto take some input and predict the corresponding output. \nFor example, given a set of attributes characterizing a \nfinancial transaction, an ML algorithm can predict the \nlong-term return on investment. Given images from a CT \nscan, the algorithm can assign a probability that the scan \ndepicts a cancerous tumor. The ML algorithm takes in a \nlarge corpus of (input, output) pairs, and outputs a model \nthat can predict the output corresponding to a previously \nunseen input. Formally, researchers call this problem \nsetting supervised learning. Then, to automate decisions \nfully, one feeds the model’s output into some decision rule. \nFor example, spam filters programmatically discard emails \npredicted to be spam with a level of confidence exceeding \nsome threshold.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                384.94512939453125,
                347.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "76197ddce6636239d932def4c4454a0c",
        "text": "I",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                27.069499969482422,
                202.58937072753906,
                42.34989929199219,
                286.66644287109375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2febbe4bfc9c7c81fe66e7341929cf8d",
        "text": "n the academic \nliterature, \nfew authors \narticulate \nprecisely what \ninterpretability \nmeans or \nprecisely how \ntheir proposed \nsolution is \nuseful.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                30.0,
                217.5598907470703,
                119.34878540039062,
                338.3598937988281
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b162a2c0586f62502fa1264efad9b5ad",
        "text": "Thus, ML-based systems do not know why a given input \nshould receive some label, only that certain inputs are \ncorrelated with that label. For example, shown a dataset \nin which the only orange objects are basketballs, an image \nclassifier might learn to classify all orange objects as \nbasketballs.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                348.7239990234375,
                380.9970703125,
                431.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d4470826cde2899240b70ef7e6e84193",
        "text": "This model would achieve high accuracy even on held \nout images, despite failing to grasp the difference that \nactually makes a difference.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                432.7239990234375,
                370.9470520019531,
                473.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bb7f30cb5d6f79d15141758344be0af3",
        "text": "As ML penetrates critical areas such as medicine, the \ncriminal justice system, and financial markets, the inability \nof humans to understand these models seems problematic. \nSome suggest model interpretability as a remedy, but in the",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                474.7239990234375,
                386.0999450683594,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "be2657c113baf744d3615c3c311a0ed0",
        "text": "academic literature, few authors articulate precisely what \ninterpretability means or precisely how their proposed \nsolution is useful.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                382.9184265136719,
                137.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4d4ed12cb18402cd3383d7290866adb1",
        "text": "Despite the lack of a definition, a growing body of \nliterature proposes purportedly interpretable algorithms. \nFrom this, you might conclude that either: (1) the definition \nof interpretability is universally agreed upon, but no \none has bothered to set it in writing; or (2) the term \ninterpretability is ill-defined, and, thus, claims regarding \ninterpretability of various models exhibit a quasi-scientific \ncharacter. An investigation of the literature suggests \nthe latter. Both the objectives and methods put forth in \nthe literature investigating interpretability are diverse, \nsuggesting that interpretability is not a monolithic concept \nbut several distinct ideas that must be disentangled before \nany progress can be made.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                138.7239990234375,
                384.9765319824219,
                319.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "83d5d57d0ad73152374e4637d7cf8e28",
        "text": "This article focuses on supervised learning rather than \nother ML paradigms such as reinforcement learning and \ninteractive learning. This scope derives from the current \nprimacy of supervised learning in real-world applications \nand an interest in the common claim that linear models are \ninterpretable while deep neural networks are not.15 To gain \nconceptual clarity, consider these refining questions: What \nis interpretability? Why is it important?",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                320.7239990234375,
                384.8084716796875,
                431.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e1d2e0a36bb50e9b4c967de5171ec004",
        "text": "Let’s address the second question first (expanded in \nthe section, “Desiderata of Interpretability Research”). \nMany authors have proposed interpretability as a \nmeans to engender trust.9,24 This leads to a similarly \nvexing epistemological question: What is trust? Does it \nrefer to faith that a model will perform well? Or does \ninterpretability simply mean a low-level mechanistic",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                432.7239990234375,
                367.1159362792969,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "11f94b3b1e8b32969756d81cca19ede3",
        "text": "understanding of models? Is trust defined subjectively?",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                368.5545654296875,
                109.62850189208984
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "209b63d2edb6ff94ccc1995be038684c",
        "text": "Other authors suggest that an interpretable model is \ndesirable because it might help uncover causal structure \nin observational data.1 The legal notion of a right to \nexplanation offers yet another lens on interpretability. One \ngoal of interpretability might simply be to get more useful \ninformation from the model.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                110.7239990234375,
                385.3726806640625,
                193.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cbf1bacf4893d8fc30cab5617ed081d5",
        "text": "While the discussed desiderata, or objectives of \ninterpretability, are diverse, they typically speak to \nsituations where standard ML problem formulations, \ne.g. maximizing accuracy on a set of hold-out data for \nwhich the training data is perfectly representative, \nare imperfectly matched to the complex real-life \ntasks they are meant to solve. Consider medical \nresearch with longitudinal data. The real goal may be \nto discover potentially causal associations that can \nguide interventions, as with smoking and cancer.29 The \noptimization objective for most supervised learning \nmodels, however, is simply to minimize error, a feat that \nmight be achieved in a purely correlative fashion.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                194.7239990234375,
                369.3210754394531,
                375.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "91d19b39ae5a5a1ef6d71acaf2a9f441",
        "text": "Another example of such a mismatch is that available \ntraining data imperfectly represents the likely deployment \nenvironment. Real environments often have changing \ndynamics. Imagine training a product recommender for \nan online store, where new products are periodically \nintroduced, and customer preferences can change over \ntime. In more extreme cases, actions from an ML-based \nsystem may alter the environment, invalidating future \npredictions.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                376.7239990234375,
                383.6851501464844,
                501.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "66887772a2e65de75b64808d9e8fe9fc",
        "text": "render them interpretable (expanded in the section, \n“Properties of Interpretable Models”). Some papers equate \ninterpretability with understandability or intelligibility,16 \n(i.e., you can grasp how the models work). In these papers, \nunderstandable models are sometimes called transparent, \nwhile incomprehensible models are called black boxes. \nBut what constitutes transparency? You might look to the \nalgorithm itself: Will it converge? Does it produce a unique \nsolution? Or you might look to its parameters: Do you \nunderstand what each represents? Alternatively, you could \nconsider the model’s complexity: Is it simple enough to be \nexamined all at once by a human?",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                385.60650634765625,
                263.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c315735dcee55801f2e3e35a4ed67511",
        "text": "Other work has investigated so-called post hoc \ninterpretations. These interpretations might explain \npredictions without elucidating the mechanisms by which \nmodels work. Examples include the verbal explanations \nproduced by people or the saliency maps used to analyze \ndeep neural networks. Thus, human decisions might admit \npost hoc interpretability despite the black-box nature \nof human brains, revealing a contradiction between two \npopular notions of interpretability.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                264.7239990234375,
                379.0650634765625,
                389.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6b2649609cbefcc4e6d28b90fb550349",
        "text": "DESIDERATA OF INTERPRETABILITY RESEARCH \nThis section spells out the various desiderata of \ninterpretability research. The demand for interpretability \narises when a mismatch occurs between the formal \nobjectives of supervised learning (test set predictive \nperformance) and the real-world costs in a deployment \nsetting.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                405.1925048828125,
                378.0150146484375,
                501.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7c444eb0f2b474a388e43be8c94226be",
        "text": "demand interpretability, you might infer the existence \nof objectives that cannot be captured in this fashion. \nConsider that most common evaluation metrics for \nsupervised learning require only predictions, together with \nground truth, to produce a score. So, the very desire for \nan interpretation suggests that sometimes predictions \nalone and metrics calculated on them do not suffice to \ncharacterize the model. You should then ask, What are \nthese other objectives and under what circumstances are \nthey sought?",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                385.0394592285156,
                235.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dcab816e2d34d3f2bdcb0e526a42d113",
        "text": "Often, real-world objectives are difficult to encode \nas simple mathematical functions. Otherwise, they \nmight just be incorporated into the objective function \nand the problem would be considered solved. For \nexample, an algorithm for making hiring decisions should \nsimultaneously optimize productivity, ethics, and legality. \nBut how would you go about writing a function that \nmeasures ethics or legality? The problem can also arise \nwhen you desire robustness to changes in the dynamics \nbetween the training and deployment environments.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                236.7239990234375,
                376.82855224609375,
                375.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c382a48606c6eacc40aac93beff1a21f",
        "text": "Trust\nSome authors suggest interpretability is a prerequisite \nfor trust.9,23 Again, what is trust? Is it simply confidence \nthat a model will perform well? If so, a sufficiently \naccurate model should be demonstrably trustworthy, and \ninterpretability would serve no purpose. Trust might also \nbe defined subjectively. For example, a person might feel \nmore at ease with a well-understood model, even if this \nunderstanding serves no obvious purpose. Alternatively, \nwhen the training and deployment objectives diverge, trust",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                390.6400146484375,
                385.06048583984375,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f79b207fc131a3683ea117368a2730d9",
        "text": "might denote confidence that the model will perform well \nwith respect to the real objectives and scenarios.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                380.67144775390625,
                123.62850189208984
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ecfeb85890683502a4e535478f04673e",
        "text": "For example, consider the growing use of ML models \nto forecast crime rates for purposes of allocating police \nofficers. The model may be trusted to make accurate \npredictions but not to account for racial biases in the \ntraining data for the model’s own effect in perpetuating \na cycle of incarceration by over-policing some \nneighborhoods.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                124.7239990234375,
                373.02752685546875,
                221.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c23b8742060cd9a0357ab5de5842a655",
        "text": "Another sense in which an end user might be said to \ntrust an ML model might be if they are comfortable with \nrelinquishing control to it. Through this lens, you might \ncare not only about how often a model is right, but also \nfor which examples it is right. If the model tends to make \nmistakes on only those kinds of inputs where humans also \nmake mistakes, and thus is typically accurate whenever \nhumans are accurate, then you might trust the model \nowing to the absence of any expected cost of relinquishing \ncontrol. If a model tends to make mistakes for inputs that \nhumans classify accurately, however, then there may \nalways be an advantage to maintaining human supervision \nof the algorithms.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                222.7239990234375,
                382.77142333984375,
                403.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7205d5049504d5508dd85cbf5db9d681",
        "text": "Causality\nAlthough supervised learning models are only optimized \ndirectly to make associations, researchers often use them \nin the hope of inferring properties of the natural world. For \nexample, a simple regression model might reveal a strong \nassociation between thalidomide use and birth defects, or \nbetween smoking and lung cancer.29",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                418.6400146484375,
                384.8504943847656,
                515.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "27d295b3c440836f6b243fd3fb57e9d1",
        "text": "algorithms are not guaranteed to reflect causal \nrelationships. There could always be unobserved causes \nresponsible for both associated variables. You might hope, \nhowever, that by interpreting supervised learning models, \nyou could generate hypotheses that scientists could then \ntest. Liu et al.,14 for example, emphasize regression trees \nand Bayesian neural networks, suggesting that these \nmodels are interpretable and thus better able to provide \nclues about the causal relationships between physiologic \nsignals and affective states. The task of inferring causal \nrelationships from observational data has been extensively \nstudied.22 Causal inference methods, however, tend to \nrely on strong assumptions and are not widely used by \npractitioners, especially on large, complex data sets.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                385.8375244140625,
                291.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0ea6e8d6c40b7657c161d6e54894c9cb",
        "text": "W",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                29.30229949951172,
                202.58937072753906,
                72.42232513427734,
                286.66644287109375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5b005a5e25d292e0361c402d23bda901",
        "text": "hile \nthe \nmachine-\nlearning \nobjective\nmight be to \nreduce error, \nthe real-world \npurpose is to \nprovide useful \ninformation.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                30.0,
                217.5598907470703,
                120.84310150146484,
                338.3598937988281
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9e25297c6408ec3646f3951313939824",
        "text": "Transferability\nTypically, training and test data are chosen by randomly \npartitioning examples from the same distribution. A \nmodel’s generalization error is then judged by the gap \nbetween its performance on training and test data. \nHumans exhibit a far richer capacity to generalize, \nhowever, transferring learned skills to unfamiliar \nsituations. ML algorithms are already used in these \nsituations, such as when the environment is nonstationary. \nModels are also deployed in settings where their use might \nalter the environment, invalidating their future predictions. \nAlong these lines, Caruana et al.3 describe a model trained \nto predict probability of death from pneumonia that \nassigned less risk to patients if they also had asthma. \nPresumably, asthma was predictive of a lower risk of death \nbecause of the more aggressive treatment these patients",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                306.6400146484375,
                385.4593811035156,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "78cb6a20c3edb5cc64e4261b90f3fb4a",
        "text": "received. If the model were deployed to aid in triage, these \npatients might then receive less aggressive treatment, \ninvalidating the model.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                381.88958740234375,
                137.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e39aee5b6edc8412761354ed221a39a0",
        "text": "Even worse, there are situations, such as machine \nlearning for security, where the environment might be \nactively adversarial. Consider the recently discovered \nsusceptibility of CNNs (convolutional neural networks). \nThe CNNs were made to misclassify images that were \nimperceptibly (to a human) perturbed.26 Of course, this isn’t \noverfitting in the classical sense. The models both achieve \nstrong results on training data and generalize well when \nused to classify held out test data. The crucial distinction \nis that these images have been altered in ways that, while \nsubtle to human observers, the models never encountered \nduring training. However, these are mistakes a human \nwouldn’t make, and it would be preferable that models \nnot make these mistakes, either. Already, supervised \nlearning models are regularly subject to such adversarial \nmanipulation. Consider the models used to generate \ncredit ratings; higher scores should signify a higher \nprobability that an individual repays a loan. According to \nits own technical report, FICO trains credit models using \nlogistic regression,6 specifically citing interpretability as \na motivation for the choice of model. Features include \ndummy variables representing binned values for average \nage of accounts, debt ratio, the number of late payments, \nand the number of accounts in good standing.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                138.7239990234375,
                385.0693054199219,
                473.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0b6ca8a28faaa641fede05ac634a8e82",
        "text": "Several of these factors can be manipulated at will \nby credit-seekers. For example, one’s debt ratio can be \nimproved simply by requesting periodic increases to \ncredit lines while keeping spending patterns constant.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                474.7239990234375,
                364.3546142578125,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b7966e6ca37b15db740f53bebcec45ad",
        "text": "Similarly, the total number of accounts can be increased by \nsimply applying for new accounts when the probability of \nacceptance is reasonably high. Indeed, FICO and Experian \nboth acknowledge that credit ratings can be manipulated, \neven suggesting guides for improving one’s credit rating. \nThese rating-improvement strategies may fundamentally \nchange one’s underlying ability to pay a debt. The fact \nthat individuals actively and successfully game the rating \nsystem may invalidate its predictive power.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                385.3966369628906,
                221.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dd413b2b2ec155e502eb89d4f5b2bdc0",
        "text": "Informativeness\nSometimes, decision theory is applied to the outputs of \nsupervised models to take actions in the real world. In \nanother common use paradigm, however, the supervised \nmodel is used instead to provide information to human \ndecision-makers, a setting considered by Kim et al.11 and \nHuysmans et al.8 While the machine-learning objective \nmight be to reduce error, the real-world purpose is to \nprovide useful information. The most obvious way that a \nmodel conveys information is via its outputs, but it may \nbe possible via some procedure to convey additional \ninformation to the human decision-maker.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                236.63999938964844,
                374.1193542480469,
                403.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a074007387ab7aebddcdbb052cf8537f",
        "text": "An interpretation may prove informative even without \nshedding light on a model’s inner workings. For example, \na diagnosis model might provide intuition to a human \ndecision maker by pointing to similar cases in support \nof a diagnostic decision. In some cases, a supervised \nlearning model is trained when the real task more closely \nresembles unsupervised learning. The real goal might be \nto explore the underlying structure of the data, and the \nlabeling objective serves only as weak supervision.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                404.7239990234375,
                377.95196533203125,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3fe905d35177dcb1b33e7523540fcc68",
        "text": "Fair and ethical decision making\nAt present, politicians, journalists, and researchers have \nexpressed concern that interpretations must be produced \nfor assessing whether decisions produced automatically \nby algorithms conform to ethical standards.7 Recidivism \npredictions are already used to determine whom to \nrelease and whom to detain, raising ethical concerns. How \ncan you be sure that predictions do not discriminate on \nthe basis of race? Conventional evaluation metrics such \nas accuracy or AUC (area under the curve) offer little \nassurance that ML-based decisions will behave acceptably. \nThus, demands for fairness often lead to demands for \ninterpretable models.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.63999938964844,
                382.9499816894531,
                277.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "832c4eefd689431c321e03e2b11ec2d5",
        "text": "THE TRANSPARENCY NOTION OF INTERPRETABILITY \nLet’s now consider the techniques and model properties \nthat are proposed to confer interpretability. These \nfall broadly into two categories. The first relates to \ntransparency (i.e., how does the model work?). The second \nconsists of post hoc explanations (i.e., what else can the \nmodel tell me?)",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                293.1925048828125,
                379.48492431640625,
                389.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "61ca5704302ce4dff8e80dceb3fa3652",
        "text": "Informally, transparency is the opposite of opacity or \n“black-boxness.” It connotes some sense of understanding \nthe mechanism by which the model works. Transparency \nis considered here at the level of the entire model \n(simulatability), at the level of individual components such \nas parameters (decomposability), and at the level of the \ntraining algorithm (algorithmic transparency).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                390.7239990234375,
                380.5454406738281,
                487.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "28ab15df65efb08e035dd30f40e324fe",
        "text": "if a person can contemplate the entire model at once. This \ndefinition suggests that an interpretable model is a simple \nmodel. For example, for a model to be fully understood, \na human should be able to take the input data together \nwith the parameters of the model and in reasonable \ntime step through every calculation required to produce \na prediction. This accords with the common claim that \nsparse linear models, as produced by lasso regression,27 \nare more interpretable than dense linear models learned \non the same inputs. Ribeiro et al.23 also adopt this notion of \ninterpretability, suggesting that an interpretable model is \none that “can be readily presented to the user with visual \nor textual artifacts.”",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                383.96307373046875,
                277.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "24aa8b5216bc0818a23fdaafe6165d00",
        "text": "The tradeoffs between model size and computation \nto apply a single prediction varies across models. For \nexample, in some models, such as decision trees, the size \nof the model (total number of nodes) may grow quite \nlarge compared to the time required to perform inference \n(length of pass from root to leaf). This suggests that \nsimulatability may admit two subtypes: one based on the \nsize of the model and another based on the computation \nrequired to perform inference.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                278.7239990234375,
                380.9970703125,
                403.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ef5ac406fbd71f8dfcc7027abedc8405",
        "text": "Fixing a notion of simulatability, the quantity denoted \nby reasonable is subjective. Clearly, however, given the \nlimited capacity of human cognition, this ambiguity might \nspan only several orders of magnitude. In this light, neither \nlinear models, rule-based systems, nor decision trees are \nintrinsically interpretable. Sufficiently high-dimensional \nmodels, unwieldy rule lists, and deep decision trees could \nall be considered less transparent than comparatively \ncompact neural networks.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                404.7239990234375,
                383.3175964355469,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e9a7170c379005e8fd8da3fd157300af",
        "text": "Decomposability\nA second notion of transparency might be that each part \nof the model—input, parameter, and calculation—admits \nan intuitive explanation. This accords with the property of \nintelligibility as described by Lou et al.15 For example, each \nnode in a decision tree might correspond to a plain text \ndescription (e.g., all patients with diastolic blood pressure \nover 150). Similarly, the parameters of a linear model could \nbe described as representing strengths of association \nbetween each feature and the label.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                110.28999328613281,
                384.4200439453125,
                249.2784881591797
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "af7ee62dc822e0ca7aba65ddec532e85",
        "text": "T",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                29.58139991760254,
                202.58937072753906,
                56.86283874511719,
                286.66644287109375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e389a80aadb69f852b3a5c0314ba3500",
        "text": "he weights \nof a lin­\near model \nmight seem \nintuitive, \nbut they can be \nfragile with re­\nspect to feature \nselection and \npreprocessing.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                30.0,
                217.5598907470703,
                118.77790069580078,
                327.3598937988281
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "62295c8ac9b7fcd99950ab9b9f908180",
        "text": "Note that this notion of interpretability requires \nthat inputs themselves be individually interpretable, \ndisqualifying some models with highly engineered or \nanonymous features. While this notion is popular, it \nshouldn’t be accepted blindly. The weights of a linear model \nmight seem intuitive, but they can be fragile with respect \nto feature selection and preprocessing. For example, the \ncoefficient corresponding to the association between \nflu risk and vaccination might be positive or negative, \ndepending on whether the feature set includes indicators \nof old age, infancy, or immunodeficiency.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                250.37399291992188,
                385.9950866699219,
                403.27850341796875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "34b690468147bf0a76a269adb2561d44",
        "text": "Algorithmic transparency\nA final notion of transparency might apply at the level of \nthe learning algorithm itself. In the case of linear models, \nyou may understand the shape of the error surface. You \ncan prove that training will converge to a unique solution, \neven for previously unseen data sets. This might provide \nsome confidence that the model will behave in an online \nsetting requiring programmatic retraining on previously",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                418.2900085449219,
                378.21453857421875,
                529.2784423828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "09559914f9a75eaac79124a62c72821b",
        "text": "unseen data. On the other hand, modern deep learning \nmethods lack this sort of algorithmic transparency. While \nthe heuristic optimization procedures for neural networks \nare demonstrably powerful, we don’t understand how they \nwork, and at present cannot guarantee a priori that they \nwill work on new problems. Note, however, that humans \nexhibit none of these forms of transparency.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                383.6218566894531,
                193.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1420736857d520a1cc592ab1773e0023",
        "text": "Post hoc interpretability \nPost hoc interpretability represents a distinct approach to \nextracting information from learned models. While post hoc \ninterpretations often do not elucidate precisely how a model \nworks, they may nonetheless confer useful information \nfor practitioners and end users of machine learning. Some \ncommon approaches to post hoc interpretations include \nnatural language explanations, visualizations of learned \nrepresentations or models, and explanations by example \n(e.g., this tumor is classified as malignant because to the \nmodel it looks a lot like these other tumors).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                208.63999938964844,
                385.8795166015625,
                361.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "89db834780bcf0f34c8d916036113f6b",
        "text": "To the extent that we might consider humans to \nbe interpretable, this is the sort of interpretability \nthat applies. For all we know, the processes by which \nhumans make decisions and those by which they explain \nthem may be distinct. One advantage of this concept of \ninterpretability is that opaque models can be interpreted \nafter the fact, without sacrificing predictive performance.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                362.7239990234375,
                380.7134704589844,
                459.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6e4b814b3ff4d91056586dd8d1383d28",
        "text": "Text explanations\nHumans often justify decisions verbally. Similarly, one \nmodel might be trained to generate predictions, and \na separate model, such as a recurrent neural network",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                474.6400146484375,
                362.5275573730469,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1691dc717fec10c2088fb7d3d2d710af",
        "text": "language model, to generate an explanation. Such an \napproach is taken in a line of work by Krening et al.12 They \npropose a system in which one model (a reinforcement \nlearner) chooses actions to optimize cumulative \ndiscounted return. They train another model to map a \nmodel’s state representation onto verbal explanations \nof strategy. These explanations are trained to maximize \nthe likelihood of previously observed ground-truth \nexplanations from human players and may not faithfully \ndescribe the agent’s decisions, however plausible they \nappear. A connection exists between this approach and \nrecent work on neural image captioning in which the \nrepresentations learned by a discriminative CNN (trained \nfor image classification) are coopted by a second model to \ngenerate captions. These captions might be regarded as \ninterpretations that accompany classifications.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                381.6164855957031,
                319.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "46df662889e7697d5bd374765c508305",
        "text": "In work on recommender systems, McAuley and \nLeskovec18 use text to explain the decisions of a latent \nfactor model. Their method consists of simultaneously \ntraining a latent factor model for rating prediction and \na topic model for product reviews. During training they \nalternate between decreasing the squared error on rating \nprediction and increasing the likelihood of review text. \nThe models are connected because they use normalized \nlatent factors as topic distributions. In other words, latent \nfactors are regularized such that they are also good \nat explaining the topic distributions in review text. The \nauthors then explain user-item compatibility by examining \nthe top words in the topics corresponding to matching \ncomponents of their latent factors. Note that the practice \nof interpreting topic models by presenting the top words is",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                320.7239990234375,
                384.2520751953125,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c4e947618e86c79031aa4ef049bafc47",
        "text": "itself a post hoc interpretation technique that has invited \nscrutiny.4 Moreover note that here we have only spoken \nto the form factor of an explanation (that it consists of \nnatural language), but not what precisely constitutes \ncorrectness. So far, the literature has dodged the issue of \ncorrectness, sometimes punting the issue by embracing \na subjective view of the problem and asking people what \nthey prefer.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                378.84442138671875,
                207.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0704d533d1e418fd2136c2ddd2cf6f8c",
        "text": "Visualization\nAnother common approach to generating post hoc \ninterpretations is to render visualizations in the hope of \ndetermining qualitatively what a model has learned. One \npopular method is to visualize high-dimensional distributed \nrepresentations with t-SNE (t-distributed stochastic \nneighbor embedding),28 a technique that renders 2D \nvisualizations in which nearby data points are likely to \nappear close together.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                208.63999938964844,
                385.41754150390625,
                333.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f828ef9108962e5c32675f38bea2c0c7",
        "text": "Mordvintsev et al.20 attempt to explain what an image \nclassification network has learned by altering the input \nthrough gradient descent to enhance the activations \nof certain nodes selected from the hidden layers. An \ninspection of the perturbed inputs can give clues to what \nthe model has learned. Likely because the model was \ntrained on a large corpus of animal images, they observed \nthat enhancing some nodes caused certain dog faces to \nappear throughout the input image.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                334.7239990234375,
                379.9576416015625,
                459.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4a0e9a0512cfe91f7f51c3f41fcfa869",
        "text": "In the computer vision community, similar approaches \nhave been explored to investigate what information is \nretained at various layers of a neural network. Mahendran \nand Vedaldi17 pass an image through a discriminative CNN \nto generate a representation. They then demonstrate that",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                460.7239990234375,
                382.30963134765625,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5c529bbb8ee74747674b6b923473a735",
        "text": "the original image can be recovered with high fidelity even \nfrom reasonably high-level representations (level 6 of an \nAlexNet) by performing gradient descent on randomly \ninitialized pixels. As before with text, discussions of \nvisualization focus on form factor and appeal, but we still \nlack a rigorous standard of correctness.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                381.64788818359375,
                179.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0ff5d4e03a3df513236726d41f891b29",
        "text": "Local explanations\nWhile it may be difficult to describe succinctly the full \nmapping learned by a neural network, some of the \nliterature focuses instead on explaining what a neural \nnetwork depends on locally. One popular approach for \ndeep neural nets is to compute a saliency map. Typically, \nthey take the gradient of the output corresponding to \nthe correct class with respect to a given input vector. For \nimages, this gradient can be applied as a mask, highlighting \nregions of the input that, if changed, would most influence \nthe output.25,30",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                194.63999938964844,
                383.0129089355469,
                347.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "021d11deaa75c3a8195727b96e33520c",
        "text": "Note that these explanations of what a model is \nfocusing on may be misleading. The saliency map is a local \nexplanation only. Once you move a single pixel, you may get \na very different saliency map. This contrasts with linear \nmodels, which model global relationships between inputs \nand outputs.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                348.7239990234375,
                385.5960998535156,
                431.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f8b39839b61d729af7f38b0751463f3f",
        "text": "Another attempt at local explanations is made by \nRibeiro et al.23 In this work, the authors explain the \ndecisions of any model in a local region near a particular \npoint by learning a separate sparse linear model to explain \nthe decisions of the first. Strangely, although the method’s \nappeal over saliency maps owes to its ability to provide \nexplanations for non-differentiable models, it is more",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                432.7239990234375,
                383.4645690917969,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "659612e9ea54572e68fd38aec481e28c",
        "text": "often used when the model subject to interpretation is in \nfact differentiable. In this case, what is provided, besides \na noisy estimate of the gradient, remains unclear. In this \npaper, the explanation is offered in terms of a set of \nsuperpixels. Whether or not this is more informative than \na plain gradient may depend strongly on how one chooses \nthe superpixels. Moreover, absent a rigorously defined \nobjective, who is to say which hyper-parameters are \ncorrect?",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 19,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                378.9915466308594,
                221.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "27fe8acec4c4ec0cbe279fd6ae9357b4",
        "text": "Explanation by example\nOne post hoc mechanism for explaining the decisions of \na model might be to report (in addition to predictions) \nwhich other examples are most similar with respect to \nthe model, a method suggested by Caruana et al.2 Training \na deep neural network or latent variable model for a \ndiscriminative task provides access to not only predictions \nbut also the learned representations. Then, for any \nexample, in addition to generating a prediction, you can \nuse the activations of the hidden layers to identify the \nk-nearest neighbors based on the proximity in the space \nlearned by the model. This sort of explanation by example \nhas precedent in how humans sometimes justify actions by \nanalogy. For example, doctors often refer to case studies \nto support a planned treatment protocol.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 19,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                236.63999938964844,
                382.970947265625,
                445.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "25902947932f2fa63957015dd3a05a19",
        "text": "In the neural network literature, Mikolov et al.19 use \nsuch an approach to examine the learned representations \nof words after training the word2vec model. While their \nmodel is trained for discriminative skip-gram prediction, \nto examine which relationships the model has learned \nthey enumerate nearest neighbors of words based on",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 19,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                446.7239990234375,
                380.16741943359375,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0a711f4688f3812a8ba546be3fe65979",
        "text": "distances calculated in the latent space. Kim et al.10 and \nDoshi-Velez et al.5 have done related work in Bayesian \nmethods, investigating case-based reasoning approaches \nfor interpreting generative models.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                378.3823547363281,
                151.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9a8d129e4b7c274be871dc47e94d2949",
        "text": "DISCUSSION \nThe concept of interpretability appears simultaneously \nimportant and slippery. Earlier, this article analyzed both \nthe motivations for interpretability and some attempts by \nthe research community to confer it. Now let’s consider the \nimplications of this analysis and offer several takeaways.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                167.1925048828125,
                386.04754638671875,
                249.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "94581a5a5cc5cb9db4c0f535c6bca244",
        "text": "L",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.99970054626465,
                202.58937072753906,
                52.815895080566406,
                286.66644287109375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4fd861b57126de671ea0a0b437197bb4",
        "text": "inear models \nlose simu­\nlatability \nor decom­\nposability, \nrespectively.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                30.0,
                217.5598907470703,
                118.76277923583984,
                283.3598937988281
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a662bf7316dacdca1ceda21a50c21416",
        "text": "3 Linear models are not strictly more interpretable \nthan deep neural networks. Despite this claim’s enduring \npopularity, its truth value depends on which notion of \ninterpretability is employed. With respect to algorithmic \ntransparency, this claim seems uncontroversial, but \ngiven high-dimensional or heavily engineered features, \nlinear models lose simulatability or decomposability, \nrespectively.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                251.7239990234375,
                374.8334655761719,
                362.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a4540b63863bba65579d050f6900b87d",
        "text": "When choosing between linear and deep models, \nyou must often make a tradeoff between algorithmic \ntransparency and decomposability. This is because \ndeep neural networks tend to operate on raw or lightly \nprocessed features. So, if nothing else, the features are \nintuitively meaningful, and post hoc reasoning is sensible. \nTo get comparable performance, however, linear models \noften must operate on heavily hand-engineered features. \nLipton et al.13 demonstrate such a case where linear \nmodels can approach the performance of recurrent neural \nnetworks (RNNs) only at the cost of decomposability.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                363.7239990234375,
                384.45159912109375,
                516.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fc65c9a01bac898281ddcd76700000c3",
        "text": "neural networks exhibit a clear advantage. They learn \nrich representations that can be visualized, verbalized, \nor used for clustering. Considering the desiderata for \ninterpretability, linear models appear to have a better \ntrack record for studying the natural world, but there \nseems to be no theoretical reason why this must be so. \nConceivably, post hoc interpretations could prove useful in \nsimilar scenarios.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 21,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                383.31756591796875,
                207.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "deac4352d689ee91cf7b36c7f4728fda",
        "text": "3 Claims about interpretability must be qualified. \nAs demonstrated here, the term interpretability does \nnot reference a monolithic concept. To be meaningful, \nany assertion regarding interpretability should fix \na specific definition. If the model satisfies a form of \ntransparency, this can be shown directly. For post hoc \ninterpretability, work in this field should fix a clear \nobjective and demonstrate evidence that the offered form \nof interpretation achieves it.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 21,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                209.7239990234375,
                382.14141845703125,
                334.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5e9635c530caaf5c96df7efe328a0a31",
        "text": "3 In some cases, transparency may be at odds with \nthe broader objectives of AI (artificial intelligence). \nSome arguments against black-box algorithms appear \nto preclude any model that could match or surpass \nhuman abilities on complex tasks. As a concrete example, \nthe short-term goal of building trust with doctors by \ndeveloping transparent models might clash with the \nlonger-term goal of improving health care. Be careful \nwhen giving up predictive power that the desire for \ntransparency is justified and not simply a concession to \ninstitutional biases against new methods.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 21,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                336.7239990234375,
                377.185546875,
                489.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "91d98b88c8eb6c8aa588fff0abff3ae7",
        "text": "3 Post hoc interpretations can potentially mislead. \nBeware of blindly embracing post hoc notions of \ninterpretability, especially when optimized to placate",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 21,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                491.7239990234375,
                361.3484191894531,
                532.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cd1d913ec9e706ac64178fb9b2ca9e07",
        "text": "subjective demands. In such cases, one might—deliberately \nor not—optimize an algorithm to present misleading \nbut plausible explanations. As humans, we are known to \nengage in this behavior, as evidenced in hiring practices \nand college admissions. Several journalists and social \nscientists have demonstrated that acceptance decisions \nattributed to virtues such as leadership or originality \noften disguise racial or gender discrimination.21 In the",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 22,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                383.2125244140625,
                207.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4279c8ac0bda9d62b33c99efb6d2c4de",
        "text": "Related articles \n3 Accountability in Algorithmic \nDecision-making\nNicholas Diakopoulos\nA view from computational journalism\nhttps://queue.acm.org/detail.cfm?id=2886105\n3 Black Box Debugging\nJames A. Whittaker, Herbert H. Thompson\nIt’s all about what takes place at the \nboundary of an application.\nhttps://queue.acm.org/detail.cfm?id=966807\n3 Hazy: Making it Easier to Build and \nMaintain Big-data Analytics\nArun Kumar, Feng Niu, and Christopher Ré\nRacing to unleash the full potential of \nbig data with the latest statistical and \nmachine-learning techniques.\nhttps://queue.acm.org/detail.cfm?id=2431055",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 22,
            "languages": [
                "eng"
            ],
            "coordinates": [
                30.680500030517578,
                228.9199981689453,
                220.646484375,
                480.0660095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "88c7a54e9343a870bb0dc6be93f24286",
        "text": "FUTURE WORK\nThere are several promising \ndirections for future work. \nFirst, for some problems, the \ndiscrepancy between real-life and \nmachine-learning objectives could \nbe mitigated by developing richer \nloss functions and performance \nmetrics. Exemplars of this \ndirection include research on \nsparsity-inducing regularizers \nand cost-sensitive learning. \nSecond, this analysis can be \nexpanded to other ML paradigms \nsuch as reinforcement learning. \nReinforcement learners can address some (but not all) \nof the objectives of interpretability research by directly",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 22,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                293.1925048828125,
                383.6455078125,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bbaeef7e72baa19dcc9a048fb228201e",
        "text": "rush to gain acceptance for \nmachine learning and to emulate \nhuman intelligence, we should \nall be careful not to reproduce \npathological behavior at scale.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 22,
            "languages": [
                "eng"
            ],
            "coordinates": [
                235.28050231933594,
                208.7239990234375,
                377.1984558105469,
                277.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ab79b2f4f30d3823cf1521a2ab0a0b87",
        "text": "modeling interaction between models and environments. \nThis capability, however, may come at the cost of allowing \nmodels to experiment in the world, incurring real \nconsequences.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 23,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                379.00201416015625,
                151.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d3c92706d6fd9d695fac8746375bae46",
        "text": "Notably, reinforcement learners are able to learn \ncausal relationships between their actions and real-world \nimpacts. Like supervised learning, however, reinforcement \nlearning relies on a well-defined scalar objective. For \nproblems such as fairness, where we struggle to verbalize \nprecise definitions of success, a shift of the ML paradigm is \nunlikely to eliminate the problems we face.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 23,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                152.7239990234375,
                382.9606018066406,
                249.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bf479d0bef786df7ce3b72b621e1e2c6",
        "text": "References \n1.   \u0007Athey, S., Imbens, G. W. 2015 Machine-learning methods",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 23,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                264.6400146484375,
                381.1651306152344,
                291.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d2b6f3683b8558d56ede3bf92deb1ca6",
        "text": "https://arxiv.org/abs/1504.01132v1 (see also ref. 7).\n2.   \u0007Caruana, R., Kangarloo, H., Dionisio, J. D, Sinha, U.,",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 23,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                292.7239990234375,
                355.7340393066406,
                319.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a4909f0701aadda2ddda5d0edff0ba60",
        "text": "Johnson, D. 1999. Case-based explanation of non-case-\nbased learning methods. In Proceedings of the American \nMedical Informatics Association (AMIA) Symposium: \n212-215. \n3.   \u0007Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M.,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 23,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                320.7239990234375,
                384.80816650390625,
                389.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "570b2db165b3b2294c13d314bfcd8aee",
        "text": "Elhadad, N. 2015. Intelligible models for healthcare: \nPredicting pneumonia risk and hospital 30-day \nreadmission. In Proceedings of the 21st Annual SIGKDD \nInternational Conference on Knowledge Discovery and \nData Mining, 1721-1730. \n4.   \u0007Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J. L., Blei,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 23,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                390.7239990234375,
                376.55914306640625,
                473.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5b61d2c3879390c1fab7c18db2a3dc07",
        "text": "D. M. 2009. Reading tea leaves: how humans interpret \ntopic models. In Proceedings of the 22nd International \nConference on Neural Information Processing Systems \n(NIPS), 288-296.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 23,
            "languages": [
                "eng"
            ],
            "coordinates": [
                145.1696014404297,
                474.7239990234375,
                377.8074645996094,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "628fce361dbfc47c5112d92ed69eb31f",
        "text": "5.   \u0007Doshi-Velez, F., Wallace, B., Adams, R. 2015. Graph-",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 24,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                359.1045837402344,
                109.62850189208984
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8820e761abe7243c7b087516b266136c",
        "text": "sparse lDA: a topic model with structured sparsity. In \nProceedings of the 29th Association for the Advancement \nof Artificial Intelligence (AAAI) Conference, 2575-2581. \n6.   \u0007FICO (Fair Isaac Corporation). 2011. Introduction to",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 24,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                110.7239990234375,
                385.760986328125,
                165.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "12bda9114ce674446b744cc2689df37b",
        "text": "model builder scorecard; http://www.fico.com/en/latest-\nthinking/white-papers/introduction-to-model-builder-\nscorecard.  \n7.   \u0007Goodman, B., Flaxman, S. 2016. European Union",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 24,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                166.7239990234375,
                382.7951354980469,
                221.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9744b6addad3c9afc0bcfd884d6995e3",
        "text": "regulations on algorithmic decision-making and a “right \nto explanation.” https://arxiv.org/abs/1606.08813v3. \n8.   \u0007Huysmans, J., Dejaeger, K., Mues, C., Vanthienen,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 24,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                222.7239990234375,
                380.6855773925781,
                263.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3c2c5c56b4fba1f167e803476580bd46",
        "text": "J., Baesens, B. 2011. An empirical evaluation of the \ncomprehensibility of decision table, tree- and rule-\nbased predictive models. Journal of Decision Support \nSystems 51(1), 141-154. \n9.   \u0007Kim, B. 2015. Interactive and interpretable machine-",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 24,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                264.7239990234375,
                374.51690673828125,
                333.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "370f395d037b8044635317b5bbe36dff",
        "text": "learning models for human-machine collaboration. \nPh.D. thesis. Massachusetts Institute of Technology. \n10. \u0007Kim, B., Rudin, C., Shah, J. A. 2014. The Bayesian case",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 24,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                334.7239990234375,
                368.22314453125,
                375.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "96935be5d5de87f146d065efb8c23902",
        "text": "model: A generative approach for case-based reasoning \nand prototype classification. In Proceedings of the \n27th International Conference on Neural Information \nProcessing Systems (NIPS), volume 2, 1952-1960. \n11 . \u0007Kim, B., Glassman, E., Johnson, B., Shah, J. 2015. iBCM:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 24,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                376.7239990234375,
                386.1068420410156,
                445.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "baa56cd95a2f9209adbf912999bcb79f",
        "text": "Interactive Bayesian case model empowering humans \nvia intuitive interaction. Massachusetts Institute of \nTechnology, Cambridge, MA. \n12. \u0007Krening, S., Harrison, B., Feigh, K., Isbell, C., Riedl, M.,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 24,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                446.7239990234375,
                375.4996032714844,
                501.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "67d26f5f29682d292da41ff6a9f4dfbc",
        "text": "Cognitive and Developmental Systems 9(1), 41-55. \n13. \u0007Lipton, Z. C., Kale, D. C., Wetzel, R. 2016. Modeling",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 25,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                354.3157043457031,
                123.62850189208984
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4f62030080192ebb953da5e8a7498bc3",
        "text": "missing data in clinical time series with RNNs. In \nProceedings of Machine Learning for Healthcare. \n14. \u0007Liu, C., Rani, P., Sarkar, N. 2006. An empirical study of",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 25,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                124.7239990234375,
                369.6149597167969,
                165.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0945bd8f7fd490658919859be82583a0",
        "text": "machine-learning techniques for affect recognition \nin human-robot interaction. Pattern Analysis and \nApplications 9(1): 58-69. \n15. \u0007Lou, Y., Caruana, R., Gehrke, J. 2012. Intelligible models",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 25,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                166.7239990234375,
                375.84136962890625,
                221.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4e124e4a1756a9eaa655db4d4d4bc459",
        "text": "for classification and regression. In Proceedings of \nthe 18th ACM SIGKDD International Conference on \nKnowledge Discovery and Data Mining, 150-158. \n16. \u0007Lou, Y., Caruana, R., Gehrke, J., Hooker, G. 2013. Accurate",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 25,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.00010681152344,
                222.7239990234375,
                383.61151123046875,
                277.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "86a6a6c929fcba5e7c4f5f8cefa98606",
        "text": "intelligible models with pairwise interactions. In \nProceedings of the 19th ACM SIGKDD International \nConference on Knowledge Discovery and Data Mining, \n623-631. \n17. \u0007Mahendran, A., Vedaldi, A. 2015. Understanding deep",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 25,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.00010681152344,
                278.7239990234375,
                374.43341064453125,
                347.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b980f52cf9cdbf760a0d9d2d338a11b7",
        "text": "image representations by inverting them. In Proceedings \nof the IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR), 1-9. \n18. \u0007McAuley, J., Leskovec, J. 2013. Hidden factors and",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 25,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.00010681152344,
                348.7239990234375,
                384.8224792480469,
                403.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dd3440ea943d0c0716527f1db6cf407e",
        "text": "hidden topics: understanding rating dimensions with \nreview text. In Proceedings of the 7th ACM Conference on \nRecommender Systems, 165-172. \n19. \u0007Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., Dean,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 25,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                404.7239990234375,
                385.47149658203125,
                459.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "96386ee34c0f6d2df08875e163747568",
        "text": "J. 2013. Distributed representations of words and \nphrases and their compositionality. In Proceedings of \nthe 26th International Conference on Neural Information \nProcessing Systems (NIPS), volume 2, 3111–3119. \n20. \u0007Mordvintsev, A., Olah, C., Tyka, M. 2015. Inceptionism:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 25,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.00010681152344,
                460.7239990234375,
                382.180908203125,
                529.6284790039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6a23ef88e19c9a8267f18f7a3f604b28",
        "text": "going deeper into neural networks. Google AI Blog; \nhttps://ai.googleblog.com/2015/06/inceptionism-going-\ndeeper-into-neural.html. \n21. \u0007Mounk, Y. 2014. Is Harvard unfair to Asian-Americans?",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 26,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                378.7286682128906,
                151.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a220e71fa2b7c887ccb4c4cf81615220",
        "text": "New York Times (Nov. 24); http://www.nytimes.\ncom/2014/11/25/opinion/is-harvard-unfair-to-asian-\namericans.html. \n22. \u0007Pearl, J. 2009. Causality. Cambridge University Press. \n23. \u0007Ribeiro, M. T., Singh, S., Guestrin, C. 2016. “Why should I",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 26,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                152.7239990234375,
                379.0544738769531,
                221.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "27426d381f3b8309efc1c143ac10288d",
        "text": "trust you?”: explaining the predictions of any classifier. \nIn Proceedings of the 22nd SIGKDD International \nConference on Knowledge Discovery and Data Mining, \n1135-1144. \n24. \u0007Ridgeway, G., Madigan, D., Richardson, T., O’Kane, J.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 26,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                222.7239990234375,
                378.9122619628906,
                291.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1490cad6fac4fd34c4b0f363ba7f07cf",
        "text": "1998. Interpretable boosted naïve Bayes classification. \nIn Proceedings of the 4th International Conference on \nKnowledge Discovery and Data Mining: 101-104. \n25. \u0007Simonyan, K., Vedaldi, A., Zisserman, A. 2013. Deep",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 26,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                292.7239990234375,
                379.7398376464844,
                347.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d4536ca9b7d81e1287ca237949d9dfa0",
        "text": "inside convolutional networks: Visualising image \nclassification models and saliency maps. https://arxiv.\norg/abs/1312.6034 (see notes to refs 1, 7). \n26. \u0007Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 26,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                348.7239990234375,
                379.695068359375,
                403.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5fb9542229fd4dc5726cadc028f2d6b4",
        "text": "D., Goodfellow, I., Fergus, R. 2013. Intriguing properties \nof neural networks. https://arxiv.org/abs/1312.6199 (see \nrefs 1, 7, 25). \n27. \u0007Tibshirani, R. 1996. Regression shrinkage and selection",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 26,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                404.7239990234375,
                380.1198425292969,
                459.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b297dbc29b3f52903f85f87715e55667",
        "text": "via the lasso. Journal of the Royal Statistical Society: \nSeries B (Statistical Methodology) 58(1), 267-288. \n28. \u0007Van der Maaten, L., Hinton, G. 2008. Visualizing data",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 26,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                460.7239990234375,
                370.047607421875,
                501.6285095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1a5f871a261b2dd0d24c5f91a5d800f8",
        "text": "29. \u0007Wang, H.-X., Fratiglioni, L., Frisoni, G. B., Viitanen, M.,",
        "type": "Title",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 27,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                96.7239990234375,
                366.8115539550781,
                109.62850189208984
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0a1c9139422c0180ff901e9899d1ff9f",
        "text": "Winblad, B. 1999. Smoking and the occurrence of \nAlzheimer’s disease: cross-sectional and longitudinal \ndata in a population-based study. American Journal of \nEpidemiology 149(7), 640-644. \n30. \u0007Wang, Z., Freitas, N., Lanctot, M. 2016. Dueling network",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 27,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                110.7239990234375,
                382.7400207519531,
                179.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cf36a504941e1a4a3bb1fda61e8da864",
        "text": "architectures for deep reinforcement learning. \nProceedings of the 33rd International Conference on \nMachine Learning 48, 1995-2003.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 27,
            "languages": [
                "eng"
            ],
            "coordinates": [
                147.7633056640625,
                180.7239990234375,
                367.173095703125,
                221.6284942626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "62cbdc2daf32ce39c82f3c58fb0f5dda",
        "text": "Zachary Chase Lipton is an assistant professor at Carnegie \nMellon University. His research spans both core machine-\nlearning methods and their social impact, concentrating \non deep learning for time series data and sequential \ndecision making. This work addresses diverse application \nareas, including medical diagnosis, dialogue systems, and \nproduct recommendation. He is the founding editor of the \nApproximately Correct blog and the lead author of Deep \nLearning – The Straight Dope, an open-source interactive \nbook teaching deep learning through Jupyter Notebook. Find \nhim on Twitter (@zacharylipton) or GitHub (@zackchase).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "ref_5.pdf",
            "page_number": 27,
            "languages": [
                "eng"
            ],
            "coordinates": [
                132.0,
                238.1125030517578,
                379.45013427734375,
                400.4825134277344
            ],
            "is_full_width": false
        }
    }
]