[
    {
        "element_id": "318bc095c0503a1b274d21f53da40bef",
        "text": "Neurocomputing 21 (1998) 203—224",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                144.24000549316406,
                65.70836639404297,
                267.40802001953125,
                73.76703643798828
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d90c97b5a6339d2593b94362553f148b",
        "text": "Developments of the generative topographic mapping",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                44.01599884033203,
                131.82627868652344,
                367.8066101074219,
                145.82627868652344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cd525da2abb0dbfb20e52e9965235d70",
        "text": "Christopher M. Bishop!,*, Markus Svense´n\", 1, Christopher K.I. Williams#",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                30.33599853515625,
                156.75999450683594,
                381.4933776855469,
                168.04464721679688
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "b1650740ae28cda6b1c8aee8ea798c2b",
        "text": "! Microsoft Research, St. George House, 1 Guildhall Street, Cambridge CB2 3NH, UK\n\" Neural Computing Research Group, Aston University, Birmingham, B4 7ET, UK\n# Neural Computing Research Group, Aston University, Birmingham, B4 7ET, UK",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                63.81599426269531,
                174.13601684570312,
                347.81121826171875,
                202.30392456054688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "800c6b8e4996b79347c3c2c7a68159c1",
        "text": "Accepted 26 May 1998",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                166.416015625,
                211.20704650878906,
                245.41680908203125,
                219.20704650878906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "349ee06f5af785104ff538514bdcffee",
        "text": "Abstract",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                246.3041534423828,
                58.92900848388672,
                255.3041534423828
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "73a70d9e724cc20912bbaab130c23272",
        "text": "The generative topographic mapping (GTM) model was introduced by Bishop et al. (1998,\nNeural Comput. 10(1), 215—234) as a probabilistic re-formulation of the self-organizing map\n(SOM). It oﬀers a number of advantages compared with the standard SOM, and has already\nbeen used in a variety of applications. In this paper we report on several extensions of the GTM,\nincluding an incremental version of the EM algorithm for estimating the model parameters, the\nuse of local subspace models, extensions to mixed discrete and continuous data, semi-linear\nmodels which permit the use of high-dimensional manifolds whilst avoiding computational\nintractability, Bayesian inference applied to hyper-parameters, and an alternative framework\nfor the GTM based on Gaussian processes. All of these developments directly exploit the\nprobabilistic structure of the GTM, thereby allowing the underlying modelling assumptions to\nbe made explicit. They also highlight the advantages of adopting a consistent probabilistic\nframework for the formulation of pattern recognition algorithms. ( 1998 Elsevier Science\nB.V. All rights reserved.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                268.1769104003906,
                385.35064697265625,
                408.6488952636719
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "9b54fba6e4102b1c05702a85e50338c7",
        "text": "Keywords: Generative topographic mapping; EM algorithm; Local subspace models; Semi-\nlinear models; Bayesian inference",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592864990234375,
                421.5368957519531,
                385.24359130859375,
                441.48089599609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "7b417fabd3d340cbc48384ac92ffddb1",
        "text": "1. Introduction",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                474.563720703125,
                90.13801574707031,
                484.563720703125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6a82ee4faafc5e2d2c8e08ce97c2aaf3",
        "text": "Probability theory provides a powerful, consistent framework for dealing quantita-\ntively with uncertainty [10]. It is therefore ideally suited as a theoretical foundation\nfor pattern recognition. Recently, the self-organizing map (SOM) of Kohonen [19]",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                495.4987487792969,
                385.3789367675781,
                529.4027709960938
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "8f19b5e9a382994264f3fe774e9c0316",
        "text": "*Corresponding author. E-mail: cmbishop@microsoft.com\n1Current address: Max-Plank-Institute of Cognitive Neuroscience, Inselstra{e 22—26, D-04103, Leipzig,\nGermany.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.5936279296875,
                550.2683715820312,
                385.218505859375,
                578.27099609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "f445fa02998f0483059e0546dbc3dbed",
        "text": "was re-formulated within a probabilistic setting [7] to give the generative topographic\nmapping (GTM). In going to a probabilistic formulation, several limitations of the\nSOM were overcome, including the absence of a cost function and the lack of\na convergence proof.\nA further advantage of the probabilistic formulation of the GTM is that extensions\nto the basic model can be formulated in a principled manner in which the correspond-\ning modelling assumptions are made explicit. In this paper we present several\nextensions of the GTM, all of which build on its probabilistic formulation. We ﬁrst\nshow, in Section 2, how a generalized form of EM algorithm can be used to derive an\nincremental version in which data points are presented one at a time, while preserving\nthe convergence guarantees of the batch version. Next we show in Section 3 how the\nGaussian components of the GTM can be generalized from an isotropic distribution\nto one which reﬂects the local subspace properties of the underlying manifold. Then in\nSection 4 we show how the GTM can be extended to allow for discrete as well as con-\ntinuous data variables. A generalization of the GTM which permits the use of high-\ndimensional manifolds without running into computational intractability is described\nin Section 5. Next, in Section 6 we provide a Bayesian treatment of the hyper-\nparameters in the GTM. Finally, in Section 7 we demonstrate the use of Gaussian\nprocesses in place of standard regression models to deﬁne the non-linear manifold.\nWe begin with a brief, self-contained, review of the GTM.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                42.83476257324219,
                385.35986328125,
                279.9229431152344
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "7b172c123f7ebadbde50ab9b31424701",
        "text": "1.1. The generative topographic mapping",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                297.713623046875,
                197.67498779296875,
                307.8980712890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c942b99cd976e0144fd3d6ef26a1ac62",
        "text": "The generative topographic mapping is a probability density model which describes\nthe distribution of data in a space of several dimensions in terms of a smaller number\nof latent (or hidden) variables. By using a discrete grid of points in latent space,\nanalogous to the nodes of the SOM, it is able to use a non-linear relationship between\nthe latent space and the data space while remaining tractable. A detailed derivation of\nthe GTM can be found in [7]. Here we simply describe the resulting density model\nand summarize the parameter estimation (or training) procedure.\nOur description of the GTM starts by deﬁning a q-dimensional latent space, with\ncoordinates u\"(u1,2,uq), as shown schematically on the left-hand side of Fig. 1. For\nthe purposes of this paper we shall be primarily interested in q\"1 or q\"2. Within\nthe latent space we introduce a regular array of nodes, labelled by the index\ni\"1,2,K. These are analogous to the nodes of the SOM. Next we introduce a set of\nM ﬁxed non-linear basis functions /(u)\"M/j(u)N, where j\"1,2, M, which form\na non-orthogonal basis set. The M/jN might consist, for example, of a regular array of\nGaussian or sigmoidal functions. Using these basis functions we deﬁne a non-linear\ntransformation from the latent space to the data space given by a linear combination\nof the basis functions so that each point u in latent space is mapped to a correspond-\ning point y in the D-dimensional data space given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                321.6909484863281,
                385.348876953125,
                535.0341796875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "6114df5ba6e9f92fae59dbd8e3ee1820",
        "text": "y\"W/(u),\n(1)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50498962402344,
                544.7025756835938,
                385.13397216796875,
                556.9222412109375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "12ede5b6424cec7778912c7c66747cca",
        "text": "where W is a D]M matrix of weight parameters.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59300994873047,
                568.464111328125,
                239.54306030273438,
                578.8822021484375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5d321e0b3f6ec9bb9064e5547ebc4cb2",
        "text": "If we denote the node locations in latent space by ui, then Eq. (1) deﬁnes a corre-\nsponding set of ‘reference vectors’ given by",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                42.83476257324219,
                385.1939392089844,
                64.78678894042969
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "0bb402ad11526f1c9e5a9ef8b235b2e3",
        "text": "mi\"W/(ui).\n(2)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503997802734375,
                71.08641052246094,
                385.13201904296875,
                88.5040283203125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "451b36bd1221caa451708bc32be5a131",
        "text": "Each of the reference vectors then forms the centre of an isotropic Gaussian distribu-\ntion in data space, whose inverse variance we denote by b, so that",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592010498046875,
                91.57878112792969,
                385.28900146484375,
                113.53080749511719
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "cc045d538fe5ca0533baa0b8cefb0b8c",
        "text": "p(xDi)\"A\nb\n2pB\nD@2expG!b\n2Emi!xE2H.\n(3)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503997802734375,
                121.48797607421875,
                385.13201904296875,
                149.01597595214844
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "bb9fb70bd865fc6ddf8413790f3a0a04",
        "text": "Finally, the probability density function for the GTM model is obtained by summing\nover all of the Gaussian components, to give",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                154.21873474121094,
                385.2760009765625,
                176.1707305908203
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "873f4a65c47e47cc3b76be41e3ba4f75",
        "text": "1\nKA\nb\n2pB\nD@2expG!b\n2Emi!xE2H,\n(4)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                179.7390594482422,
                183.9840087890625,
                385.13201904296875,
                211.5120086669922
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5fbde23091bf02ce2d05c9c9b435b86a",
        "text": "p(xDW,b)\" K+\ni/1\nP(i)p(xDi)\" K+\ni/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503997802734375,
                184.199951171875,
                177.42706298828125,
                214.35995483398438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1bd0408a923bd5e807dd3c060e6b4c4b",
        "text": "where K is the total number of components (equal to the number of grid points in\nlatent space), and we have taken the prior probabilities of each of the components to\nbe constant and equal to 1/K. To summarize, we can regard the GTM model as\na constrained mixture of Gaussians, as illustrated schematically in Fig. 1, in which the\nGaussian components are isotropic with an inverse variance b and have centres given\nby Eq. (2). The GTM is an example of a latent variable model, in which the\nprobability distribution of the observed data variables x is expressed in terms of an\nintegration over the distribution of a set of latent, or hidden, variables u whose values\nare unobserved. The regular grid of points in latent space corresponds to a particular\nchoice of latent space distribution for which the integration is tractable. Since the\ntransformation from latent space to data space is non-linear, the GTM is representing\nthe distribution of data in terms of a q-dimensional non-Euclidean manifold in data\nspace. The Gaussian distribution (3) represents a noise model and allows for the fact\nthat the data will not be conﬁned precisely to such a q-dimensional manifold.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                216.64141845703125,
                385.3479309082031,
                382.0906982421875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "f4ac2d9973bbf0c9b0673d456f3a7dd8",
        "text": "Fig. 1. In order to formulate a latent variable model which is similar in spirit to the SOM, we consider\na prior distribution p(u) consisting of a superposition of delta functions located at the nodes of a regular grid\nin latent space. Each node ui is mapped to a corresponding point mi\"y(ui; W) in data space, and forms the\ncentre of a corresponding Gaussian distribution.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591949462890625,
                540.3909912109375,
                385.22491455078125,
                578.27099609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "656542573f31843f2c324cc863d4f28e",
        "text": "The adaptive parameters of the model are W and b. Since the GTM represents\na constrained mixture model, the centres of the Gaussian components cannot be\nadapted to the data independently, but instead are adjusted indirectly through\nchanges to the weight matrix W.\nWe denote the data space variables by x\"x1,2,xD, and we shall assume that the\ndata set has been normalized to zero mean (equivalently we can include a constant\nbasis function /0(u)\"1 in the mapping (1)). Since the GTM represents a parametric\nprobability density model, it can be ﬁtted to a data set MxnN, where n\"1,2,N, by\nmaximum likelihood. The log likelihood function is given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.590911865234375,
                42.57598876953125,
                385.3309020996094,
                148.4509735107422
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "839f52be921e1e0f96f3fb9878fab45d",
        "text": "L(W,b)\" N+\nn/1\nln p(xnDW,b),\n(5)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50294494628906,
                157.7042236328125,
                385.13201904296875,
                187.86422729492188
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "2c02d12f02a1ac88e769c32fe29b868a",
        "text": "where p(xDW, b) is given by Eq. (4), and we have assumed independent, identically\ndistributed data. We can maximize this log likelihood function by ﬁnding expressions\nfor its derivatives and using these in a standard non-linear optimization algorithm\nsuch as conjugate gradients.\nAlternatively, we can exploit the latent-variable structure of the model and use the\nexpectation-maximization (EM) algorithm [3,12]. In the E-step, we use the current\nvalues of the parameters W and b to evaluate the posterior probability, or responsibil-\nity, which each component i takes for every data point xn, which, using Bayes’\ntheorem, is given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.589996337890625,
                191.18423461914062,
                385.346923828125,
                297.0589599609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "eaecdcad727ba8175fa698b93100342e",
        "text": "Rni,p(iDxn)\" p(xnDi)\n+jp(xnD j)\n(6)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50199890136719,
                306.38421630859375,
                385.1319885253906,
                335.8962097167969
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3c61e79d5949f19a1e09f5c4d5f5ec91",
        "text": "in which the prior probabilities P(i)\"1/K have cancelled between numerator and\ndenominator. Using Eq. (3) we can rewrite this in the form",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592010498046875,
                338.0624084472656,
                385.1449279785156,
                362.0747375488281
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "6f5378c6beaf0fa1ad173fe38376fd47",
        "text": "Rni\" expM!b2Emi!xnE2N\n+jexpM!b2Emj!xnE2N.\n(7)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50502014160156,
                369.5263977050781,
                385.13201904296875,
                400.9119873046875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "91c79b309d048d45d5cfff564f8524f2",
        "text": "Then in the M-step we use the responsibilities to re-estimate the weight matrix W by\nsolving the following system of linear equations:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591949462890625,
                417.0187683105469,
                385.08795166015625,
                438.97076416015625
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "e1c049d2c922e8d6c969d2fbf0cc14ee",
        "text": "(UTGU)WT/%8\"UTRX\n(8)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50395202636719,
                446.49444580078125,
                385.1319274902344,
                463.9120178222656
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "97aa1766da8b11d56cd89c15b85edf2c",
        "text": "which follow by maximization of the expected complete-data log likelihood. In Eq. (8)\nU is a K]M matrix with elements Uij\"/j(ui), X is an N]D matrix with elements\nxnk, R is a K]N matrix with elements Rni, and G is a K]K diagonal matrix with\nelements Gii\"+nRni. The inverse variance parameter is also re-estimated in the\nM-step using",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.588951110839844,
                468.0667724609375,
                385.2538757324219,
                525.874755859375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "924d11f5ccec90ce1be79420c18897bc",
        "text": "N+\nn/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                100.89599609375,
                535.0560302734375,
                115.27599334716797,
                565.2160034179688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "087c02d8eb2d409c4e9143d5af8b0ba8",
        "text": "K+\ni/1\nRniEW/%8/(ui)!xnE2.\n(9)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                117.74398803710938,
                535.0560302734375,
                385.1329345703125,
                565.2160034179688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e321a4c3b9a2bbbd2827aeaf8d4cdbb4",
        "text": "1\nb/%8",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.500946044921875,
                535.3148193359375,
                69.15293884277344,
                564.6400146484375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1e10d7e4ffb23b8daf3f1172085fd6c8",
        "text": "\" 1\nND",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                69.07200622558594,
                535.3148193359375,
                97.66799926757812,
                559.2094116210938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "91662e00fdc46041c904d663b771c106",
        "text": "A detailed derivation of the EM algorithm for the GTM can be found in [7].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59295654296875,
                568.7227783203125,
                360.14691162109375,
                578.7227783203125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3dc7f83f93327f9d6611a1b5b4cc90a6",
        "text": "We can initialize the parameters W so that the GTM model initially approximates\nprincipal component analysis (PCA). To do this, we ﬁrst evaluate the data covariance\nmatrix and obtain the eigenvectors corresponding to the q largest eigenvalues, and\nthen we determine W by minimizing the sum-of-squares error between the projections\nof the latent points into data space by the GTM model and the corresponding\nprojections obtained from PCA. The value of b~1 is initialized to be the larger of\neither the q#1 eigenvalue from PCA (representing the variance of the data away\nfrom the PCA sub-space) or the square of half of the grid spacing of the PCA-\nprojected latent points in data space.\nThe latent space of the GTM is generally chosen to have a low dimensionality\n(typically q\"2). Although it is straightforward to formulate the GTM for latent\nspaces of any dimension, the model becomes computationally intractable if q becomes\nlarge, since the number of nodes in the regular grid grows exponentially with q (as\ndoes the number of basis functions). The same problem arises for the SOM. One\napproach to solving this problem is discussed in Section 5.\nThe batch SOM can be related to the GTM by considering the limit in which the\ninverse variance parameter bPR. This is analogous to the relation between a Gaus-\nsian mixture model trained by EM and the K-means algorithm (which can be\nobtained from the Gaussian mixture model by taking the limit in which the compon-\nent variances go to zero). For large data sets in many dimensions, the dominant\ncomputational cost of the GTM arises in the E-step due to the evaluation of the\nquantities Emi!xnE2 corresponding to the Euclidean distances between each refer-\nence vector and each data point. Since this same computation must also be performed\nfor the self-organizing map, the computational eﬃciency of the GTM and the batch\nSOM, for large data sets in high dimensions, are roughly comparable. Many of the\ntechniques used to speed up the learning phase of the SOM can also be adapted to the\nGTM model.\nOne useful modiﬁcation to the standard GTM is to use penalized maximum\nlikelihood by adding a regularization term to the log likelihood in Eq. (5). The\nsimplest example is a quadratic regularizer of the form",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591964721679688,
                42.83476257324219,
                385.3758544921875,
                399.4429016113281
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3b9f2723f207e8b39d8ac7f9577188e1",
        "text": "12aEwE2,\n(10)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50401306152344,
                406.32000732421875,
                385.16400146484375,
                422.1521301269531
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "16cc26e42a8b3c91bbdf79e97bd98d12",
        "text": "where w is a column vector consisting of the concatenation of the successive columns\nof W, and the hyperparameter a is a ﬁxed constant. Techniques for treating a prob-\nabilistically are discussed in Section 6, where the regularizer (10) will be interpreted as\nthe logarithm of a Gaussian prior distribution over the weights. Inclusion of the\nregularizer (10) leads to a simple modiﬁcation to the M-step (8) of the EM algorithm\nto give\nAUTGU#a\nb IBWT/%8\"UTRX,\n(11)",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                424.2907409667969,
                385.3468933105469,
                528.4559936523438
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "1cc8e4ff5d88881e753a87624942f93e",
        "text": "where I is the M]M unit matrix.\nA more complete discussion of the GTM model, and of its relation to the SOM, is\ngiven in [7]. Papers relating to the GTM, and a software implementation of the GTM\nin Matlab, are available from http://www.ncrg.aston.ac.uk/GTM/.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591026306152344,
                532.6080322265625,
                385.15594482421875,
                578.7227783203125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "65643483d3ce2ecf7e17b9604a6e646f",
        "text": "The version of the GTM described in Section 1.1 uses batch learning in which all of\nthe data points are used together to update the model parameters. For large data sets\nthis may become computationally wasteful since the M-step is performed only after all\nof the data points have been considered. Signiﬁcant computational savings could\npotentially be obtained by updating the parameters incrementally using data points\none at a time, or in small batches. This is particularly advantageous if there is\nsigniﬁcant redundancy in the data set. We therefore consider a sequential EM\nalgorithm for the GTM and provide an outline proof of its convergence.\nSuppose that, at a given stage of the algorithm, we have current estimates for MRniN\nas well as for W and b. If the next data point is xm then we can use Eq. (6) to evaluate\nthe corresponding value for R/%8\nmi , while leaving the remaining Rni for nOm un-\nchanged. Then we can revise our estimate of G using G/%8\nii \"Gii#R/%8\nmi !Rmi and\nsimilarly revise our estimate of RX using (RX )/%8\ni\n\"(RX )i#(R/%8\nmi !Rmi)xm. We then\nsolve Eq. (8) to ﬁnd W/%8 and subsequently obtain b/%8 using",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                66.73875427246094,
                385.2678527832031,
                232.2742156982422
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "9891ffb3816f47f4f3ba8d1260fd3276",
        "text": "K+\ni/1\nR/%8\nim EW/%8/(ui)!xmE2! 1\nND",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                119.61599731445312,
                241.72799682617188,
                261.2529602050781,
                271.88800048828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f6e00cc476d352da98cafa6d8e0751af",
        "text": "\"1\nb# 1\nND",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                69.07200622558594,
                241.9867706298828,
                116.31600952148438,
                265.88140869140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6ae5fd86952bb8124ee5a3c45f6c9df5",
        "text": "1\nb/%8",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503936767578125,
                241.98692321777344,
                69.15592956542969,
                271.3121337890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f6be3633fc9b883fb96d16a2fff2cdac",
        "text": "which follows from Eq. (9).\nThis incremental EM algorithm is eﬀectively performing a partial E-step since we\nare updating only one of the MRniN. A general proof that such algorithms still have\nguaranteed convergence properties was demonstrated by Neal and Hinton [27]. Here\nwe give an outline of their proof in the context of the GTM. Consider the function",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                275.8987731933594,
                385.2010192871094,
                333.7067565917969
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "1b0a447539c890142519d21151184f20",
        "text": "F(MRniN, W,b)\"+\nn\n+\ni\nRni lnG\n1\nKp(xDi, W, b)H!+\nn\n+\ni\nRni ln Rni,\n(13)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50398254394531,
                343.5787658691406,
                385.16400146484375,
                373.47998046875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "78414864af7bc470677077a18113368c",
        "text": "in which the MRniN are regarded as arbitrary non-negative numbers satisfying\n+iRni\"1 for all n. The quantity F is analogous to the (negative) free energy in\nstatistical physics. If we maximize Eq. (13) with respect to the Rni, using Lagrange\nmultipliers to take account of the summation constraints, we obtain the result (6). If\nwe then subsequently maximize over W and b keeping the Rni ﬁxed, we recover the\nstandard M-step equations (8) and (9). Our partial E-step corresponds to maximizing\nF with respect to Rmi while keeping the remaining Rni for nOm ﬁxed. It is easily\nshown [27] that a (local or global) maximum of F corresponds to a (local or global)\nmaximum of the true log likelihood. Thus, our algorithm is guaranteed to increase\nF until we reach a maximum likelihood solution.\nComparison of the incremental EM algorithm with the standard batch approach\nfor a simple Gaussian mixture model by Neal and Hinton [27] demonstrated the\npotential for substantial improvements in speed of convergence. In the case of the\nGTM, each M-step requires the solution of a set of coupled linear equations given by\nEq. (8) and the computational cost of doing so may oﬀset much of the gain of using an\nincremental approach involving data points considered one at a time. This is easily\nresolved by taking batches of data points and using these to update the corresponding",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                377.23199462890625,
                385.3359680175781,
                578.7227172851562
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "bcd56021dbd213e838f8f9152981d5ea",
        "text": "K+\ni/1\nRimEW/(ui)!xmE2\n(12)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                264.4800109863281,
                241.72799682617188,
                385.1639709472656,
                271.88800048828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5141d5ca6d495c1262949a188c3cd678",
        "text": "responsibilities before performing the M-step, thereby keeping the overhead of the\nM-step small while still ensuring that the overall cost of one EM cycle does not scale\nwith the size of the data set. Again, at each iteration the function F in Eq. (13) is\nincreased, thereby providing a guarantee of convergence. For suﬃciently large data\nsets it will always be computationally eﬃcient to use an incremental approach, for\nwhich there will exist an optimal batch size.\nWe see that the initialization of the Rni does not have to be consistent with the\ninitial values of W and b, so that we can, for instance, simply set all of the Rni\"1/K.\nOther variations of the EM algorithm are also possible. For instance, it will often be\nthe case that many of the responsibilities take very small values, particularly in later\nstages of the optimization. If these values are frozen (i.e. not recomputed when the\ncorresponding data points are presented) then the analysis based on Eq. (13) again\nshows that the value of F will not decrease and so a stable algorithm will result.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                42.83476257324219,
                385.2550048828125,
                196.952392578125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "c688e865867015cafc7fbe5ecede604d",
        "text": "3. A manifold-aligned noise model",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.594039916992188,
                222.5639190673828,
                171.798095703125,
                232.5639190673828
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ca19b7676ebf4c556da63bd3c1656963",
        "text": "The noise model (3) was introduced primarily to account for the variance of the\ndata away from the underlying non-Euclidean manifold. However, since the latent\npoints are discrete it also has to account for variance locally along the directions of the\nmanifold. Depending on the distribution of the data, and the density of points on the\nmanifold, these variances may have quite diﬀerent values. We can accommodate this\neﬀect by generalizing Eq. (3) to allow for diﬀerent variances in directions which are\n(locally) parallel and perpendicular to the manifold, as illustrated in Fig. 2. In\nparticular, we would like to ensure that the variance of the noise distribution in\ndirections tangential to the manifold is never signiﬁcantly less than the square of the\ntypical distance between neighbouring nodes, so that there is a smooth distribution\nalong the manifold even when the noise variance perpendicular to the manifold\nbecomes small.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.594039916992188,
                246.4509735107422,
                385.36297607421875,
                387.92291259765625
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "89f34571bc5ad9f07947fd74a0052151",
        "text": "Fig. 2. Illustration of the generalization of the noise model to allow for diﬀerent variances parallel and\nperpendicular to the GTM manifold. The top ﬁgure shows the standard GTM model, with the manifold\nshown as a curve and the Gaussian component densities represented as circles. In the bottom ﬁgure the\nnoise model is generalized to a manifold-aligned non-isotropic covariance model.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                540.3909912109375,
                385.2496337890625,
                578.27099609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "17f4114322e2afad6f117c12388bb7e0",
        "text": "We can construct a suitable covariance matrix as follows. The derivatives of the\nmapping function y(u;W) with respect to the latent space coordinates u1,2,uq repres-\nent linearly independent vectors lying tangentially to the manifold at the point u. The\ncovariance matrix can then be constructed in the form",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                42.83476257324219,
                385.2220764160156,
                88.69084167480469
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "96aaa1fe01cc2be7c53d9e87e9055610",
        "text": "LyT\nLuil",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                133.44000244140625,
                96.3599853515625,
                147.75601196289062,
                126.0880126953125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1661d3e400e8ae0e197b50fcf1a85164",
        "text": "Ci\"1\nbI#g q+\nl/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50505065917969,
                96.57598876953125,
                114.9999771118164,
                126.73602294921875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f08e91939805526bea0d4036cb94bdf0",
        "text": "Ly\nLuil",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                117.45597839355469,
                96.5760498046875,
                131.84397888183594,
                126.08807373046875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c74153fbf130460fc35ff355a35bf307",
        "text": ",\n(14)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                147.6959991455078,
                103.81877136230469,
                385.16400146484375,
                113.81877136230469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e64ab93bb7f80125a8d1a83bf8b52c0f",
        "text": "where uil is the lth component of ui, and g is a scaling factor equal to (some multiple of)\nthe distance between neighbouring nodes in latent space. The required derivatives are\neasily calculated since",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                128.54400634765625,
                385.2230529785156,
                162.7068328857422
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "0247b90fddafeace3e946fe61f66341d",
        "text": "Ly\nLuil",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.5040283203125,
                170.52005004882812,
                65.82002258300781,
                200.10403442382812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8525edc025eefe80e8c6c69e53837658",
        "text": "\"Wwil,\n(15)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                65.83200073242188,
                175.70240783691406,
                385.1639709472656,
                193.1199951171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "26f105e7ac1c1120550b3dae8102079a",
        "text": "where til are the (ﬁxed) partial derivatives of the basis functions /(ui) with respect\nto uil.\nThis modiﬁcation to the model results in a more complex E-step since the compon-\nent densities of the mixture distribution take the form",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                202.48800659179688,
                385.2419738769531,
                248.60276794433594
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "dad18c4a5d05996f4b7e16dc3a4eced8",
        "text": "p(xDi)\"A\n1\n2nB\nD@2DCiD~1@2 expG!1\n2 (mi!x)TC~1\ni\n(mi!x)H\n(16)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50495910644531,
                256.20001220703125,
                385.1639709472656,
                283.72802734375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "cc555660ededf79a7bfd673920a8cb80",
        "text": "and hence require that the inverse and the determinant of each covariance matrix be\nevaluated for each latent point. The inverse is eﬃciently computed using q successive",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                288.7147521972656,
                385.260009765625,
                310.666748046875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "9df4c2df0855eb800fb41974282d8cd8",
        "text": "Fig. 3. Application of the manifold-aligned noise model to a toy problem. The plots show the data space,\nwith the training data plotted as, \" and the GTM manifold shown as a curve connecting the mixture\ncomponent centres. Mixture components are plotted as ellipsoids, corresponding to unit Mahalanobis\ndistance, with ‘#’ marking the centres. The left-hand plot shows a standard GTM model (giving a log\nlikelihood of !58.9) while the right-hand plot shows the modiﬁed form of the GTM having a manifold-\naligned noise model (giving a log likelihood of !48.3).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                520.447021484375,
                385.32159423828125,
                578.27099609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "1d5512497684dd9492cae3d067a75eb4",
        "text": "(A#T)~1\"A~1!(A~1)(TA~1)\n1#TA~1 ,\n(17)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503997802734375,
                59.42241287231445,
                385.1639709472656,
                85.61009216308594
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "f7672d6a94194d0d9561d1e323c4b38a",
        "text": "which is easily veriﬁed by multiplying both sides by (A#T). Similarly, the M-step\nequations become more complex since the covariance matrices now depend on the\nweight matrix W. For the W-update we therefore approximate the re-estimation\nformulae for W by replacing C with b~1I thereby recovering the standard M-step\nupdates (8) and (9).\nAs an illustration of this model we use a 1D toy problem in 2D, similar to the one\nused in [7]. The training data, together with the converged model, are shown in Fig. 3.\nThis version of the GTM has some similarities to the adaptive subspace SOM\nmodel [20], since each mixture component now represents a local linear sub-space.\nA related form of Gaussian mixture model, with general covariance matrices and\nconstrained centres, was described by Williams et al. [39].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591934204101562,
                92.03843688964844,
                385.2899169921875,
                223.61891174316406
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "62e218b8900aaabf5957c300da0f1665",
        "text": "4. Discrete data",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591934204101562,
                250.2118377685547,
                95.32394409179688,
                260.21185302734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e816352e9978e39cf6e05f27d8777190",
        "text": "The original version of the GTM, as discussed in Section 1, was formulated for the\ncase of data variables which are continuous.2 We now extend the model to account for\ndiscrete data and for combinations of discrete and continuous variables. Consider ﬁrst\nthe case of a set of binary data variables xk3M0,1N. As for the case of continuous\nvariables, we assume that the components of x are conditionally independent, given\nthe latent space label i. We can then express the conditional distribution of the binary\nvector x, given i, using a binomial (Bernoulli) distribution of the form",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.5919189453125,
                274.098876953125,
                385.259765625,
                355.9701843261719
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "58e278551db18f0ec26f4897f365ab98",
        "text": "p(xDi)\"<\nk\nmxkik(1!mk)1~xk,\n(18)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50395202636719,
                369.45440673828125,
                385.16400146484375,
                394.4320983886719
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "74b4977eaa29b7700a06b989628c7c1a",
        "text": "where\nthe\nconditional\nmeans\nmik\nare\ngiven\nby\nmik\"p(wTk/(ui)),\np(a)\"\n(1#exp(!a))~1 is the logistic sigmoid function, and wk is the kth column of W. Note\nthat in Eq. (18) there is no analogue of the noise parameter b.\nNext, suppose instead that the D data variables represent membership of one of\nD mutually exclusive classes. Again, the data values are binary, but for a given pattern\nall values are zero except for one component which identiﬁes the class (this is called\na 1-of-D coding scheme). In this case we can represent the conditional distribution of\nthe data variables using a multi-nomial distribution [3] of the form",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                395.08642578125,
                385.2529602050781,
                490.81072998046875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "733bcddb74df7a76994f25e7f53acc73",
        "text": "p(xDi)\" D<\nk/1\nmxkik,\n(19)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.505889892578125,
                499.05596923828125,
                385.16400146484375,
                529.2159423828125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "94547137015936595e8101c34f6823ff",
        "text": "2The SOM model is also formulated for continuous variables.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                34.58399963378906,
                570.06396484375,
                249.34559631347656,
                578.27099609375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "98389a340fee901fefc9c99524c6a0c0",
        "text": "where mik are deﬁned by a softmax, or normalized exponential, transformation [3] of\nthe form",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                42.761417388916016,
                385.1899108886719,
                64.78678894042969
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3e0f0e625d66138fcb1fd0b0feff8178",
        "text": "mik\" exp(wTk/(ui))\n+j exp(wTk/(uj)).\n(20)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503997802734375,
                73.176025390625,
                385.16400146484375,
                103.23478698730469
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "50b14dc0d9e83e1e9cf4062a97f69c78",
        "text": "Finally, if we have a data set consisting of a combination of continuous, binary and\ncategorical variables, we can formulate the appropriate model by writing the condi-\ntional distribution p(xDi) as a product of Gaussian, binomial and multi-nomial distri-\nbutions. This represents the standard conditional independence framework (used in\nmany latent variable models) in which the observed variables are independent given\nthe latent variables.\nWe can again estimate the parameters in such models using the EM algorithm. The\nE-step again takes the form (6). However, the M-step now requires non-linear\noptimization, although this may be performed eﬃciently using the iterative re-\nweighted least-squares (IRLS) algorithm [25]. Note that it is not necessary to perform\nan exact optimization in the M-step, and indeed it will typically be computationally\neﬃcient to perform only a partial optimization, corresponding to the generalized EM\n(GEM) algorithm [12].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                118.50675964355469,
                385.4688720703125,
                271.9307861328125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "9d3bee56feedebd9ea0e888c574c601e",
        "text": "5. A semi-linear model",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59302520751953,
                298.8117370605469,
                123.77205657958984,
                308.8117370605469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "804f40ef4ff3b03fc3912d772f815ea7",
        "text": "We have already noted that the computational cost of the standard GTM grows\nexponentially with the number of latent dimensions (as is also the case for the SOM).\nOne approach to dealing with high-dimensional latent spaces would simply be to\nconsider a random sampling of the latent space, as used by MacKay [23] in the\n‘density network’ model. However, such sampling eﬀectively becomes very sparse as\nthe dimensionality of the latent space increases, so again this approach is limited to\nlow values of q.\nAn alternative approach is to introduce a semi-linear model, in which the data\nvariables depend non-linearly on a small number of dimensions of latent space, and\ndepend linearly on the remaining dimensions. Linear latent variable models include\nfactor analysis [15] and probabilistic principal component analysis [35]. The GTM\ncan be regarded as one possible non-linear generalization of such models.\nFirst, suppose we consider a model in which the data variables are purely linear\nfunctions of the latent variables, so that Eq. (1) becomes y\"Vu#l in which V is\na D]q matrix, and for convenience we have introduced an explicit mean vector l.\nInstead of using a ﬁnite discrete grid in latent space, we can consider a prior\ndistribution over u given by a zero mean, unit covariance Gaussian. The marginal\ndistribution of the data variables is then given by the convolution of two Gaussian\nfunctions and can be evaluated analytically, with the results that the overall density\nmodel is a Gaussian with mean l and covariance",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59294891357422,
                322.6987609863281,
                385.3089294433594,
                559.7867431640625
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "ce6def5406724faf10e0541a6cd8e868",
        "text": "C\"b~1I#VVT.\n(21)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.504966735839844,
                566.662353515625,
                385.1649475097656,
                578.8820190429688
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3717650ff889a42daad217ab69e62eca",
        "text": "An important property of this model, demonstrated by Tipping and Bishop [34,35],\nis that the maximum likelihood solution for V, b and l can be found in closed form.\nThe solution for lML is straightforward and is given by the sample mean. If we now\nintroduce the sample covariance matrix given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                42.83476257324219,
                385.3299560546875,
                88.69084167480469
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "9174d11d8523486a3f30a9391c4b9e3b",
        "text": "N+\nn/1\n(xn!lML)(xn!lML)T,\n(22)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                81.60000610351562,
                95.35198974609375,
                385.16400146484375,
                125.51202392578125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "63619459999947ca36be64b0e57b361d",
        "text": "S\"1\nN",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.5040283203125,
                95.61082458496094,
                78.38204193115234,
                119.43347930908203
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "96d6b6adcea37eb5fc1f8118ac44da5f",
        "text": "then the maximum likelihood solution for V is given by",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592010498046875,
                126.49876403808594,
                266.4330139160156,
                136.65806579589844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5ee6444fe17917b10bf178fe8ef7350d",
        "text": "VML\"U(K!b~1I)1@2,\n(23)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50404357910156,
                141.35838317871094,
                385.1650390625,
                158.7039794921875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "e79083b430e561bf053f37360c56808f",
        "text": "where U is a D]q matrix whose columns are the principal eigenvectors of S (i.e. the\neigenvectors corresponding to the q largest eigenvalues) with corresponding eigen-\nvalues in the diagonal q]q matrix K. Finally the maximum-likelihood estimator of\nb is given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.593048095703125,
                160.07998657226562,
                385.2099304199219,
                206.1947479248047
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "acae5b136736439fa5a2cf2b1955ee8e",
        "text": "D+\nj/q`1\njj\n(24)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.67201232910156,
                212.85598754882812,
                385.16400146484375,
                243.0159912109375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7d503af3e0171637abadc8427cfd545d",
        "text": "1\nbML",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50505065917969,
                213.11476135253906,
                66.8770523071289,
                242.36798095703125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "64ebbc77c1fdec9fbc029ca578844a49",
        "text": "\"\n1\nD!q",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                66.83999633789062,
                213.11476135253906,
                105.31999206542969,
                237.0093994140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7e99543fa2f8fa4fdbaf9ce34afedad9",
        "text": "where we have ordered the eigenvalues such that j15j2525jD. The result (24)\nhas a clear interpretation as the variance ‘lost’ in the projection, averaged over the lost\ndimensions. This model therefore represents a probabilistic formulation of standard\nprincipal components analysis.\nNow we consider a semi-linear formulation of the GTM in which the data variables\ndepend non-linearly on a few discretized latent variables and linearly on the remain-\ning Gaussian latent variables. Marginalizing over all of the latent variables we obtain\nthe following density model:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                241.94239807128906,
                385.3049011230469,
                337.6667175292969
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "4c31e404d9649e690769eec98c9ad615",
        "text": "1\nKA\n1\n2pB\nD@2DCD~1@2 expG!1\n2(x!mi)TC~1(x!mi)H,\n(25)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                126.45700073242188,
                344.11199951171875,
                385.1639709472656,
                371.5680236816406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4487a433d8168e6bfd1022d857cc489b",
        "text": "p(xDW,V, b)\" K+\ni/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503997802734375,
                344.32794189453125,
                124.0009994506836,
                374.4879455566406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fd8e5239bc2ac83697e0b33e91a497f6",
        "text": "where C is given by Eq. (21). The density model (25) can be interpreted as a mixture of\nprobabilistic PCA models with equal covariance matrices and with means mi lying on\nthe GTM manifold. In order to maximize the corresponding log likelihood, we could\ntreat both the discretized and the continuous latent variables as jointly missing data\nand apply the EM algorithm. However, we can make use of the above result for\nprobabilistic PCA by treating only the discrete latent variables (corresponding to the\n‘non-linear dimensions’) as missing. The E-step of the corresponding EM algorithm\ninvolves the evaluation of the responsibilities Rni which are given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                375.4747619628906,
                385.3479309082031,
                474.7547302246094
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "0f007d8e589e76e235bfe00ecaf44f0b",
        "text": "Rni\" expM!12(xn!mi)TC~1(xn!mi)N\n+j expM!12(xn!mj)TC~1(xn!mj)N.\n(26)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503997802734375,
                473.9984130859375,
                385.16400146484375,
                505.5707702636719
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3f08d7920842ab23c11fe0c344ce52f3",
        "text": "In the M-step we must maximize the expected complete-data log likelihood [3,12]\ngiven by",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592010498046875,
                506.9467468261719,
                385.1958923339844,
                528.8988037109375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "d57f8a1191e93bc4ff64129a25723ef7",
        "text": "SLCT\" N+\nn/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50401306152344,
                536.5679931640625,
                104.33199310302734,
                566.7279663085938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0c64b9620f40db2d8b02bfde28b356b6",
        "text": "K+\ni/1\nRniG!ln K!D\n2 ln(2p)!1\n2ln D CD!1\n2(xn!mi)TC~1(xn!mi)H.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                106.79998779296875,
                536.5679931640625,
                378.7740173339844,
                566.7279663085938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b2816fdc000a3a2af6d4545ae7d6fb8c",
        "text": "(27)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                369.7440185546875,
                568.7227783203125,
                385.16400146484375,
                578.7227783203125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "75b30bcbc60c8104e845800fff89d51e",
        "text": "Thus, we see that SLCT depends on the data only through the weighted covariance\nmatrix",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                42.57598876953125,
                385.3258972167969,
                64.78678894042969
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "8f5454cf90835370ac1bbf9c3ef3c688",
        "text": "S\" N+\nn/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.5040283203125,
                72.88800048828125,
                84.53202056884766,
                103.1199951171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "accad6caea52da33cc1f6eddcd01c676",
        "text": "K+\ni/1\nRni(xn!mi)(xn!mi)T.\n(28)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                87.00003051757812,
                72.88800048828125,
                385.16400146484375,
                103.1199951171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "45e2d1ccbf38b166d151e8ed83f8e0a9",
        "text": "Maximizing Eq. (27) jointly over W, V and b, for ﬁxed Rni, can then be accomplished\nas follows. We ﬁrst note that the maximum over W does not depend V or b and is\ngiven by the solution to Eq. (8). We now use this new value for W to evaluate MmiN and\nhence evaluate S given by Eq. (28). Finally, we can ﬁnd the eigenvector/eigenvalue\ndecomposition of S and use this to solve for V and b using Eqs. (23) and (24).\nWe have seen that, in the probabilistic PCA model, the solutions for V and b have\nexplicit, closed-form solutions. However, it was noted by Tipping and Bishop [33]\nthat, for problems in which the dimensionality D of the data space is high, it may be\nmore eﬃcient to treat the continuous latent variables as missing data and apply the\nEM algorithm. Although this results in an iterative optimization scheme, each step\nrequires O(ND) operations, compared with the O(ND2) operations needed to evaluate\nthe covariance matrix. Provided the number of iterations of EM needed to reach\nsatisfactory convergence is suﬃciently smaller than D, there can be an overall\ncomputational saving, which typically improves as D increases. A derivation and\ndiscussion of this EM algorithm is given in Tipping and Bishop [35].\nAs a demonstration of this model, a simple data set was generated in a three-\ndimensional space. The ﬁrst two variables, x1 and x2 were discretized over a rectangu-\nlar grid, while the third variable, x3, was computed from x1 and x2 with the formula",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.5919189453125,
                105.14398193359375,
                385.5638427734375,
                324.2027893066406
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "02ac27239e85892f8519056582258491",
        "text": "x3\"x2#0.5sin(0.5px1)\n(29)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50390625,
                324.814453125,
                385.1639099121094,
                342.2320251464844
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3c02c29539bfc718fee7700b2c204a4b",
        "text": "so that x3 depends linearly on x2 and non-linearly on x1. Gaussian noise was then\nadded to x1,x2 and x3. A semi-linear GTM with one non-linear latent variable (using",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.5919189453125,
                345.16143798828125,
                385.2569274902344,
                372.73077392578125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "ac7f596252af29607c553ded5c570d35",
        "text": "Fig. 4. Demonstration of the semi-linear model. The left plot shows a two-dimensional manifold embedded\nin data space, along with the data set generated by sampling points on the manifold and adding Gaussian\nnoise. The right-hand plot shows the result of ﬁtting a semi-linear GTM model having one non-linear latent\ndimension with 10 latent points, and one linear dimension. The mixture components are plotted as\nellipsoids corresponding to unit Mahalanobis distance.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 12,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                530.3829956054688,
                385.288818359375,
                578.27099609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "4e5a4aa5f03789855da7614e0567c6a5",
        "text": "10 nodes and 5 basis functions) and one linear latent variable was trained on this data\nset, starting from a PCA initialization. The trained model, shown in Fig. 4, captures\nthe structure of the data well. Note that this model appears to be fairly sensitive to the\ninitialization of its parameters, and is relatively prone to ﬁnding local minima.\nThe semi-linear model for the latent space distribution can easily be combined with\nthe type of mixed discrete-continuous distributions for the data space distribution\ndiscussed in Section 4.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                42.83476257324219,
                385.3608703613281,
                124.54692077636719
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "905560394c7e6cf0082c51fb4947c879",
        "text": "6. Bayesian inference for hyperparameters",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                152.0038604736328,
                205.633056640625,
                162.0038604736328
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "871608ba82c18418ea33760c2ead45ff",
        "text": "An important issue in maximum likelihood density estimation is that of model\ncomplexity, which in the context of the GTM is determined in large part by the\n‘stiﬀness’ of the manifold. A more ﬂexible manifold can provide a better ﬁt to the\ntraining data, but if the eﬀective complexity is too high the model may adapt too\nclosely to the speciﬁc data set and thereby give a poorer representation of the\nunderlying distribution from which the data was generated (a phenomenon known as\noverﬁtting).\nThe eﬀective model complexity in the GTM is controlled by the number and form\nof the basis functions as well as by the regularization coeﬃcient. Although it would be\npossible to explore a range of model complexities by altering the number of basis\nfunctions, it is computationally more convenient to arrange for the complexity to be\ngoverned by one or more real-valued parameters, and to explore the corresponding\ncontinuous space. We shall denote such parameters generically by p, which might, for\nexample, represent a common width parameter in the case of Gaussian basic func-\ntions.\nIn the discussion of the GTM in Section 1.1 the parameters W and b were estimated\nfrom the data using maximum (penalized) likelihood, while the regularization coeﬃc-\nient a (as well as any parameters governing the basis functions) was assumed to be\nconstant. Since the GTM represents a probabilistic model, it oﬀers the possibility of\na more comprehensive probabilistic treatment using a Bayesian formalism.\nIn this section it will be convenient to introduce a column vector w consisting of the\nconcatenation of the successive columns of W. From Eqs. (4) and (5) the log likelihood\nfunction for the GTM is given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591079711914062,
                175.8909149169922,
                385.3219909667969,
                448.8348083496094
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "c509bcaee089ba163fe485460d49198d",
        "text": "L(w,b, p)\"ln p(MxNDw, b, p)\" N+\nn/1\nlnG\nN+\ni/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.503082275390625,
                458.0880126953125,
                223.86404418945312,
                488.320068359375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8a3b95ce3c5e8e7e1847506ee3ac3265",
        "text": "We now introduce a prior distribution over the weights, which for simplicity we\nchoose to be an isotropic Gaussian distribution of the form",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                491.8987731933594,
                385.1459655761719,
                513.8507690429688
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "ef1682ed8c0211904b65e61b8877bf6d",
        "text": "p(wDa)\"A\na\n2pB\nW@2expG!a\n2EwE2H,\n(31)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50398254394531,
                522.8880004882812,
                385.16400146484375,
                550.343994140625
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "1fa33b29ff118ac03290255e3e257cb3",
        "text": "where ¼ is the total number of elements in w. Since a controls the distribution of\nother parameters it is often called a hyperparameter, and we shall similarly use this",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                554.7103881835938,
                385.1798095703125,
                578.7227783203125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "658e7c57fcd0ef5b87760c8ed232fd08",
        "text": "1\nKA\nb\n2pB\nD@2expC!b\n2Emi!xE2DH.\n(30)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 13,
            "languages": [
                "eng"
            ],
            "coordinates": [
                226.176025390625,
                457.87200927734375,
                385.1639709472656,
                485.4000244140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f53fad5b60922f299ed5cdb101b64f95",
        "text": "terminology to describe b and p also. A full Bayesian treatment would involve the\nintroduction of prior distributions over a, b and p followed by a marginalization over\nall the parameters and hyperparameters in the model. Instead we estimate values for\nthe hyperparameters by maximizing their marginal likelihood p(MxnNDa, b, p) in which\nwe have integrated over w. This corresponds to the type-II maximum likelihood\nprocedure [2], also known as the evidence approximation [3,21]. Utsugi [36] applies\na similar Bayesian treatment to a generalized form of the elastic net [13,14], which is\nalso a probabilistic model having close connections to the SOM.\nThe marginal likelihood for the GTM model is given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591964721679688,
                42.57598876953125,
                385.28582763671875,
                148.4509735107422
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "40dbb07c41db4bdfa51614aea497476d",
        "text": "p(MxnNDa, b, p)\"Pp(MxnNDw, b, p)p(wDa)dw.\n(32)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.504920959472656,
                157.86399841308594,
                385.16802978515625,
                183.86399841308594
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "ddc88f6bf419741d638de84d722df573",
        "text": "Since this integral is analytically intractable, we follow MacKay [22] and make a local\nGaussian approximation to the posterior distribution over w in the neighborhood of\na mode. Maximizing the posterior distribution (for given values of a, b and p)\ncorresponds to maximizing the penalized log likelihood, and the solution for w was\ngiven in Eq. (11). Suppose we have found a maximum wH of the posterior distribution.\nIf we deﬁne",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59600830078125,
                189.21080017089844,
                385.2388916015625,
                258.97076416015625
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "7ebf6573a2eb3f93fd590131cc300ea3",
        "text": "S(w, a, b, p)\"!lnMp(MxnNDw, b, p)p(wDa)N\n(33)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.508056640625,
                265.19842529296875,
                385.174072265625,
                282.6159973144531
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "8ecf9b4c3c5ed373963747331ea01d68",
        "text": "then we can write Eq. (32) in the form",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.602081298828125,
                285.5467529296875,
                190.84616088867188,
                295.5467529296875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c4f42d69505e8f6fe0e5cde3d06c9a5a",
        "text": "p(MxnNDa, b, p)\"PexpM!S(w,a, b, p)N dw",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.51408386230469,
                304.88800048828125,
                224.0399627685547,
                330.88800048828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "253e48675bcc7affae25284991592be3",
        "text": "KexpM!S(wH, a, b, p)NPexpG!1\n2(w!wH)TA(w!wH)Hdw",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.02597045898438,
                336.09075927734375,
                359.75799560546875,
                363.1440124511719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9db7260e4c9d582316f898df2f58d034",
        "text": "\"expM!S(wH, a, b, p)N(2p)W@2DAD~1@2,\n(34)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.02398681640625,
                366.28643798828125,
                385.16705322265625,
                383.7040100097656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0b1c7d3c2e12360588ed16b208b91b8a",
        "text": "where we have performed a Taylor expansion of the logarithm of the integrand and\nretained terms up to second order. Note that the ﬁrst-order terms vanish since the\nintegrand is proportional to the posterior distribution, through Bayes’ theorem, and\nwe are at a local maximum. We have also introduced the Hessian matrix A given by\nthe second derivatives of S with respect to the elements of w, evaluated at wH. Making\nuse of Eqs. (30) and (31) we then obtain the log-evidence for p, a and b in the form",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59503173828125,
                386.634765625,
                385.2799987792969,
                456.3947448730469
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "26a59449f8d49498d25fbac5d7a32ce1",
        "text": "ln p(MxnNDa, b, p)\"L(wH, b, p)!a\n2EwHE2!1\n2lnDAD#¼\n2 ln a.\n(35)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.506988525390625,
                462.6944274902344,
                385.16400146484375,
                488.6507568359375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3705820fb9ece38502a2bc7b457d3ee0",
        "text": "Although the Hessian matrix can be calculated exactly, the resulting expression is\ncomputationally expensive to evaluate. Here we consider an approximation obtained\nby neglecting terms involving derivatives of the responsibilities Rni with respect to w.\nThis approximation becomes exact when the responsibility for each data point n is\ntaken by just one of the mixture components (as is often eﬀectively the case during the\nlater stages of GTM training) so that Rni 3 M0,1N. The Hessian matrix then takes\na block diagonal form (with one block corresponding to each column from the",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 14,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592010498046875,
                497.0107727050781,
                385.2989807128906,
                578.7227783203125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "b39bd4f6c395cfb7c3b65d627f0dd368",
        "text": "original W matrix) in which all blocks are identical and have the form bUTGU#aI.\nNote that this expression will already have been evaluated for use in the regularized\nM-step (11).\nWe can maximize Eq. (35) with respect to a and b by setting the respective\nderivatives to zero, yielding the update formulae",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                40.7744026184082,
                385.30804443359375,
                100.64286804199219
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "8ad9aa4e9f71de14f1b9a5fab04cd682",
        "text": "a\"\nc\nEwHE2\n(36)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50506591796875,
                109.0321044921875,
                385.1639709472656,
                138.54412841796875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "4556b4bda4af00f7a5efdb54f974724b",
        "text": "and",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                141.9067840576172,
                42.63198471069336,
                151.9067840576172
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ac22d4927bd29298175ae58d6ef5c9d7",
        "text": "b\"\nND!c\n+n+i RniExn!miE2,\n(37)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50398254394531,
                155.5424041748047,
                385.16400146484375,
                187.04141235351562
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "6745153798b6878b7cd17ee34c7735b4",
        "text": "where we have deﬁned",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592010498046875,
                190.21876525878906,
                123.89004516601562,
                200.21876525878906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c8389071a7f38da1da54b07f9aa2bfac",
        "text": "ji!a\nji",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                84.62400817871094,
                206.80638122558594,
                110.0,
                238.19195556640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ecea3962126b4cfc986c0a8ba6a34193",
        "text": "c\" W+\ni/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50401306152344,
                208.60797119140625,
                82.16800689697266,
                238.76797485351562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "94c553e700b177686fd89c7e23aae2db",
        "text": "and ji are the eigenvalues of A. Note that we have neglected terms involving\nderivatives of wH with respect to a and b. Comparison of Eq. (37) with the correspond-\ning maximum likelihood update (9) shows that they have identical form except for the\nappearance of c which can be interpreted as the eﬀective number of w-parameters in\nthe model [21].\nIn a practical implementation, maximization with respect to W using the EM\nalgorithm is interleaved with re-estimation of a and b. Since the dependence of the\nmarginal log likelihood on p is more complex we do not obtain a simple re-estimation\nformula, but since we are now down to a single variable, we can simply evaluate\nEq. (35) for a range of diﬀerent p values, while estimating a and b on-line, and select\nthe model with the highest log-evidence score.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                241.15200805664062,
                385.3507995605469,
                370.93072509765625
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "6cb5197260693088ff8a6af18e77aa6c",
        "text": "Fig. 5. Log-evidence and log-likelihood plotted against log2(p). Each plot shows the individual results for\nthe 20 data sets after training, together with a line plot summarizing the mean. The log-likelihood plot\nshows results for the training and test sets (normalized by the respective size of the data sets).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591964721679688,
                550.1199951171875,
                385.1375732421875,
                578.27099609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "204b4c31e59f32da3d4a2890d79592f1",
        "text": "(38)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 15,
            "languages": [
                "eng"
            ],
            "coordinates": [
                369.7439880371094,
                215.8507843017578,
                385.1639709472656,
                225.8507843017578
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "497ea69c63796984c0ea40c4c90bf14f",
        "text": "To evaluate this method, synthetic data was generated from a curved 2D manifold\nin a 3D space. Twenty data sets were generated by adding random Gaussian noise\nwith standard deviation 0.2 to 400 points drawn from a regular grid on the manifold.\nA corresponding test data set of 1024 points was also generated. A GTM model, with\na 15]15 latent grid and a 5]5 grid of Gaussian basis functions with common width\nparameter p, was trained on the 20 data sets, each time starting from a PCA\ninitialization. The plots in Fig. 5 show the log-evidence and log-likelihoods after\ntraining, plotted against log2(p). The data generating manifold is shown in the top left\npanel of Fig. 6, together with a sample data set. Fig. 6 also shows an example of the\nmodel with the highest log-evidence (p\"1), together examples of models in which p is\nﬁxed to values which are either too small or too large.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59100341796875,
                42.83476257324219,
                385.31494140625,
                172.35499572753906
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "3a9be4e918f8978d44dabe9d38d87961",
        "text": "Fig. 6. Illustration of Bayesian parameter re-estimation. The data generating manifold is shown in the top\nleft plot, together with a sample data set. The top right plot shows the manifold of a GTM model trained on\nthis data set, with p ﬁxed to 1 and a and b being re-estimated during training, ﬁnal values being a\"9.2 and\nb\"18.3. The bottom left plot shows a signiﬁcantly more ﬂexible model, p\"0.25, trained using the\nstandard GTM and no weight regularization; the ﬁnal estimated value for b was 40.9. The bottom right\nplot, shows a much stiﬀer model, p\"2, trained using the standard GTM and constant weight regulariz-\nation of 50; the ﬁnal estimated value for b was 10.1.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 16,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                510.510986328125,
                385.26470947265625,
                578.27099609375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "a8295d78d59a3c8711b2e20b6c002410",
        "text": "In the original GTM, described in Section 1.1, there is a hard constraint on the form\nof the latent-space to data-space mapping due to the ﬁnite number of basis functions\nused, as well as a soft constraint due to the regularization term (10). An alternative\napproach is to enforce the smoothness of this mapping entirely through regulariz-\nation, using a Gaussian process prior over functions. For each dimension j in the data\nspace ( j\"1,2,D), let m(j) be a vector of length K consisting of the jth components of\nm1 through mK, so that if the column vectors mi are arranged side-by-side, one obtains\nthe D ] K matrix M in which m(j) is the jth row of this matrix. Consider a Gaussian\nprior distribution on the centre locations given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                66.73875427246094,
                385.3019714355469,
                172.3549346923828
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "f4ccf4ca73bbde3235dfebfd99168aa9",
        "text": "p(M)\" D<\nj/1",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50505065917969,
                182.97616577148438,
                97.59305572509766,
                213.13616943359375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "509ada68744eba07846cf4ea31a7f5e8",
        "text": "1\n(2p)K@2DB(j)D1@2 expG!1\n2m(j)T(B(j))~1m(j)H\n(39)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                100.03306579589844,
                183.23475646972656,
                385.1639709472656,
                210.2159881591797
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6fe13feec4d1becb9f707ce4a363f21a",
        "text": "where the B(j)’s are positive-deﬁnite matrices. In practice, it will usually not be\nnecessary to use diﬀerent covariance matrices for each dimension, and the B(j)’s will be\ndenoted generically by B.\nThe EM algorithm is now used to maximize the penalized log likelihood",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                217.60800170898438,
                385.1289367675781,
                263.9387512207031
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "5b0415d5e0a99afcd01a59ccdc151639",
        "text": "L1(M, b)\" N+\nn/1\nln p(xnDM, b)#ln p(M).\n(40)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50398254394531,
                274.6319885253906,
                385.166015625,
                304.7919921875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "ba1760ac7068bc317524786a46cc7547",
        "text": "In the E-step, the usual responsibilities are calculated. With these ﬁxed, the M-step\ninvolves the inversion of a K]K matrix (where K is the number of latent points). This\nshould be contrasted with Eq. (8) which involves the inversion of an M]M matrix (in\nwhich M is the number of basis functions). The m’s can be initialized via PCA, as in the\nstandard GTM.\nWe now focus on the speciﬁcation of B. The theory of Gaussian process regression\n[37,38] or equivalently regularization networks [30] allows B to be quite general. The\ncovariance between mkj and mlj can be taken to depend on the positions of their\nrespective nodes uk and ul, so that Bkl\"f (uk,ul), where f ( ) , ) ) is a covariance function.\nFor example, one can use",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592910766601562,
                309.73876953125,
                385.2618713378906,
                427.3067321777344
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "d30cf33002c5ef1bdb44834f8b269e98",
        "text": "Bkl\"v expG!Euk!ulE2\n2j2 H,\n(41)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.504913330078125,
                436.1264343261719,
                385.16400146484375,
                465.239990234375
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "910628989c54415fbd7247f856c6cabf",
        "text": "where j is a length scale in the latent space and v sets the overall scale of B. A wide\nvariety of covariance functions can be used, and there is a substantial literature\nconcerning valid covariance functions (see, for example [40]).\nIn the original GTM, and in the Gaussian process formulation of the GTM\noutlined above, the overall regression problem decomposes into separate problems for\neach dimension in the data space. (These problems are coupled only through the\nresponsibilities.) However, if the prior on M couples the various dimensions (as in the\ntechnique of co-kriging in geostatistics [11]), then this would no longer be the case,\nand the M-step would involve a KD]KD matrix inversion.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 17,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                472.8480224609375,
                385.27294921875,
                578.7227783203125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "ac52c9f096243fcc30b3c75bad00a77f",
        "text": "Utsugi [36] provides a similar analysis to that above, but uses a relatively simple\ncovariance matrix B based on a discretized approximation to derivatives of the\nM surface.3 He gives details of the EM algorithm which can easily be extended to the\nmore general case, and also provides a Bayesian treatment of hyper-parameters which\nis similar in spirit to that given in Section 6. By specifying B through a covariance\nfunction we would expect to obtain rather better control over the prior on M. For\nexample, the length scale j in Eq. (41) aﬀords direct and readily understandable\ncontrol over the ﬂexibility of the latent-space to data-space mapping. One other\nimportant advantage of formulating the Gaussian process prior via the covariance\nfunction, rather than through a diﬀerence operator as in [36], is that it deﬁnes the\nmanifold in the data space not just at the reference vectors but everywhere on the 2-D\nsurface. This can be achieved because the machinery of Gaussian process regression\npredicts the data-space locations corresponding to new u points.\nThe use of spline smoothing for the M-step in work on principal curves [17,32] is\nanother example of the use of Gaussian process-type priors over functions in SOM-\nlike models.\nAn advantage of the Gaussian process formulation of the GTM is that it empha-\nsizes the similarities between the GTM and SOM, by eliminating the use of basis\nfunctions in the regression model. Furthermore, the update for m(j) is given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59100341796875,
                42.83476257324219,
                385.4018859863281,
                268.1302795410156
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "76ad914836bc7fa5dde321fbe8191575",
        "text": "m(j)\"(G#b~1(B(j))~1)~1G(G~1Rx(j)),\n(42)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.50299072265625,
                274.2706298828125,
                385.1650085449219,
                286.49029541015625
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "c1ee40e7d9afeb2c65866e6ba54d6ed2",
        "text": "where x(j) is the jth column of X, and G~1Rx(j) $%&\n\"xM (j) is the vector of weighted means of\nthe data at the K points in latent space. In the SOM, the update for m(j) is given by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.593017578125,
                298.3922119140625,
                385.179931640625,
                324.7222900390625
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "bc8655f2115c3d0ae87874ea8262d67f",
        "text": "m(j)\"H(j)xM (j),\n(43)",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.5050048828125,
                330.7906188964844,
                385.16497802734375,
                346.9601745605469
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "afca5c47ae3ee24e15106c4ecb9ad466",
        "text": "where H is the neighborhood function evaluated between pairs of latent points. It is\nhard to draw an analogy between H(j) and B(j) because of the matrix inversions\ninvolved in Eq. (42).\nAnother advantage of the Gaussian process formulation is that it avoids issues of\ndiscrete model order selection that arise in the GTM concerning the number of basis\nfunctions used. However, the continuous parameters that control p(M) through\nthe covariance function still need to be addressed. Utsugi [36] has discussed a\nMAP (maximum a posteriori probability) treatment of these parameters, and a\nfully Bayesian treatment using Markov chain Monte Carlo methods would also be\npossible.\nOne disadvantage of using Gaussian processes to formulate the GTM model is that\nthe matrices to be inverted will be larger than those in the parametric GTM case.\nHowever, using up to 1000 nodes in latent space should not present too many\nproblems on modern workstations, and techniques for eﬃcient approximate treat-\nment of Gaussian processes for larger problems have been explored by Gibbs and\nMacKay [16].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59295654296875,
                351.2109680175781,
                385.2268981933594,
                540.490966796875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "443191bd653c07f1902de303cf8dcc86",
        "text": "3In fact, Utsugi’s matrix is only positive semi-deﬁnite due to the presence of a linear null space.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 18,
            "languages": [
                "eng"
            ],
            "coordinates": [
                34.58399963378906,
                570.06396484375,
                364.728759765625,
                578.27099609375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "214e11448150eb371d318e67118054c7",
        "text": "One of the many beneﬁts of the probabilistic foundation of the GTM is\nthat extensions of the model can be formulated in a principled manner, and we\nhave explored a number of such extensions in this paper. There are many other ways\nin which the basic GTM model can be extended, again by taking advantage of the\nprobabilistic setting. For example, it is straightforward to construct a probabil-\nistic mixture of GTM models. The parameters of the component models as well\nas the mixing coeﬃcients between the models, can be determined by maximum\nlikelihood using the EM algorithm, again retaining the attractive convergence\nproperties. This can be further extended to hierarchical mixtures, as discussed\nin [8].\nAnother reﬁnement of the basic model would be to allow the parameters p and b to\nbe continuous functions of the latent space variable u, deﬁned by a parametric\ntransformation. Similarly, the individual nodes can be assigned adaptive mixing\ncoeﬃcients (ﬁxed at 1/K in the original formulation of the GTM) and these could be\nindependent variables (non-negative and summing to unity) or they could again be\nsmooth functions of u. In all such cases, there is a well-deﬁned learning procedure\nbased on maximization of the likelihood function, and the EM algorithm can be\nexploited to handle the hidden variables.\nIn many applications involving real-world data, the data set will suﬀer from missing\nvalues. Provided the values can be assumed to be ‘missing at random’ (i.e. the\nmissingness is not itself informative) then maximum likelihood speciﬁes that the\ncorrect procedure for treating such data is to marginalize over the missing values. For\nmany of the distributions considered in this paper this marginalization is trivial to\nimplement, and corresponds to simply ignoring the missing values, as has been done\nin the case of the SOM [31]. For more complex distributions, such as the non-\nisotropic Gaussians considered in Section 3, the marginalization is more complex but\nstill analytically tractable.\nWhile both the SOM and the GTM represent the data in terms of an underlying\ntwo-dimensional structure, an elegant property of the GTM is that there exists an\nexplicit manifold deﬁned by the continuous non-linear mapping from latent space to\ndata space speciﬁed by the basis functions. The corresponding magniﬁcation factors,\nwhich characterize the way in which portions of the latent space are stretched and\ndistorted by the transformation to data space, can therefore be evaluated as continu-\nous functions of the latent space coordinates using the techniques of diﬀerential\ngeometry [5]. This technique can also be applied to the batch version of the SOM [6],\nby exploiting the existence of a natural interpolating surface arising through a kernel\nsmoothing interpretation [26].\nAnother role for the GTM is as the emission distribution of a hidden Markov\nmodel, leading to G¹M through time as described in Bishop et al. [4]. Finally, we note\nthat the technique of independent component analysis (ICA) [1,9,18] can be for-\nmulated as a latent variable model with a linear transformation from latent space to\ndata space [24,29]. ICA can therefore be extended to allow non-linear transforma-\ntions by employing the framework of the GTM [28].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 19,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.5919189453125,
                66.73875427246094,
                385.36383056640625,
                578.7227783203125
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "69d4d2682ee4babb4fd229f708321c4b",
        "text": "In summary, the GTM retains the many appealing features of the SOM, and oﬀers\ncomparable computational speed, while its probabilistic formulation permits a wide\nvariety of extensions to be developed in a theoretically well-founded setting.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                42.83476257324219,
                385.2469482421875,
                76.73881530761719
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "7b75c4da4f9dec19b01ef751a052c999",
        "text": "Acknowledgements",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.592002868652344,
                102.61177062988281,
                106.61202239990234,
                112.61177062988281
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f1c0d6c538029d55ecc80894e4f67291",
        "text": "This work was supported by EPSRC grant GR/K51808: Neural Networks for\n»isualization of High-Dimensional Data. We would like to thank Geoﬀrey Hinton,\nIain Strachan and Michael Tipping for useful discussions, and Tim Cootes and\nAndreas Lanitis (University of Manchester) for a helpful conversation concerning\nsemi-linear models. Also we wish to thank the anonymous referees for their helpful\ncomments. Markus Svense´ n is grateful to the SANS group at the Royal Institute of\nTechnology for their hospitality. Chris Bishop and Chris Williams would like to thank\nthe Isaac Newton Institute for Mathematical Sciences in Cambridge for providing\nsuch a stimulating research environment during the Neural Networks and Machine\n¸earning programme.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197998046875,
                126.42548370361328,
                385.3339538574219,
                244.0668487548828
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "82003a457e3547b2790023612e355647",
        "text": "References",
        "type": "Title",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.59197235107422,
                269.9397888183594,
                71.71196746826172,
                279.9397888183594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fa27737d7f68b7862b60d8f98751625c",
        "text": "[1] A.J. Bell, T.J. Sejnowski, An information maximization approach to blind separation and blind\ndeconvolution, Neural Comput. 7 (6) (1995) 1129—1159.\n[2] J.O. Berger, Statistical Decision Theory and Bayesian Analysis, 2nd ed., Springer, New York, 1985.\n[3] C.M. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, Oxford, 1995.\n[4] C.M. Bishop G.E. Hinton, I.G.D. Strachan, GTM through time, Proc. IEE 5th Int. Conf. on Artiﬁcial\nNeural Networks, Cambridge, UK, 1997, pp. 111—116.\n[5] C.M. Bishop, M. Svense´ n, C.K.I. Williams, Magniﬁcation factors for the GTM algorithm, Proc. IEE\n5th Int. Conf. on Artiﬁcial Neural Networks, Cambridge, UK, 1997a, pp. 64—69.\n[6] C.M. Bishop, M. Svense´ n, C.K.I. Williams, Magniﬁcation factors for the SOM and GTM algorithms,\nProc. 1997 Workshop on Self-Organizing Maps, Helsinki University of Technology, Finland, 1997b,\npp. 333—338.\n[7] C.M. Bishop, M. Svense´ n, C.K.I. Williams, GTM: the generative topographic mapping, Neural\nComput. 10 (1) (1998) 215—234.\n[8] C.M. Bishop, M.E. Tipping, A hierarchical latent variable model for data visualization, IEEE Trans.\nPattern Anal. Mach. Intell. 20 (3) (1998) 281—293.\n[9] P. Comon, Independent component analysis: a new concept? Signal Process. 36 (1994) 287—314.\n[10] R.T. Cox, Probability, frequency and reasonable expectation, Amer. J. Phys. 14 (1) (1946) 1—13.\n[11] N.A.C. Cressie, Statistics for Spatial Data, Wiley, New York, 1993.\n[12] A.P. Dempster, N.M. Laird, D.B. Rublin, Maximum likelihood from incomplete data via the EM\nalgorithm, J. Roy. Statist. Soc. B 39 (1) (1977) 1—38.\n[13] R. Durbin, R. Szeliski, A. Yuille, An analysis of the elastic net approach to the travelling salesman\nproblem, Neural Comput. 1 (3) (1989) 348—358.\n[14] R. Durbin, D. Willshaw, An analogue approach to the travelling salesman problem, Nature 326 (1987)\n689—691.\n[15] B.S. Everitt An Introduction to Latent Variable Models, Chapman & Hall, London, 1984.\n[16] M. Gibbs, D.J.C. MacKay, Eﬃcient implementation of Gaussian processes, Draft manuscript.\nAvailable from http://wol.ra.phy.cam.ac.uk/mackay/homepage.html, 1997.\n[17] T. Hastie, W. Stuetzle, Principle curves, J. Amer. Statist. Assoc. 84 (406) (1989) 502—516.\n[18] C. Jutten, J. Herault, Blind separation of sources, Signal Process. 24 (1991) 1—10.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 20,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591964721679688,
                291.343017578125,
                385.2752685546875,
                578.2710571289062
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "9eb24779e0279bc044dfce047f6eb555",
        "text": "[19] T. Kohonen, Self-organized formation of topologically correct feature maps, Biol. Cybernet. 43 (1982)\n59—69.\n[20] T. Kohonen, Self-organizing maps, Springer, Berlin, 1995.\n[21] D.J.C. MacKay, Bayesian interpolation, Neural Comput. 4 (3) (1992a) 415—447.\n[22] D.J.C. MacKay, A practical Bayesian framework for back-propagation networks, Neural Comput.\n4 (3) (1992b) 448—472.\n[23] D.J.C. MacKay, Bayesian neural networks and density networks, Nucl. Instrum. Methods Phys. Res.\nA 354 (1) (1995) 73—80.\n[24] D.J.C. MacKay Maximum likelihood and covariant algorithms for independent component analysis,\nDraft manuscript. Available from http://wol.ra.phy.cam.ac.uk/mackay/homepage.html, 1996.\n[25] P. McCullagh, J.A. Nelder, Generalized Linear Models, 2nd ed., Chapman & Hall, London, 1989.\n[26] F. Mulier, V. Cherkassky, Self-organization as an iterative kernel smoothing process, Neural Comput.\n7 (6) (1995) 1165—1177.\n[27] R.M. Neal, G.E. Hinton, A new view of the EM algorithm that justiﬁes incremental and other\nvariants, in: M.I. Jordan (Ed.), Learning in Graphical Models, Kluwer, Dordrecht, 1998.\n[28] P. Pajunen, J. Karhunen, A maximum likelihood approach to nonlinear blind source separation,\nProc. 1997 Int. Conf. on Artiﬁcial Neural Networks, ICANN’97, Lausanne, Switzerland, 1997, pp.\n541—546.\n[29] B.A. Pearlmutter, L.C. Parra, A context-sensitive generalization of ICA, Int. Conf. on Neural\nInformation Processing, 1996.\n[30] T. Poggio, F. Girosi, Networks for approximation and learning, Proc. IEEE 78 (9) (1990) 1481—1497.\n[31] T. Samard, S.A. Harp, Self-organization with partial data, Network: Comput. Neural Systems 3 (2)\n(1992) 205—212.\n[32] R. Tibshirani, Principal curves revisited, Statist. Comput. 2 (1992) 183—190.\n[33] M.E. Tipping, C.M. Bishop, Mixtures of principal component analysers. Proc. IEE 5th Int. Conf. on\nArtiﬁcial Neural Networks, Cambridge, UK, July 1997, pp. 13—18.\n[34] M.E. Tipping, C.M. Bishop, Mixtures of probabilistic principal component analysers, Technical\nReport NCRG/97/003, Neural Computing Research Group, Aston University, Birmingham, UK,\nNeural Comput. (1997) accepted for publication.\n[35] M.E. Tipping, C.M. Bishop, Probabilistic principal component analysis, Technical report, Neural\nComputing Research Group, Aston University, Birmingham, UK, The Roy. Statist. Soc. B. (1997)\naccepted for publication.\n[36] A. Utsugi, Hyperparameter selection for self-organizing maps, Neural Comput. 9 (3) (1997) 623—635.\n[37] P. Whittle, Prediction and Regulation by Linear Least-square Methods, English Universities Press,\nLondon, 1963.\n[38] C.K.I. Williams C.E. Rasmussen, Gaussian processes for regression, in: D.S. Touretzky, M.C. Mozer,\nM.E. Hasselmo (Eds.), Advances in Neural Information Processing Systems, vol. 8, MIT Press,\nCambridge, 1996, pp. 514—520.\n[39] C.K.I. Williams, M.D. Revow, G.E. Hinton, Hand-printed digit recognition using deformable models,\nin: L. Harris, M. Jenkin (Eds.), Spatial Vision in Humans and Robots, Cambridge University Press,\nCambridge, 1993.\n[40] A.M. Yaglom, Correlation Theory of Stationary and Related Random Functions vol. I: Basic Results,\nSpringer, Berlin, 1987.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 21,
            "languages": [
                "eng"
            ],
            "coordinates": [
                26.591995239257812,
                44.38300704956055,
                385.3408508300781,
                470.70281982421875
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "912af1163df5d2ad269124ba95108597",
        "text": ".",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 21,
            "languages": [
                "eng"
            ],
            "coordinates": [
                34.58404541015625,
                570.2708129882812,
                36.33604431152344,
                578.2708129882812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2d0d84f1692c33eaebd69f7d8d430cc9",
        "text": "Christopher Bishop graduated from the University of Oxford in 1980 with First\nClass Honours in Physics, and obtained a Ph.D. from the University of Edinburgh\nin quantum ﬁeld theory in 1983. After period at Culham Laboratory researching\ninto the theory of magnetically conﬁned plasmas for the fusion programme, he\ndeveloped an interest in statistical pattern recognition, and became Head of the\nApplied Neurocomputing Centre at Harwell Laboratory. In 1993 he was ap-\npointed to a Chair in the Department of Computer Science and Applied Mathe-\nmatics at Aston University, and he was the Principal Organiser of the six month\nprogramme on Neural Networks and Machine Learning at the Issac Newton\nInstitute for Mathematical Sciences in Cambridge in 1997. Recently, he moved to\nthe Microsoft Research laboratory in Cambridge, and has also been elected to\na Chair of Computer Science at the University of Edinburgh. His current research\ninterests include probabilistic inference, graphical models and pattern recognition.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 22,
            "languages": [
                "eng"
            ],
            "coordinates": [
                110.2559814453125,
                44.38300704956055,
                385.31268310546875,
                153.97486877441406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c3f5e4f2e78fff98213fc37212316afb",
        "text": "Chris Williams read Physics at Cambridge, graduating in 1982 and continued on\nto do Part III Maths (1983). He then did a M.Sc. in Water Resources at the\nUniversity of Newcastle upon Tyne before going to work in Lesotho, Southern\nAfrica in low-cost sanitation. In 1988 he returned to academia, studying neural\nnetworks/Al with GeoﬀHinton at the University of Toronto (M.Sc., 1990; Ph.D.,\n1994). He moved to Aston University in 1994 and is currently a lecturer in the\nDivision of Electronic Engineering and Computer Science.\nHis research interests cover a wide range of theoretical and practical issues in\nneural networks, statistical pattern recognition, computer vision and Artiﬁcial\nIntelligence.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 22,
            "languages": [
                "eng"
            ],
            "coordinates": [
                110.25599670410156,
                166.3510284423828,
                385.234375,
                250.52708435058594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "13b5cd6dd0b757a54edaaf697f764bf9",
        "text": "Markus Svense´ n read Computer Science at Linko¨ ping University (M.Sc., 1994)\ncombined with an M.Sc. at Aston University (1995). In 1995 he started studies for\na Ph.D. under the supervision of Prof. Bishop, which have been primarily con-\ncerned with the development of the GTM. His research interest is focused on\nmethods for latent structure modelling, but also includes to more general issues on\nstatistical pattern recognition, neural computing and AI.\nHe is currently working at the Max-Plank-Institute of Cognitive Neuroscience\nin Leipzig.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1-s2.0-S0925231298000435-main.pdf",
            "page_number": 22,
            "languages": [
                "eng"
            ],
            "coordinates": [
                110.25599670410156,
                288.3910217285156,
                385.25762939453125,
                355.6470642089844
            ],
            "is_full_width": false
        }
    }
]