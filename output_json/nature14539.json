[
    {
        "element_id": "9c5ec55897568cfdcbe0f95a3a5b3846",
        "text": "REVIEW",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                25.260074615478516,
                178.2104034423828,
                76.06607055664062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3ecc8722e03abd30cbd785769b6260ed",
        "text": "Deep learning",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                99.21809387207031,
                213.88320922851562,
                136.90609741210938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "22d29befba938b8c50b7b080a06bfb42",
        "text": "Yann LeCun1,2, Yoshua Bengio3 & Geoffrey Hinton4,5",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                139.944580078125,
                241.28370666503906,
                152.18472290039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d40821c39c1de3711cdb9237f2be59fe",
        "text": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of \ndata with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech rec­\nognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep \nlearning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine \nshould change its internal parameters that are used to compute the representation in each layer from the representation in \nthe previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and \naudio, whereas recurrent nets have shone light on sequential data such as text and speech.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                45.35430145263672,
                181.0115966796875,
                546.5995483398438,
                256.5293884277344
            ],
            "is_full_width": true
        }
    },
    {
        "element_id": "78cfdcbcba7b193c347560236526be88",
        "text": "M",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                35.76599884033203,
                264.70806884765625,
                72.83846282958984,
                320.83685302734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "36918bf289164ee5e2b9bca999c168d8",
        "text": "achine-learning technology powers many aspects of modern \nsociety: from web searches to content filtering on social net­\nworks to recommendations on e-commerce websites, and \nit is increasingly present in consumer products such as cameras and \nsmartphones. Machine-learning systems are used to identify objects \nin images, transcribe speech into text, match news items, posts or \nproducts with users’ interests, and select relevant results of search. \nIncreasingly, these applications make use of a class of techniques called \ndeep learning.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                275.660400390625,
                291.32464599609375,
                372.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "00dc9ceb50cdba5a91fd0c748648fa83",
        "text": "Conventional machine-learning techniques were limited in their \nability to process natural data in their raw form. For decades, con­\nstructing a pattern-recognition or machine-learning system required \ncareful engineering and considerable domain expertise to design a fea­\nture extractor that transformed the raw data (such as the pixel values \nof an image) into a suitable internal representation or feature vector \nfrom which the learning subsystem, often a classifier, could detect or \nclassify patterns in the input.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                370.160400390625,
                291.3262023925781,
                456.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "946fb213e0b0b17c026a01052e5f1e2b",
        "text": "Representation learning is a set of methods that allows a machine to \nbe fed with raw data and to automatically discover the representations \nneeded for detection or classification. Deep-learning methods are \nrepresentation-learning methods with multiple levels of representa­\ntion, obtained by composing simple but non-linear modules that each \ntransform the representation at one level (starting with the raw input) \ninto a representation at a higher, slightly more abstract level. With the \ncomposition of enough such transformations, very complex functions \ncan be learned. For classification tasks, higher layers of representation \namplify aspects of the input that are important for discrimination and \nsuppress irrelevant variations. An image, for example, comes in the \nform of an array of pixel values, and the learned features in the first \nlayer of representation typically represent the presence or absence of \nedges at particular orientations and locations in the image. The second \nlayer typically detects motifs by spotting particular arrangements of \nedges, regardless of small variations in the edge positions. The third \nlayer may assemble motifs into larger combinations that correspond \nto parts of familiar objects, and subsequent layers would detect objects \nas combinations of these parts. The key aspect of deep learning is that \nthese layers of features are not designed by human engineers: they \nare learned from data using a general-purpose learning procedure.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                454.160400390625,
                291.3352966308594,
                676.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ef26f50aa54f13e31cb3e541810cb0f3",
        "text": "Deep learning is making major advances in solving problems that \nhave resisted the best attempts of the artificial intelligence commu­\nnity for many years. It has turned out to be very good at discovering",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                674.660400390625,
                291.3550109863281,
                708.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d1a89d7fcd2e895b93af1fb9656982fb",
        "text": "doi:10.1038/nature14539",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                464.52880859375,
                64.22351837158203,
                552.7567749023438,
                72.73551940917969
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "171d9865b7eb892b1ca5a35498f39a4a",
        "text": "intricate structures in high-dimensional data and is therefore applica­\nble to many domains of science, business and government. In addition \nto beating records in image recognition1–4 and speech recognition5–7, it \nhas beaten other machine-learning techniques at predicting the activ­\nity of potential drug molecules8, analysing particle accelerator data9,10, \nreconstructing brain circuits11, and predicting the effects of mutations \nin non-coding DNA on gene expression and disease12,13. Perhaps more \nsurprisingly, deep learning has produced extremely promising results \nfor various tasks in natural language understanding14, particularly \ntopic classification, sentiment analysis, question answering15 and lan­\nguage translation16,17.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                275.660400390625,
                554.9647216796875,
                393.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2a3e049e8dd68bc0648f8c1f6e10da2b",
        "text": "We think that deep learning will have many more successes in the \nnear future because it requires very little engineering by hand, so it \ncan easily take advantage of increases in the amount of available com­\nputation and data. New learning algorithms and architectures that are \ncurrently being developed for deep neural networks will only acceler­\nate this progress.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                391.160400390625,
                554.9835205078125,
                456.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5cd94230eb949934c7ea71dc1f368857",
        "text": "Supervised learning \nThe most common form of machine learning, deep or not, is super­\nvised learning. Imagine that we want to build a system that can classify \nimages as containing, say, a house, a car, a person or a pet. We first \ncollect a large data set of images of houses, cars, people and pets, each \nlabelled with its category. During training, the machine is shown an \nimage and produces an output in the form of a vector of scores, one \nfor each category. We want the desired category to have the highest \nscore of all categories, but this is unlikely to happen before training. \nWe compute an objective function that measures the error (or dis­\ntance) between the output scores and the desired pattern of scores. The \nmachine then modifies its internal adjustable parameters to reduce \nthis error. These adjustable parameters, often called weights, are real \nnumbers that can be seen as ‘knobs’ that define the input–output func­\ntion of the machine. In a typical deep-learning system, there may be \nhundreds of millions of these adjustable weights, and hundreds of \nmillions of labelled examples with which to train the machine.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                463.80810546875,
                555.0245361328125,
                645.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a91056081162a0c407b09c98ca1d2815",
        "text": "To properly adjust the weight vector, the learning algorithm com­\nputes a gradient vector that, for each weight, indicates by what amount \nthe error would increase or decrease if the weight were increased by a \ntiny amount. The weight vector is then adjusted in the opposite direc­\ntion to the gradient vector.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                643.160400390625,
                554.80810546875,
                697.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d64d21914d94c4cbd249bf3ea9bc7634",
        "text": "The objective function, averaged over all the training examples, can",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                308.9762878417969,
                695.660400390625,
                554.7677001953125,
                708.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c69b2bbbcd4cbdd469f08c6bd4c3244d",
        "text": "be seen as a kind of hilly landscape in the high-dimensional space of \nweight values. The negative gradient vector indicates the direction \nof steepest descent in this landscape, taking it closer to a minimum, \nwhere the output error is low on average.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                55.16039276123047,
                297.0330505371094,
                99.20609283447266
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e8edb4ffb81896d234d0b75fc4771f66",
        "text": "In practice, most practitioners use a procedure called stochastic \ngradient descent (SGD). This consists of showing the input vector \nfor a few examples, computing the outputs and the errors, computing \nthe average gradient for those examples, and adjusting the weights \naccordingly. The process is repeated for many small sets of examples \nfrom the training set until the average of the objective function stops \ndecreasing. It is called stochastic because each small set of examples \ngives a noisy estimate of the average gradient over all examples. This \nsimple procedure usually finds a good set of weights surprisingly \nquickly when compared with far more elaborate optimization tech­\nniques18. After training, the performance of the system is measured \non a different set of examples called a test set. This serves to test the \ngeneralization ability of the machine — its ability to produce sensible \nanswers on new inputs that it has never seen during training.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                97.16039276123047,
                297.03759765625,
                246.2061004638672
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d5a266b4353f0696da739ce41b243fb3",
        "text": "a\nb",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                81.08961486816406,
                256.6863098144531,
                438.2496032714844,
                269.142333984375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d045e15d4646c7eae2a922d94deadd2f",
        "text": "Output\n(1 sigmoid)\nHidden\n(2 sigmoid)",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                239.8996124267578,
                361.8923034667969,
                413.3747253417969,
                378.3699951171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "33782d2e733999ad3769b70f5d4f68d3",
        "text": "Input\n(2)",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                104.94950103759766,
                362.7366027832031,
                120.59500122070312,
                378.36907958984375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "42887d642c239773ef9ac45d5478a8f0",
        "text": "d\nc",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                83.16961669921875,
                401.8542785644531,
                328.3216247558594,
                412.310302734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bdb053ff854c97b9f1127ca85d07ec21",
        "text": "yl = f (zl)",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                234.23660278320312,
                419.45355224609375,
                258.4839782714844,
                428.93780517578125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a9fdfcf99888c3110c57bb5a918f9d86",
        "text": "Output units",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                118.87129974365234,
                425.8655090332031,
                158.0792694091797,
                433.697998046875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7e0a075789670933b176c4772e3002c1",
        "text": "zl =\nwkl yk\nl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                190.8878936767578,
                426.7513122558594,
                271.6227111816406,
                441.0238342285156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e2d8de129cc69a01bdc060f84166ab85",
        "text": "k  H2",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                242.47390747070312,
                441.95654296875,
                261.0430908203125,
                450.28228759765625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "87dd550525f50208d4d7f1a28c183a4a",
        "text": "wkl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                189.981201171875,
                442.92816162109375,
                197.43869018554688,
                451.9745178222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bd9d6efd8e3e39a3d53d199cdcd93a21",
        "text": "yk = f (zk)",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                233.5081024169922,
                467.1889343261719,
                259.5251159667969,
                476.67413330078125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9ae13a1f73afa7eec257a97fd8209767",
        "text": "k",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                177.38259887695312,
                472.4866943359375,
                181.28988647460938,
                480.3191833496094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fee0157c3d94e40ed592cf22afcaf2f5",
        "text": "Hidden units H2",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                95.11380004882812,
                472.68499755859375,
                146.00228881835938,
                480.5174865722656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "288327e8b90b2b3b03ecc1c75bacdb7c",
        "text": "zk =\nwjk yj",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                233.33828735351562,
                479.27484130859375,
                271.5107421875,
                488.759033203125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "60a3cf9d5f6984c5df7b21c6eaa23dfe",
        "text": "wjk",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                184.68820190429688,
                486.18585205078125,
                192.4328155517578,
                495.2334289550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cd18779abdc5c94adb135e620a505597",
        "text": "j  H1",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                242.47390747070312,
                490.77001953125,
                259.6065979003906,
                499.09576416015625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8ba64e9158d4ea9686375d11fd51919d",
        "text": "yj = f (zj)",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                233.50900268554688,
                508.5567321777344,
                259.1872863769531,
                518.0424194335938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bb4d560848fd8cc324a9e0b89879dbb1",
        "text": "j",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                186.952392578125,
                510.4144287109375,
                189.06944274902344,
                518.2469482421875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c15912ae6f4a23d749b1fd6a28a07368",
        "text": "Hidden units H1",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                91.24629974365234,
                511.5419921875,
                142.13479614257812,
                519.37451171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "23c31a1f7604cb4b701ef8c0645f122e",
        "text": "zj =\nwij xi",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                233.33828735351562,
                522.5578002929688,
                272.44635009765625,
                532.0462646484375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "09c1919792c5691e4abc5152a65d8562",
        "text": "wij",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                186.2415008544922,
                527.1390991210938,
                192.8319091796875,
                536.1865234375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "08105951080d89e0581cd0ef8e56df52",
        "text": "i  Input",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                242.4737091064453,
                535.2589111328125,
                266.6609802246094,
                543.58154296875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f8b957dbdf91d8f16a5da2e5fcdbf1a5",
        "text": "i",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                179.40699768066406,
                550.0897827148438,
                181.58824157714844,
                558.1697998046875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "65d236e02444420e2d11292272d29aac",
        "text": "Input units",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                115.81629943847656,
                550.886474609375,
                149.9932861328125,
                558.718994140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "03c8eae55bcbde0f2df0c5cd7db27dd9",
        "text": "Figure 1 | Multilayer neural networks and backpropagation.  a, A multi-\nlayer neural network (shown by the connected dots) can distort the input \nspace to make the classes of data (examples of which are on the red and \nblue lines) linearly separable. Note how a regular grid (shown on the left) \nin input space is also transformed (shown in the middle panel) by hidden \nunits. This is an illustrative example with only two input units, two hidden \nunits and one output unit, but the networks used for object recognition \nor natural language processing contain tens or hundreds of thousands of \nunits. Reproduced with permission from C. Olah (http://colah.github.io/). \nb, The chain rule of derivatives tells us how two small effects (that of a small \nchange of x on y, and that of y on z) are composed. A small change Δx in \nx gets transformed first into a small change Δy in y by getting multiplied \nby ∂y/∂x (that is, the definition of partial derivative). Similarly, the change \nΔy creates a change Δz in z. Substituting one equation into the other \ngives the chain rule of derivatives — how Δx gets turned into Δz through \nmultiplication by the product of ∂y/∂x and ∂z/∂x. It also works when x, \ny and z are vectors (and the derivatives are Jacobian matrices). c, The \nequations used for computing the forward pass in a neural net with two \nhidden layers and one output layer, each constituting a module through",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                568.297607421875,
                296.6681213378906,
                750.4942626953125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "492776d664e54b8019ea4e3eeb0fa7f3",
        "text": "Many of the current practical applications of machine learning use \nlinear classifiers on top of hand-engineered features. A two-class linear \nclassifier computes a weighted sum of the feature vector components. \nIf the weighted sum is above a threshold, the input is classified as \nbelonging to a particular category.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                55.16039276123047,
                560.6041259765625,
                109.70609283447266
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c3f5c071bf15a6fb61a9b0306bcc09c7",
        "text": "Since the 1960s we have known that linear classifiers can only carve \ntheir input space into very simple regions, namely half-spaces sepa­\nrated by a hyperplane19. But problems such as image and speech recog­\nnition require the input–output function to be insensitive to irrelevant \nvariations of the input, such as variations in position, orientation or \nillumination of an object, or variations in the pitch or accent of speech, \nwhile being very sensitive to particular minute variations (for example, \nthe difference between a white wolf and a breed of wolf-like white \ndog called a Samoyed). At the pixel level, images of two Samoyeds in \ndifferent poses and in different environments may be very different \nfrom each other, whereas two images of a Samoyed and a wolf in the \nsame position and on similar backgrounds may be very similar to each \nother. A linear classifier, or any other ‘shallow’ classifier operating on",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                107.66039276123047,
                560.705322265625,
                246.2061004638672
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9077f733407af2e07f425131a3fefec8",
        "text": "z\ny\nz\nz\ny\n\n\n=\nΔ\nΔ",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                437.5628662109375,
                289.0021667480469,
                496.343017578125,
                303.82421875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "672e3712024795f5f12292e205ed01fd",
        "text": "y\ny\nx\ny\nx\n\n\n=\ny\nz\n\n",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                443.1317138671875,
                304.91400146484375,
                496.483154296875,
                327.2886962890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3306538727a62c471a915ef7f11f76fa",
        "text": "Δ\nΔ",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                465.7191162109375,
                311.5088806152344,
                493.5260009765625,
                319.9981994628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ac995db528b19521bf70eaf2a626b0bb",
        "text": "Δ\nΔ\nz\ny\nz",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                465.582275390625,
                327.29522705078125,
                501.44195556640625,
                341.2247009277344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "60603e79e0bfd236f78ea776ffe961ed",
        "text": "x\ny\nx\n\n",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                480.7562255859375,
                327.4841613769531,
                505.18817138671875,
                341.6742248535156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "16e26a7c9bbb53fa3d79100d03ce64b8",
        "text": "x\ny\n\n",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                443.2489013671875,
                327.84637451171875,
                449.5816650390625,
                342.43389892578125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e45c3220acf7e0f835f5e57b6edbba57",
        "text": "\n\n=",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                474.2344970703125,
                328.40277099609375,
                492.9917297363281,
                341.6742248535156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c8b767436855d3fe2b504ea662f47425",
        "text": "y\n\n",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                465.55523681640625,
                344.7694091796875,
                496.56103515625,
                359.3894958496094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "eff1abee49b002cb83d467c1f7999744",
        "text": "x\nz",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                468.8519287109375,
                344.79547119140625,
                472.0118103027344,
                358.9334411621094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "848aef877a218e125982ca7c3be9355a",
        "text": "y\nz",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                484.60565185546875,
                344.9583435058594,
                487.6482849121094,
                358.9334411621094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "92b10c6820da5dd494c25835465c850e",
        "text": "\n\n=",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                474.77423095703125,
                345.6945495605469,
                493.5314636230469,
                359.3894958496094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ea30236ed83b7cddea458910c44e61a2",
        "text": "\n",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                481.29595947265625,
                345.88348388671875,
                484.5991516113281,
                359.3894958496094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f8c0815f0bc8ba7f4c3eb63955b66ec1",
        "text": "x\nx",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                437.878662109375,
                350.1509704589844,
                496.37213134765625,
                358.9334411621094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b7dc47a585ab7c0e966137500e278778",
        "text": "Compare outputs with correct",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                368.20928955078125,
                401.8943176269531,
                459.2418212890625,
                409.726806640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cf233260407cca762f4f7b300d11c92a",
        "text": "answer to get error derivatives",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                368.77484130859375,
                409.6943054199219,
                458.6828918457031,
                417.52679443359375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e400b3f44a64d0cdb8dadbd58fb3beb4",
        "text": "E\nyl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                441.3763122558594,
                418.6690979003906,
                445.2947692871094,
                436.8368835449219
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "36343b84b4d482bac049fa3da828ec88",
        "text": "=yl\ntl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                447.9143981933594,
                422.5954895019531,
                466.3116760253906,
                431.6357727050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d96f9d0243e7e076ba991b892b257e2c",
        "text": "l",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                423.25030517578125,
                433.37060546875,
                425.37139892578125,
                441.26336669921875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8bd4b35190b3a5baf46cec8ff0741c8b",
        "text": "E\nzl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                441.3763122558594,
                437.60711669921875,
                445.1404724121094,
                455.77587890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fa81d3358bab5ccc42e7956c46f06684",
        "text": "= E",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                447.6077880859375,
                437.60711669921875,
                459.8797912597656,
                449.4014587402344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "611efe8b590abde82cc879484c1b7011",
        "text": "yl\nzl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                466.0823974609375,
                437.60711669921875,
                469.99530029296875,
                455.77587890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bf601bed6ae109d5fc6e0026885a89e2",
        "text": "yl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                455.9613037109375,
                446.7366027832031,
                459.8761901855469,
                455.77587890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "efa3fab990bf594ad3872f0b9d353d7a",
        "text": "wkl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                423.95050048828125,
                449.12420654296875,
                431.56182861328125,
                458.5655212402344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cadf56d61364e18aba0e90810091b44a",
        "text": "E\nyk",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                325.518310546875,
                458.2762145996094,
                330.5360412597656,
                476.4449768066406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0237eeca92b833505ad179ad718b594e",
        "text": "E\nzl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                363.05938720703125,
                458.2762145996094,
                367.031005859375,
                476.4449768066406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6d71af5f7e73127be2bcb4fe456a116f",
        "text": "=\nwkl",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                332.9363098144531,
                462.2023010253906,
                357.3671875,
                471.2417907714844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "14897fcfc7b69b5ce5af37808f959b2b",
        "text": "I  out",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                336.2255554199219,
                471.7481384277344,
                351.7435607910156,
                478.8363952636719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "65c5852cb477a748de8edf3329ae4c49",
        "text": "k",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                412.75189208984375,
                477.1701965332031,
                416.68817138671875,
                485.0629577636719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "186108a71e6e2902b07d2d12b7ff7835",
        "text": "E\nzk",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                325.518310546875,
                486.7757263183594,
                330.2312316894531,
                504.94586181640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b4492d4c95a58dc1a87e64654994b24b",
        "text": "= E",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                332.6317138671875,
                486.7757263183594,
                345.3440856933594,
                498.5693664550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b69f8d44c981df46dc1f47f53c1e8983",
        "text": "yk\nzk",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                351.989013671875,
                486.7757263183594,
                357.0076599121094,
                504.94586181640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "85b93d8279137bba8f6fd11c510b28f1",
        "text": "wjk",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                411.9877014160156,
                487.16162109375,
                419.49530029296875,
                496.2161865234375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "69fba69508cee08ef5b59ca255844e85",
        "text": "yk",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                340.98419189453125,
                495.9054260253906,
                346.0067443847656,
                504.94586181640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5265b98749e437243e5ef753e350ac4e",
        "text": "E\nyj",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                458.9818115234375,
                496.5057067871094,
                463.6087646484375,
                514.6763916015625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "697f78e7195e67a45ad40fa5ec5f69d9",
        "text": "E\nzk",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                499.40948486328125,
                496.5057067871094,
                504.12371826171875,
                514.6763916015625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ffffd808075f7d10a9196d91605aad36",
        "text": "=\nwjk",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                466.22979736328125,
                500.4344177246094,
                493.9325256347656,
                509.4743347167969
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "946ca625be16147ed7735d81a5be4eb1",
        "text": "j",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                424.6517028808594,
                509.0293884277344,
                426.7687072753906,
                516.922119140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "af1bb229e2654fff83c02933d7d75634",
        "text": "k  H2",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                470.2239990234375,
                509.9981384277344,
                486.03265380859375,
                517.08642578125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "415dcfb1da9ed4e07c40a01aec7ec155",
        "text": "yj\nzj",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                485.18389892578125,
                518.3211669921875,
                489.8118591308594,
                537.2911987304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "61ca506a320e1a5c8d5e9b6de070e1cd",
        "text": "E\nzj",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                459.0520935058594,
                519.1214599609375,
                463.3724670410156,
                537.2911987304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a148450b2ba27943978cf0c27a866653",
        "text": "= E",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                465.9955139160156,
                519.1214599609375,
                478.6234436035156,
                530.914794921875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "deeb7e87a4d0bff3f908de9ffd1b6ab0",
        "text": "wij",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                423.6195068359375,
                524.3692626953125,
                430.2122497558594,
                533.4227905273438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "72add1abeb41b831b5834b48b5d0b737",
        "text": "yj",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                474.3500061035156,
                528.2512817382812,
                478.9769592285156,
                537.2911987304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4a6ecf05f108032b788df2dac2dac589",
        "text": "i",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                416.08428955078125,
                548.8407592773438,
                418.2655334472656,
                556.9207763671875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "faceb5b4aeebc1f6963fc26678c419c6",
        "text": "which one can backpropagate gradients. At each layer, we first compute \nthe total input z to each unit, which is a weighted sum of the outputs of \nthe units in the layer below. Then a non-linear function f(.) is applied to \nz to get the output of the unit. For simplicity, we have omitted bias terms. \nThe non-linear functions used in neural networks include the rectified \nlinear unit (ReLU) f(z) = max(0,z), commonly used in recent years, as \nwell as the more conventional sigmoids, such as the hyberbolic tangent, \nf(z) = (exp(z) − exp(−z))/(exp(z) + exp(−z)) and logistic function logistic, \nf(z) = 1/(1 + exp(−z)). d, The equations used for computing the backward \npass. At each hidden layer we compute the error derivative with respect to \nthe output of each unit, which is a weighted sum of the error derivatives \nwith respect to the total inputs to the units in the layer above. We then \nconvert the error derivative with respect to the output into the error \nderivative with respect to the input by multiplying it by the gradient of f(z). \nAt the output layer, the error derivative with respect to the output of a unit \nis computed by differentiating the cost function. This gives yl − tl if the cost \nfunction for unit l is 0.5(yl − tl)2, where tl is the target value. Once the ∂E/∂zk \nis known, the error-derivative for the weight wjk on the connection from \nunit j in the layer below is just yj ∂E/∂zk.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1415100097656,
                568.297607421875,
                557.8800659179688,
                750.9591064453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bc719e6c25fa97439ac25d2cc065a3dc",
        "text": "Samoyed (16); Papillon (5.7); Pomeranian (2.7); Arctic fox (1.0); Eskimo dog (0.6); white wolf (0.4); Siberian husky (0.4)",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                39.24599838256836,
                56.92406463623047,
                404.0259704589844,
                64.7565689086914
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6bdc16bf71f5184adcff43ab87825d4b",
        "text": "Red\nGreen\nBlue",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                197.46080017089844,
                265.0658874511719,
                344.17230224609375,
                272.6448669433594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4242a64690e608ad9814219207414b36",
        "text": "Figure 2 | Inside a convolutional network.  The outputs (not the filters) \nof each layer (horizontally) of a typical convolutional network architecture \napplied to the image of a Samoyed dog (bottom left; and RGB (red, green, \nblue) inputs, bottom right). Each rectangular image is a feature map",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                277.98260498046875,
                277.69482421875,
                317.6793212890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "366164d91f881d919e44bb49e8c253ff",
        "text": "raw pixels could not possibly distinguish the latter two, while putting \nthe former two in the same category. This is why shallow classifiers \nrequire a good feature extractor that solves the selectivity–invariance \ndilemma — one that produces representations that are selective to \nthe aspects of the image that are important for discrimination, but \nthat are invariant to irrelevant aspects such as the pose of the animal. \nTo make classifiers more powerful, one can use generic non-linear \nfeatures, as with kernel methods20, but generic features such as those \narising with the Gaussian kernel do not allow the learner to general­\nize well far from the training examples21. The conventional option is \nto hand design good feature extractors, which requires a consider­\nable amount of engineering skill and domain expertise. But this can \nall be avoided if good features can be learned automatically using a \ngeneral-purpose learning procedure. This is the key advantage of \ndeep learning.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                328.160400390625,
                291.3320007324219,
                487.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f8ee3c8ad9178e9b0edd013d95de988c",
        "text": "A deep-learning architecture is a multilayer stack of simple mod­\nules, all (or most) of which are subject to learning, and many of which \ncompute non-linear input–output mappings. Each module in the \nstack transforms its input to increase both the selectivity and the \ninvariance of the representation. With multiple non-linear layers, say \na depth of 5 to 20, a system can implement extremely intricate func­\ntions of its inputs that are simultaneously sensitive to minute details \n— distinguishing Samoyeds from white wolves — and insensitive to \nlarge irrelevant variations such as the background, pose, lighting and \nsurrounding objects.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                485.660400390625,
                291.26776123046875,
                592.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "afcc4bf9ff763747338469ccd17881f5",
        "text": "Backpropagation to train multilayer architectures \nFrom the earliest days of pattern recognition22,23, the aim of research­\ners has been to replace hand-engineered features with trainable \nmultilayer networks, but despite its simplicity, the solution was not \nwidely understood until the mid 1980s. As it turns out, multilayer \narchitectures can be trained by simple stochastic gradient descent. \nAs long as the modules are relatively smooth functions of their inputs \nand of their internal weights, one can compute gradients using the \nbackpropagation procedure. The idea that this could be done, and \nthat it worked, was discovered independently by several different \ngroups during the 1970s and 1980s24–27.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                600.30810546875,
                291.36370849609375,
                718.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "842b703abedbef571c60d09e1f6766a0",
        "text": "The backpropagation procedure to compute the gradient of an \nobjective function with respect to the weights of a multilayer stack \nof modules is nothing more than a practical application of the chain",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                716.660400390625,
                291.3424377441406,
                750.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b49730b24ca318aeaddff19237481039",
        "text": "Convolutions and ReLU",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                362.7965087890625,
                90.02208709716797,
                434.00396728515625,
                97.8545913696289
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ed0d5114e1ea56e7db0702f34a141fe0",
        "text": "Max pooling",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                362.7965087890625,
                113.83806610107422,
                399.89849853515625,
                121.67057037353516
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d7f4ee3010ac9c8dd00cf5ca2b977a6e",
        "text": "Convolutions and ReLU",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                362.7965087890625,
                144.550537109375,
                434.00396728515625,
                152.38302612304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0b7c082e7b240053a5c9c7a43a8da965",
        "text": "Max pooling",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                364.1289978027344,
                171.174560546875,
                401.23095703125,
                179.00704956054688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "67d8198e0d5b67d88cfeff4df2808f40",
        "text": "Convolutions and ReLU",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                362.7965087890625,
                210.298095703125,
                434.00396728515625,
                218.13058471679688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "09e96e4ad2800d8042f74795e7897685",
        "text": "corresponding to the output for one of the learned features, detected at each \nof the image positions. Information flows bottom up, with lower-level features \nacting as oriented edge detectors, and a score is computed for each image class \nin output. ReLU, rectified linear unit.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                277.98260498046875,
                553.1555786132812,
                317.6793212890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "373afbfc562554c1570ad55212816b95",
        "text": "rule for derivatives. The key insight is that the derivative (or gradi­\nent) of the objective with respect to the input of a module can be \ncomputed by working backwards from the gradient with respect to \nthe output of that module (or the input of the subsequent module) \n(Fig. 1). The backpropagation equation can be applied repeatedly to \npropagate gradients through all modules, starting from the output \nat the top (where the network produces its prediction) all the way to \nthe bottom (where the external input is fed). Once these gradients \nhave been computed, it is straightforward to compute the gradients \nwith respect to the weights of each module.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                328.160400390625,
                554.9782104492188,
                435.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1fd656dfabe2e1f5a8f862328c284650",
        "text": "Many applications of deep learning use feedforward neural net­\nwork architectures (Fig. 1), which learn to map a fixed-size input \n(for example, an image) to a fixed-size output (for example, a prob­\nability for each of several categories). To go from one layer to the \nnext, a set of units compute a weighted sum of their inputs from the \nprevious layer and pass the result through a non-linear function. At \npresent, the most popular non-linear function is the rectified linear \nunit (ReLU), which is simply the half-wave rectifier f(z) = max(z, 0). \nIn past decades, neural nets used smoother non-linearities, such as \ntanh(z) or 1/(1 + exp(−z)), but the ReLU typically learns much faster \nin networks with many layers, allowing training of a deep supervised \nnetwork without unsupervised pre-training28. Units that are not in \nthe input or output layer are conventionally called hidden units. The \nhidden layers can be seen as distorting the input in a non-linear way \nso that categories become linearly separable by the last layer (Fig. 1).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                433.160400390625,
                554.9456176757812,
                592.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d5db94a672cc841388fefa78c777a639",
        "text": "In the late 1990s, neural nets and backpropagation were largely \nforsaken by the machine-learning community and ignored by the \ncomputer-vision and speech-recognition communities. It was widely \nthought that learning useful, multistage, feature extractors with lit­\ntle prior knowledge was infeasible. In particular, it was commonly \nthought that simple gradient descent would get trapped in poor local \nminima — weight configurations for which no small change would \nreduce the average error.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                590.660400390625,
                554.9651489257812,
                676.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0e50bc372ff3bfec30dabfdad7821bfd",
        "text": "In practice, poor local minima are rarely a problem with large net­\nworks. Regardless of the initial conditions, the system nearly always \nreaches solutions of very similar quality. Recent theoretical and \nempirical results strongly suggest that local minima are not a serious \nissue in general. Instead, the landscape is packed with a combinato­\nrially large number of saddle points where the gradient is zero, and \nthe surface curves up in most dimensions and curves down in the",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                674.660400390625,
                554.9483642578125,
                750.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "395b38e9ec17c6a5c50b55f301cbb8a5",
        "text": "remainder29,30. The analysis seems to show that saddle points with \nonly a few downward curving directions are present in very large \nnumbers, but almost all of them have very similar values of the objec­\ntive function. Hence, it does not much matter which of these saddle \npoints the algorithm gets stuck at.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                54.886959075927734,
                297.0186462402344,
                109.70609283447266
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "908d490403004aae62919d408f5f7c73",
        "text": "Interest in deep feedforward networks was revived around 2006 \n(refs 31–34) by a group of researchers brought together by the Cana­\ndian Institute for Advanced Research (CIFAR). The researchers intro­\nduced unsupervised learning procedures that could create layers of \nfeature detectors without requiring labelled data. The objective in \nlearning each layer of feature detectors was to be able to reconstruct \nor model the activities of feature detectors (or raw inputs) in the layer \nbelow. By ‘pre-training’ several layers of progressively more complex \nfeature detectors using this reconstruction objective, the weights of a \ndeep network could be initialized to sensible values. A final layer of \noutput units could then be added to the top of the network and the \nwhole deep system could be fine-tuned using standard backpropaga­\ntion33–35. This worked remarkably well for recognizing handwritten \ndigits or for detecting pedestrians, especially when the amount of \nlabelled data was very limited36.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                107.66039276123047,
                297.0278625488281,
                267.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "37e9cf08a2f672970d054ef0151080ec",
        "text": "The first major application of this pre-training approach was in \nspeech recognition, and it was made possible by the advent of fast \ngraphics processing units (GPUs) that were convenient to program37 \nand allowed researchers to train networks 10 or 20 times faster. In \n2009, the approach was used to map short temporal windows of coef­\nficients extracted from a sound wave to a set of probabilities for the \nvarious fragments of speech that might be represented by the frame \nin the centre of the window. It achieved record-breaking results on a \nstandard speech recognition benchmark that used a small vocabu­\nlary38 and was quickly developed to give record-breaking results on \na large vocabulary task39. By 2012, versions of the deep net from 2009 \nwere being developed by many of the major speech groups6 and were \nalready being deployed in Android phones. For smaller data sets, \nunsupervised pre-training helps to prevent overfitting40, leading to \nsignificantly better generalization when the number of labelled exam­\nples is small, or in a transfer setting where we have lots of examples \nfor some ‘source’ tasks but very few for some ‘target’ tasks. Once deep \nlearning had been rehabilitated, it turned out that the pre-training \nstage was only needed for small data sets.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                265.160400390625,
                297.01629638671875,
                466.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8e578182bcf9148ff2c974bccff1db03",
        "text": "There was, however, one particular type of deep, feedforward net­\nwork that was much easier to train and generalized much better than \nnetworks with full connectivity between adjacent layers. This was \nthe convolutional neural network (ConvNet)41,42. It achieved many \npractical successes during the period when neural networks were out \nof favour and it has recently been widely adopted by the computer-\nvision community.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                464.660400390625,
                296.94964599609375,
                540.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4783dc01fc133267e7cece8fc24cde6b",
        "text": "Convolutional neural networks \nConvNets are designed to process data that come in the form of \nmultiple arrays, for example a colour image composed of three 2D \narrays containing pixel intensities in the three colour channels. Many \ndata modalities are in the form of multiple arrays: 1D for signals and \nsequences, including language; 2D for images or audio spectrograms; \nand 3D for video or volumetric images. There are four key ideas \nbehind ConvNets that take advantage of the properties of natural \nsignals: local connections, shared weights, pooling and the use of \nmany layers.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                547.80810546875,
                297.020751953125,
                655.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0517d887b2ea9cd52727e887ec57088b",
        "text": "The architecture of a typical ConvNet (Fig. 2) is structured as a \nseries of stages. The first few stages are composed of two types of \nlayers: convolutional layers and pooling layers. Units in a convolu­\ntional layer are organized in feature maps, within which each unit \nis connected to local patches in the feature maps of the previous \nlayer through a set of weights called a filter bank. The result of this \nlocal weighted sum is then passed through a non-linearity such as a \nReLU. All units in a feature map share the same filter bank. Differ­\nent feature maps in a layer use different filter banks. The reason for",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                653.660400390625,
                297.01397705078125,
                750.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "456078577ae81c9e5f511165a6e84217",
        "text": "this architecture is twofold. First, in array data such as images, local \ngroups of values are often highly correlated, forming distinctive local \nmotifs that are easily detected. Second, the local statistics of images \nand other signals are invariant to location. In other words, if a motif \ncan appear in one part of the image, it could appear anywhere, hence \nthe idea of units at different locations sharing the same weights and \ndetecting the same pattern in different parts of the array. Mathemati­\ncally, the filtering operation performed by a feature map is a discrete \nconvolution, hence the name.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                55.16039276123047,
                560.6140747070312,
                151.7061004638672
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8a0317ea5d7cef5b92a8c1b2a21d523a",
        "text": "Although the role of the convolutional layer is to detect local con­\njunctions of features from the previous layer, the role of the pooling \nlayer is to merge semantically similar features into one. Because the \nrelative positions of the features forming a motif can vary somewhat, \nreliably detecting the motif can be done by coarse-graining the posi­\ntion of each feature. A typical pooling unit computes the maximum \nof a local patch of units in one feature map (or in a few feature maps). \nNeighbouring pooling units take input from patches that are shifted \nby more than one row or column, thereby reducing the dimension of \nthe representation and creating an invariance to small shifts and dis­\ntortions. Two or three stages of convolution, non-linearity and pool­\ning are stacked, followed by more convolutional and fully-connected \nlayers. Backpropagating gradients through a ConvNet is as simple as \nthrough a regular deep network, allowing all the weights in all the \nfilter banks to be trained.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                149.660400390625,
                560.6256103515625,
                309.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "14d3b7d2ac2aa9873a6a2701dbf73f99",
        "text": "Deep neural networks exploit the property that many natural sig­\nnals are compositional hierarchies, in which higher-level features \nare obtained by composing lower-level ones. In images, local combi­\nnations of edges form motifs, motifs assemble into parts, and parts \nform objects. Similar hierarchies exist in speech and text from sounds \nto phones, phonemes, syllables, words and sentences. The pooling \nallows representations to vary very little when elements in the previ­\nous layer vary in position and appearance.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                307.160400390625,
                560.6499633789062,
                393.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f83635d2844a323bb8fe1e7c957aaf19",
        "text": "The convolutional and pooling layers in ConvNets are directly \ninspired by the classic notions of simple cells and complex cells in \nvisual neuroscience43, and the overall architecture is reminiscent of \nthe LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral path­\nway44. When ConvNet models and monkeys are shown the same pic­\nture, the activations of high-level units in the ConvNet explains half \nof the variance of random sets of 160 neurons in the monkey’s infer­\notemporal cortex45. ConvNets have their roots in the neocognitron46, \nthe architecture of which was somewhat similar, but did not have an \nend-to-end supervised-learning algorithm such as backpropagation. \nA primitive 1D ConvNet called a time-delay neural net was used for \nthe recognition of phonemes and simple words47,48.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                391.160400390625,
                560.6708984375,
                519.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3c0a9b1c777661bfd1e5474576d0420c",
        "text": "There have been numerous applications of convolutional net­\nworks going back to the early 1990s, starting with time-delay neu­\nral networks for speech recognition47 and document reading42. The \ndocument reading system used a ConvNet trained jointly with a \nprobabilistic model that implemented language constraints. By the \nlate 1990s this system was reading over 10% of all the cheques in the \nUnited States. A number of ConvNet-based optical character recog­\nnition and handwriting recognition systems were later deployed by \nMicrosoft49. ConvNets were also experimented with in the early 1990s \nfor object detection in natural images, including faces and hands50,51, \nand for face recognition52.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416015625,
                517.160400390625,
                560.625244140625,
                634.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c0363d97d06ea1d0d9906a7187cefa6c",
        "text": "Image understanding with deep convolutional networks \nSince the early 2000s, ConvNets have been applied with great success to \nthe detection, segmentation and recognition of objects and regions in \nimages. These were all tasks in which labelled data was relatively abun­\ndant, such as traffic sign recognition53, the segmentation of biological \nimages54 particularly for connectomics55, and the detection of faces, \ntext, pedestrians and human bodies in natural images36,50,51,56–58. A major \nrecent practical success of ConvNets is face recognition59.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416015625,
                642.30810546875,
                560.6100463867188,
                729.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fe4c05812304baceef9463fc99c490dd",
        "text": "Language\nGenerating RNN",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                290.035400390625,
                66.20018768310547,
                339.4022521972656,
                81.83267974853516
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "03bc9e99dc6f5d26a419b02985f793d2",
        "text": "Vision\nDeep CNN",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                204.72940063476562,
                67.99419403076172,
                236.6768798828125,
                83.6266860961914
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "650de2367039cae15f1383a023af423c",
        "text": "A woman is throwing a frisbee in a park.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                61.09859848022461,
                287.0729064941406,
                184.1761016845703,
                294.9768981933594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4b8194b02a20e6863cc8036fd7b18529",
        "text": "A dog is standing on a hardwood ﬂoor.\nA stop sign is on a road with a",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                236.9040985107422,
                287.0729064941406,
                518.7830200195312,
                294.9768981933594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b7aa9b28c098e8455ea0f9fa09e3bf03",
        "text": "A little girl sitting on a bed with a teddy bear.\nA group of people sitting on a boat in the water.\nA giraﬀe standing in a forest with",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                53.18159866333008,
                396.74090576171875,
                517.711181640625,
                404.6448974609375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9b053830a8c4a36174bfe00128593205",
        "text": "Figure 3 | From image to text.  Captions generated by a recurrent neural \nnetwork (RNN) taking, as extra input, the representation extracted by a deep \nconvolution neural network (CNN) from a test image, with the RNN trained to \n‘translate’ high-level representations of images into captions (top). Reproduced",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                423.5888977050781,
                286.65301513671875,
                463.2856140136719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b6c4edba6ca1fc0530b801357701660b",
        "text": "self-driving cars60,61. Companies such as Mobileye and NVIDIA are \nusing such ConvNet-based methods in their upcoming vision sys­\ntems for cars. Other applications gaining importance involve natural \nlanguage understanding14 and speech recognition7.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                474.8869934082031,
                291.3109130859375,
                519.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "96e6852240adc84251eb43715e564adb",
        "text": "Despite these successes, ConvNets were largely forsaken by the \nmainstream computer-vision and machine-learning communities \nuntil the ImageNet competition in 2012. When deep convolutional \nnetworks were applied to a data set of about a million images from \nthe web that contained 1,000 different classes, they achieved spec­\ntacular results, almost halving the error rates of the best compet­\ning approaches1. This success came from the efficient use of GPUs, \nReLUs, a new regularization technique called dropout62, and tech­\nniques to generate more training examples by deforming the existing \nones. This success has brought about a revolution in computer vision; \nConvNets are now the dominant approach for almost all recognition \nand detection tasks4,58,59,63–65 and approach human performance on \nsome tasks. A recent stunning demonstration combines ConvNets \nand recurrent net modules for the generation of image captions \n(Fig. 3).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                517.160400390625,
                291.3503723144531,
                676.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7fcb2ae543d2e7e237bab12a28fe9ac6",
        "text": "Recent ConvNet architectures have 10 to 20 layers of ReLUs, hun­\ndreds of millions of weights, and billions of connections between \nunits. Whereas training such large networks could have taken weeks \nonly two years ago, progress in hardware, software and algorithm \nparallelization have reduced training times to a few hours.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                674.660400390625,
                291.2592468261719,
                729.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9c72703c98c657d00a42f45711e7f1d5",
        "text": "A group of people \nshopping at an outdoor",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                377.29638671875,
                93.92339324951172,
                450.71380615234375,
                109.5558853149414
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "874f8cfbb6fe3fe121e891c605a55021",
        "text": "market.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                401.5934143066406,
                109.52336883544922,
                424.3752746582031,
                117.35587310791016
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b8190322d75599ab40fc09250645ce7a",
        "text": "There are many \nvegetables at the",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                387.2868957519531,
                125.92290496826172,
                440.723388671875,
                141.55538940429688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d2f6539dc624f509ed23a731d77268a2",
        "text": "fruit stand.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                396.5819091796875,
                141.52288818359375,
                429.39385986328125,
                149.35537719726562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "250f67d6405199030dd0a8184c5831d8",
        "text": "mountain in the background",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                429.3301086425781,
                294.9118957519531,
                516.007568359375,
                302.744384765625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4aa3f76be857b2e95a9ba01c034ac0ac",
        "text": "trees in the background.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                430.3310852050781,
                404.5408935546875,
                504.9452209472656,
                412.44488525390625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "325df54f8b5f35c3d4362e036b70b5d9",
        "text": "with permission from ref. 102. When the RNN is given the ability to focus its \nattention on a different location in the input image (middle and bottom; the \nlighter patches were given more attention) as it generates each word (bold), we \nfound86 that it exploits this to achieve better ‘translation’ of images into captions.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                423.5888977050781,
                550.0751342773438,
                463.2856140136719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "43c7dbb4115c3c0090e5593fe3b16d74",
        "text": "Microsoft, IBM, Yahoo!, Twitter and Adobe, as well as a quickly \ngrowing number of start-ups to initiate research and development \nprojects and to deploy ConvNet-based image understanding products \nand services.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                475.160400390625,
                554.940185546875,
                519.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "73b5a29e2876e44efd061d281c927550",
        "text": "ConvNets are easily amenable to efficient hardware implemen­\ntations in chips or field-programmable gate arrays66,67. A number \nof companies such as NVIDIA, Mobileye, Intel, Qualcomm and \nSamsung are developing ConvNet chips to enable real-time vision \napplications in smartphones, cameras, robots and self-driving cars.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                517.160400390625,
                554.9097900390625,
                571.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1aeb9573b2476fe88065f79b361d5a60",
        "text": "Distributed representations and language processing \nDeep-learning theory shows that deep nets have two different expo­\nnential advantages over classic learning algorithms that do not use \ndistributed representations21. Both of these advantages arise from the \npower of composition and depend on the underlying data-generating \ndistribution having an appropriate componential structure40. First, \nlearning distributed representations enable generalization to new \ncombinations of the values of learned features beyond those seen \nduring training (for example, 2n combinations are possible with n \nbinary features)68,69. Second, composing layers of representation in \na deep net brings the potential for another exponential advantage70 \n(exponential in the depth).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                579.30810546875,
                555.0001220703125,
                708.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2fb73cf08e9b543e848a6844d0ba7267",
        "text": "The hidden layers of a multilayer neural network learn to repre­\nsent the network’s inputs in a way that makes it easy to predict the \ntarget outputs. This is nicely demonstrated by training a multilayer \nneural network to predict the next word in a sequence from a local",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                706.160400390625,
                554.9539794921875,
                750.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c09640fd1881bb127d065907cfff29a2",
        "text": "context of earlier words71. Each word in the context is presented to \nthe network as a one-of-N vector, that is, one component has a value \nof 1 and the rest are 0. In the first layer, each word creates a different \npattern of activations, or word vectors (Fig. 4). In a language model, \nthe other layers of the network learn to convert the input word vec­\ntors into an output word vector for the predicted next word, which \ncan be used to predict the probability for any word in the vocabulary \nto appear as the next word. The network learns word vectors that \ncontain many active components each of which can be interpreted \nas a separate feature of the word, as was first demonstrated27 in the \ncontext of learning distributed representations for symbols. These \nsemantic features were not explicitly present in the input. They were \ndiscovered by the learning procedure as a good way of factorizing \nthe structured relationships between the input and output symbols \ninto multiple ‘micro-rules’. Learning word vectors turned out to also \nwork very well when the word sequences come from a large corpus \nof real text and the individual micro-rules are unreliable71. When \ntrained to predict the next word in a news story, for example, the \nlearned word vectors for Tuesday and Wednesday are very similar, as \nare the word vectors for Sweden and Norway. Such representations \nare called distributed representations because their elements (the \nfeatures) are not mutually exclusive and their many configurations \ncorrespond to the variations seen in the observed data. These word \nvectors are composed of learned features that were not determined \nahead of time by experts, but automatically discovered by the neural \nnetwork. Vector representations of words learned from text are now \nvery widely used in natural language applications14,17,72–76.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                54.886959075927734,
                297.01129150390625,
                340.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3c82615d29bbf583385cbb0222db4d3a",
        "text": "The issue of representation lies at the heart of the debate between \nthe logic-inspired and the neural-network-inspired paradigms for \ncognition. In the logic-inspired paradigm, an instance of a symbol is \nsomething for which the only property is that it is either identical or \nnon-identical to other symbol instances. It has no internal structure \nthat is relevant to its use; and to reason with symbols, they must be \nbound to the variables in judiciously chosen rules of inference. By \ncontrast, neural networks just use big activity vectors, big weight \nmatrices and scalar non-linearities to perform the type of fast ‘intui­\ntive’ inference that underpins effortless commonsense reasoning.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                338.660400390625,
                297.0106506347656,
                445.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bcd90abdc13d2f7442e767f40d6ca012",
        "text": "Before the introduction of neural language models71, the standard \napproach to statistical modelling of language did not exploit distrib­\nuted representations: it was based on counting frequencies of occur­\nrences of short symbol sequences of length up to N (called N-grams). \nThe number of possible N-grams is on the order of VN, where V is \nthe vocabulary size, so taking into account a context of more than a",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                443.3869934082031,
                296.92474365234375,
                508.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "00b8e9027c2707719ba2e3043f154963",
        "text": "14",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.81916809082031,
                523.2672119140625,
                59.87916946411133,
                531.0997314453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "802b5675a174cbbd8c60c9d713afe042",
        "text": "body",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                168.5396728515625,
                525.7632446289062,
                185.420166015625,
                533.5957641601562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2a43e25245b6f677d9e57ab8376d1e4e",
        "text": "oﬃce",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                229.66566467285156,
                526.4717407226562,
                247.62515258789062,
                534.3042602539062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d998e7d2247d08ab83b03c43c594fd5a",
        "text": "13.5",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                46.41096878051758,
                537.9161376953125,
                59.878971099853516,
                545.7486572265625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "acbee5223a67b2a973da39ca0c34e375",
        "text": "Agency",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                137.09266662597656,
                542.5982055664062,
                160.6616668701172,
                550.4307250976562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a8389bcda7b493ea8d6f64d8300ed062",
        "text": "school",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                73.50316619873047,
                543.7227172851562,
                94.71916961669922,
                551.5552368164062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4c31dccd9659fb1d99a2adaadf3b4312",
        "text": "schools",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                72.13166809082031,
                545.4451904296875,
                96.42217254638672,
                553.2777099609375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fb13c584fcc683bb6eb301b255236234",
        "text": "13",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.81916809082031,
                552.5631103515625,
                59.87916946411133,
                560.3956298828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "095627d4214967cc3d516889c64daf69",
        "text": "agencies",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                156.31967163085938,
                559.8492431640625,
                184.1981658935547,
                567.6817626953125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0016ea0ae9c9478115c6a2b29f47f583",
        "text": "organization",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                119.80916595458984,
                565.4847412109375,
                158.60765075683594,
                573.3172607421875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a47122a4e3b87a90e623e2c634e192c6",
        "text": "12.5",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                46.41096878051758,
                567.2115478515625,
                59.878971099853516,
                575.0440673828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c0faa113d944ec1f5dea87997686c304",
        "text": "organizations  institutions",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                145.6856689453125,
                575.3777465820312,
                223.88714599609375,
                588.0332641601562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f3105c06e1219f6014bd5d9be356b6f2",
        "text": "12",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.81916809082031,
                581.8946533203125,
                59.87916946411133,
                589.7271728515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4e24ae7902fdf136a02b3c6d3f7f973b",
        "text": "Association",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                105.82766723632812,
                587.2792358398438,
                141.42166137695312,
                595.1117553710938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4408286c886c96460a1e5c678fc130cc",
        "text": "11.5",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                46.41096878051758,
                596.5426025390625,
                59.878971099853516,
                604.3751220703125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "be6ec9b914be035405ef6b6c6c88abc9",
        "text": "11",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.81916809082031,
                611.1905517578125,
                59.87916946411133,
                619.0230712890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7ee86bb1bca3d474d6e34ef216fd69ab",
        "text": "10.5",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                46.41096878051758,
                625.8385009765625,
                59.878971099853516,
                633.6710205078125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "72b9da94fa2a7b23b068f35bc5591efa",
        "text": "10",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.81916809082031,
                640.5220947265625,
                59.87916946411133,
                648.3546142578125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9322044819ce11713eac736d390eb6ac",
        "text": "companies",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                133.60867309570312,
                644.4987182617188,
                168.149658203125,
                652.3312377929688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "da73ff690c35a5cd9c664f69dfb47edf",
        "text": "community",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                54.41917037963867,
                651.6552124023438,
                90.39667510986328,
                659.4877319335938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f9fe594314e3d79b1569a64fc9a60ff8",
        "text": "society",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                111.46317291259766,
                653.3387451171875,
                134.00518798828125,
                661.1712646484375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "18d3750616daf6b1c6895c0eb5399b21",
        "text": "industry\n company",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                125.02217102050781,
                654.814208984375,
                215.7296600341797,
                667.437255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "233ba65a1e931b76ebaaa84b471c08ee",
        "text": "communities",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                47.44466781616211,
                656.0557250976562,
                88.2581787109375,
                663.8882446289062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c82d3e4efd7f2e22f7d0d8608cc37bc3",
        "text": "Community",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                54.20466995239258,
                660.1702270507812,
                91.1636734008789,
                668.0027465820312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e93decc159b91285cbacd0bfdd886955",
        "text": "9",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                55.817169189453125,
                669.8179931640625,
                59.879669189453125,
                677.6505126953125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1385b3c4819f40b089f04b881e55560b",
        "text": "−37\n−36\n−35\n−34\n−33\n−32\n−31\n−30\n−29",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                79.49349212646484,
                685.2349853515625,
                275.769287109375,
                693.0675048828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "71de6221ea6914cb8deb04097f7603d8",
        "text": "Figure 4 | Visualizing the learned word vectors.  On the left is an illustration \nof word representations learned for modelling language, non-linearly projected \nto 2D for visualization using the t-SNE algorithm103. On the right is a 2D \nrepresentation of phrases learned by an English-to-French encoder–decoder \nrecurrent neural network75. One can observe that semantically similar words",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                700.3605346679688,
                296.1602478027344,
                749.5571899414062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e4c15476d75f41d6a26a5a17eb518b6c",
        "text": "handful of words would require very large training corpora. N-grams \ntreat each word as an atomic unit, so they cannot generalize across \nsemantically related sequences of words, whereas neural language \nmodels can because they associate each word with a vector of real \nvalued features, and semantically related words end up close to each \nother in that vector space (Fig. 4).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                55.16039276123047,
                560.5709228515625,
                120.20609283447266
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2c447a0fac56cc73b734ef6a9909c848",
        "text": "Recurrent neural networks \nWhen backpropagation was first introduced, its most exciting use was \nfor training recurrent neural networks (RNNs). For tasks that involve \nsequential inputs, such as speech and language, it is often better to \nuse RNNs (Fig. 5). RNNs process an input sequence one element at a \ntime, maintaining in their hidden units a ‘state vector’ that implicitly \ncontains information about the history of all the past elements of \nthe sequence. When we consider the outputs of the hidden units at \ndifferent discrete time steps as if they were the outputs of different \nneurons in a deep multilayer network (Fig. 5, right), it becomes clear \nhow we can apply backpropagation to train RNNs.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                127.80809020996094,
                560.676025390625,
                246.2061004638672
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4f9de4d916b778da7bc71ce0ee53af3c",
        "text": "RNNs are very powerful dynamic systems, but training them has \nproved to be problematic because the backpropagated gradients \neither grow or shrink at each time step, so over many time steps they \ntypically explode or vanish77,78.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                244.160400390625,
                560.62451171875,
                288.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "38413d8a14d96488d8eba4497217ba77",
        "text": "Thanks to advances in their architecture79,80 and ways of training \nthem81,82, RNNs have been found to be very good at predicting the \nnext character in the text83 or the next word in a sequence75, but they \ncan also be used for more complex tasks. For example, after reading \nan English sentence one word at a time, an English ‘encoder’ network \ncan be trained so that the final state vector of its hidden units is a good \nrepresentation of the thought expressed by the sentence. This thought \nvector can then be used as the initial hidden state of (or as extra input \nto) a jointly trained French ‘decoder’ network, which outputs a prob­\nability distribution for the first word of the French translation. If a \nparticular first word is chosen from this distribution and provided \nas input to the decoder network it will then output a probability dis­\ntribution for the second word of the translation and so on until a \nfull stop is chosen17,72,76. Overall, this process generates sequences of \nFrench words according to a probability distribution that depends on \nthe English sentence. This rather naive way of performing machine \ntranslation has quickly become competitive with the state-of-the-art, \nand this raises serious doubts about whether understanding a sen­\ntence requires anything like the internal symbolic expressions that are \nmanipulated by using inference rules. It is more compatible with the \nview that everyday reasoning involves many simultaneous analogies",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                285.8869934082031,
                560.6172485351562,
                508.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a1c3901630a03a7f77a0267daafc6417",
        "text": "−2.2",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                525.165283203125,
                328.4167175292969,
                532.997802734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c2bf04bbb817e881ca2a4d64c70ea455",
        "text": "&quot; the two groups",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                326.82421875,
                531.197265625,
                394.9115905761719,
                539.02978515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8044b3cf45472ffa29b139edef4a360c",
        "text": "−2.4",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                540.3717041015625,
                328.4167175292969,
                548.2042236328125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "84d116ecfd8283d31e72b48178408b95",
        "text": "of the two groups",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                317.1847229003906,
                542.604736328125,
                370.4195861816406,
                550.437255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1e10f367611bfbdabbb39da6d412eded",
        "text": "over the last two decades",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                322.8852233886719,
                550.7102661132812,
                399.99468994140625,
                558.5427856445312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f7996dc4e310104510a1f07b4189f688",
        "text": "two months before being",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                426.2221984863281,
                555.5332641601562,
                502.25909423828125,
                563.3657836914062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "964d1432c0679697753235b70dfb93be",
        "text": "−2.6",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                555.6240844726562,
                328.4167175292969,
                563.4566040039062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1b66e84ae9790846f0880091b94992da",
        "text": "the last two decades",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                341.93670654296875,
                559.1082763671875,
                404.05718994140625,
                566.9407958984375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "aa2413720c98631bb145058b166e66fa",
        "text": "for nearly two months",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                441.9847106933594,
                564.38623046875,
                508.85662841796875,
                572.21875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "02c67598e812a426c1220cdf005623c6",
        "text": "−2.8",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                570.8765869140625,
                328.4167175292969,
                578.7091064453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f59888e7b3c4e3284bddafba6cc7a4a8",
        "text": "dispute between the two",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                304.2951965332031,
                584.373779296875,
                378.6486511230469,
                592.206298828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "18dfc90f7c3d7acbbf952cbe6c49fc05",
        "text": "−3",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                320.017822265625,
                586.0831298828125,
                328.41583251953125,
                593.9156494140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "43070b1a7de8adfa2748b6cdcdcd7b4b",
        "text": "−3.2",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                601.336181640625,
                328.4167175292969,
                609.168701171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d96371e1da060ff815d0c290f6b82583",
        "text": "In a few months",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                450.1357116699219,
                613.8577880859375,
                498.99615478515625,
                621.6903076171875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3ef4ede0b4cce00118ab6159fec6e5df",
        "text": "that a few days",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                392.92919921875,
                614.039794921875,
                438.9296875,
                621.872314453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "52525d7b6394fda694ea903bd64cb576",
        "text": "−3.4",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                616.5889892578125,
                328.4167175292969,
                624.4215087890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "36d9435b2c683b1d0708a62c44fd40f3",
        "text": "a few months ago",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                414.0736999511719,
                619.7467651367188,
                468.12762451171875,
                627.5792846679688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "376ba392c2caf79cf1f1016cb86cfff3",
        "text": "within a few months",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                428.1722106933594,
                629.2887573242188,
                489.53863525390625,
                637.1212768554688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ac6764564cb1237ddff8e6a4387d2ee7",
        "text": "−3.6",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                631.7945556640625,
                328.4167175292969,
                639.6270751953125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "563db0b2750eaa01ead8179fb3ea4371",
        "text": "over the last few months",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                426.9176940917969,
                641.4892578125,
                501.6676025390625,
                649.32177734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3ae75d027794a3b80f7907e38bbdfa96",
        "text": "over the past few months",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                435.5367126464844,
                646.7218017578125,
                512.6006469726562,
                654.5543212890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d19f031ca46cd0faf948f13d30c59093",
        "text": "−3.8",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                647.0474853515625,
                328.4167175292969,
                654.8800048828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4737e22074f28b57b3eb84a15a61f34a",
        "text": "the next six months",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                464.1432189941406,
                653.4947509765625,
                523.826171875,
                661.3272705078125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b6215e4b5cd312e79d9de10ec19055e3",
        "text": "In the last few days\nthe past few days",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                375.17120361328125,
                657.9927978515625,
                435.15966796875,
                669.2117919921875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "52fdd713d7561855a5b1ae2dc7131233",
        "text": "−4",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                320.017822265625,
                662.2998657226562,
                328.41583251953125,
                670.1323852539062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9e74b03fb8239080248bf9cf814906d8",
        "text": "−5.5\n−5\n−4.5\n−4\n−3.5\n−3\n−2.5\n−2\n−4.2",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.480712890625,
                677.5523681640625,
                509.0232238769531,
                693.0675048828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e2d9dc92cea352f5f7d1d9231986faaa",
        "text": "in the coming months",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                478.001220703125,
                677.603271484375,
                545.003173828125,
                685.435791015625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "593bfbad1e726af696cf01e6b544a2f7",
        "text": "or sequences of words are mapped to nearby representations. The distributed \nrepresentations of words are obtained by using backpropagation to jointly learn \na representation for each word and a function that predicts a target quantity \nsuch as the next word in a sequence (for language modelling) or a whole \nsequence of translated words (for machine translation)18,75.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                700.3605346679688,
                559.8191528320312,
                749.5571899414062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c5f0c9a1320b6f9c2455ab761fbad300",
        "text": "o",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                76.80470275878906,
                58.10661315917969,
                80.0947036743164,
                67.05960845947266
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e22bc4793b05999e8c1251a3e7c68fae",
        "text": "ot−1\not",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                162.4969024658203,
                63.42552185058594,
                200.86160278320312,
                74.42232513427734
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1e4ae8b80d27680bf821bce64fde2d7c",
        "text": "ot+1",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                229.5594024658203,
                64.05412292480469,
                239.6114959716797,
                75.0509262084961
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2968d01773bce39afe8db8aa30f7db0b",
        "text": "V\nW\nW",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                70.92469787597656,
                80.36662292480469,
                150.18569946289062,
                98.23758697509766
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ed86852471a423ab957265ae5f52cc64",
        "text": "V\nV\nV",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                156.9687042236328,
                80.95457458496094,
                229.26470947265625,
                90.7126235961914
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "239b51a5becb8490ce99ceac5f8ad801",
        "text": "st−1",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                166.86669921875,
                86.70158386230469,
                175.98739624023438,
                97.7002182006836
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b638690b6f7894fcf0320aa340cdace3",
        "text": "st\nst+1",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                200.62049865722656,
                86.72642517089844,
                242.9390869140625,
                97.8623275756836
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4d493b63592b2b8159b9df1f83cea537",
        "text": "s",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                71.86969757080078,
                90.25059509277344,
                74.22869873046875,
                99.2035903930664
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "590a68a8ea07fa432cb062bdd4636cd9",
        "text": "W\nW\nW",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                176.6876983642578,
                100.00157165527344,
                248.08070373535156,
                109.0876235961914
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7b07deaed7458a5f131c4792240c43c4",
        "text": "Unfold",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.8938980102539,
                104.06261444091797,
                128.00491333007812,
                111.8951187133789
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a346e57866dffbca9b2b2fb7e65fcad1",
        "text": "U\nU\nU\nU",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                71.07170104980469,
                109.45860290527344,
                237.18870544433594,
                118.9366226196289
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "443352228ac5de83b5aba98394aaf319",
        "text": "xt\nxt−1\nxt+1\nx",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                76.23079681396484,
                121.37077331542969,
                239.1617889404297,
                132.36782836914062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "942694167d2b5916123e22e16290528e",
        "text": "Figure 5 | A recurrent neural network and the unfolding in time of the \ncomputation involved in its forward computation.  The artificial neurons \n(for example, hidden units grouped under node s with values st at time t) get \ninputs from other neurons at previous time steps (this is represented with the \nblack square, representing a delay of one time step, on the left). In this way, a \nrecurrent neural network can map an input sequence with elements xt into an \noutput sequence with elements ot, with each ot depending on all the previous \nxtʹ (for tʹ ≤ t). The same parameters (matrices U,V,W ) are used at each time \nstep. Many other architectures are possible, including a variant in which the \nnetwork can generate a sequence of outputs (for example, words), each of \nwhich is used as inputs for the next time step. The backpropagation algorithm \n(Fig. 1) can be directly applied to the computational graph of the unfolded \nnetwork on the right, to compute the derivative of a total error (for example, \nthe log-probability of generating the right sequence of outputs) with respect to \nall the states st and all the parameters.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85039138793945,
                140.43858337402344,
                290.2519226074219,
                284.50244140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "67d0a32a8d2e827ac907f864e536ba39",
        "text": "that each contribute plausibility to a conclusion84,85.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                296.3869934082031,
                230.8118133544922,
                309.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "985d8fd5c06e962e7a07705ca01183d2",
        "text": "Instead of translating the meaning of a French sentence into an \nEnglish sentence, one can learn to ‘translate’ the meaning of an image \ninto an English sentence (Fig. 3). The encoder here is a deep Con­\nvNet that converts the pixels into an activity vector in its last hidden \nlayer. The decoder is an RNN similar to the ones used for machine \ntranslation and neural language modelling. There has been a surge of \ninterest in such systems recently (see examples mentioned in ref. 86).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                307.160400390625,
                291.31597900390625,
                382.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "254925eb2f6de47a67f2a9d000f6a13b",
        "text": "RNNs, once unfolded in time (Fig. 5), can be seen as very deep \nfeedforward networks in which all the layers share the same weights. \nAlthough their main purpose is to learn long-term dependencies, \ntheoretical and empirical evidence shows that it is difficult to learn \nto store information for very long78.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                380.660400390625,
                291.2862548828125,
                435.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f3ce865a24c310fa354a287a23c39b7b",
        "text": "To correct for that, one idea is to augment the network with an \nexplicit memory. The first proposal of this kind is the long short-term \nmemory (LSTM) networks that use special hidden units, the natural \nbehaviour of which is to remember inputs for a long time79. A special \nunit called the memory cell acts like an accumulator or a gated leaky \nneuron: it has a connection to itself at the next time step that has a \nweight of one, so it copies its own real-valued state and accumulates \nthe external signal, but this self-connection is multiplicatively gated \nby another unit that learns to decide when to clear the content of the \nmemory.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                433.160400390625,
                291.3119201660156,
                540.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "055d02f2410ba0704a20038f24229f0f",
        "text": "LSTM networks have subsequently proved to be more effective \nthan conventional RNNs, especially when they have several layers for \neach time step87, enabling an entire speech recognition system that \ngoes all the way from acoustics to the sequence of characters in the \ntranscription. LSTM networks or related forms of gated units are also \ncurrently used for the encoder and decoder networks that perform \nso well at machine translation17,72,76.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                538.160400390625,
                291.3597106933594,
                613.7061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3d860d666efe63a0f66b83155c0be903",
        "text": "Over the past year, several authors have made different proposals to \naugment RNNs with a memory module. Proposals include the Neural \nTuring Machine in which the network is augmented by a ‘tape-like’ \nmemory that the RNN can choose to read from or write to88, and \nmemory networks, in which a regular network is augmented by a \nkind of associative memory89. Memory networks have yielded excel­\nlent performance on standard question-answering benchmarks. The \nmemory is used to remember the story about which the network is \nlater asked to answer questions.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                611.660400390625,
                291.35015869140625,
                708.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e17b1be952a766bb08469f2d26ca79d5",
        "text": "Beyond simple memorization, neural Turing machines and mem­\nory networks are being used for tasks that would normally require \nreasoning and symbol manipulation. Neural Turing machines can \nbe taught ‘algorithms’. Among other things, they can learn to output",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                706.160400390625,
                291.3206481933594,
                750.2061157226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d8669e9fb31cc6760ae5c902b90cc0c7",
        "text": "a sorted list of symbols when their input consists of an unsorted \nsequence in which each symbol is accompanied by a real value that \nindicates its priority in the list88. Memory networks can be trained \nto keep track of the state of the world in a setting similar to a text \nadventure game and after reading a story, they can answer questions \nthat require complex inference90. In one test example, the network is \nshown a 15-sentence version of the The Lord of the Rings and correctly \nanswers questions such as “where is Frodo now?”89.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                55.16039276123047,
                554.9583129882812,
                141.2061004638672
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "eddef3d3af393e95f14937dd6fd5de0e",
        "text": "The future of deep learning \nUnsupervised learning91–98 had a catalytic effect in reviving interest in \ndeep learning, but has since been overshadowed by the successes of \npurely supervised learning. Although we have not focused on it in this \nReview, we expect unsupervised learning to become far more important \nin the longer term. Human and animal learning is largely unsupervised: \nwe discover the structure of the world by observing it, not by being told \nthe name of every object.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                148.80809020996094,
                554.9622192382812,
                235.7061004638672
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0f9f452e2046bb95fc90af915bbd563e",
        "text": "Human vision is an active process that sequentially samples the optic \narray in an intelligent, task-speciﬁc way using a small, high-resolution \nfovea with a large, low-resolution surround. We expect much of the \nfuture progress in vision to come from systems that are trained end-to-\nend and combine ConvNets with RNNs that use reinforcement learning \nto decide where to look. Systems combining deep learning and rein­\nforcement learning are in their infancy, but they already outperform \npassive vision systems99 at classification tasks and produce impressive \nresults in learning to play many different video games100.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                233.660400390625,
                555.0067749023438,
                330.2060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7fcbb4b69af48207098a48e4fb56f154",
        "text": "Natural language understanding is another area in which deep learn­\ning is poised to make a large impact over the next few years. We expect \nsystems that use RNNs to understand sentences or whole documents \nwill become much better when they learn strategies for selectively \nattending to one part at a time76,86.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                328.160400390625,
                554.9525756835938,
                382.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ef36d2b99efae755c698217faa865c15",
        "text": "Ultimately, major progress in artificial intelligence will come about \nthrough systems that combine representation learning with complex \nreasoning. Although deep learning and simple reasoning have been \nused for speech and handwriting recognition for a long time, new \nparadigms are needed to replace rule-based manipulation of symbolic \nexpressions by operations on large vectors101. ■",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                380.660400390625,
                554.973388671875,
                445.7060852050781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4c048001216f8d72bc2425b1cf25e891",
        "text": "Received 25 February; accepted 1 May 2015.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                451.5120849609375,
                440.96014404296875,
                460.0240783691406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "99ef9616a1eaa585b704cd8f9f53a01d",
        "text": "1.\t\nKrizhevsky, A., Sutskever, I. & Hinton, G. ImageNet classification with deep \nconvolutional neural networks. In Proc. Advances in Neural Information \nProcessing Systems 25 1090–1098 (2012).\n\t\nThis report was a breakthrough that used convolutional nets to almost halve \nthe error rate for object recognition, and precipitated the rapid adoption of \ndeep learning by the computer vision community.\n2.\t\nFarabet, C., Couprie, C., Najman, L. & LeCun, Y. Learning hierarchical features for \nscene labeling. IEEE Trans. Pattern Anal. Mach. Intell. 35, 1915–1929 (2013). \n3.\t\nTompson, J., Jain, A., LeCun, Y. & Bregler, C. Joint training of a convolutional \nnetwork and a graphical model for human pose estimation. In Proc. Advances in \nNeural Information Processing Systems 27 1799–1807 (2014). \n4.\t\nSzegedy, C. et al. Going deeper with convolutions. Preprint at http://arxiv.org/\nabs/1409.4842 (2014). \n5.\t\nMikolov, T., Deoras, A., Povey, D., Burget, L. & Cernocky, J. Strategies for training \nlarge scale neural network language models. In Proc. Automatic Speech \nRecognition and Understanding 196–201 (2011). \n6.\t\nHinton, G. et al. Deep neural networks for acoustic modeling in speech \nrecognition. IEEE Signal Processing Magazine 29, 82–97 (2012).\n\t\nThis joint paper from the major speech recognition laboratories, summarizing \nthe breakthrough achieved with deep learning on the task of phonetic \nclassification for automatic speech recognition, was the first major industrial \napplication of deep learning.\n7.\t\nSainath, T., Mohamed, A.-R., Kingsbury, B. & Ramabhadran, B. Deep \nconvolutional neural networks for LVCSR. In Proc. Acoustics, Speech and Signal \nProcessing 8614–8618 (2013). \n8.\t\nMa, J., Sheridan, R. P., Liaw, A., Dahl, G. E. & Svetnik, V. Deep neural nets as a \nmethod for quantitative structure-activity relationships. J. Chem. Inf. Model. 55, \n263–274 (2015). \n9.\t\nCiodaro, T., Deva, D., de Seixas, J. & Damazio, D. Online particle detection with \nneural networks based on topological calorimetry information. J. Phys. Conf. \nSeries 368, 012030 (2012). \n10.\t Kaggle. Higgs boson machine learning challenge. Kaggle https://www.kaggle.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                467.5541076660156,
                554.5825805664062,
                723.9890747070312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cf8d49654989367147afc10c9bf8e43a",
        "text": "12.\t Leung, M. K., Xiong, H. Y., Lee, L. J. & Frey, B. J. Deep learning of the tissue-",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                55.26557922363281,
                275.5889892578125,
                63.70058059692383
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "555a1ae037c83dd3bcf1853477b29f80",
        "text": "regulated splicing code. Bioinformatics 30, i121–i129 (2014). \n13.\t Xiong, H. Y. et al. The human splicing code reveals new insights into the genetic",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                63.22357940673828,
                293.4225158691406,
                79.80061340332031
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "374596bb3c8bc3d821ad07969dfbfde1",
        "text": "determinants of disease. Science 347, 6218 (2015). \n14.\t Collobert, R., et al. Natural language processing (almost) from scratch. J. Mach.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                79.42359161376953,
                292.0645751953125,
                96.00062561035156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0b64305abb420834a9da4d12f7502229",
        "text": "Learn. Res. 12, 2493–2537 (2011). \n15.\t Bordes, A., Chopra, S. & Weston, J. Question answering with subgraph",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                95.62360382080078,
                265.8140563964844,
                112.10060119628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6ac62ce8d3e0f19d49970e13e0cd426d",
        "text": "embeddings. In Proc. Empirical Methods in Natural Language Processing http://\narxiv.org/abs/1406.3676v3 (2014). \n16.\t Jean, S., Cho, K., Memisevic, R. & Bengio, Y. On using very large target",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                111.66560363769531,
                289.95440673828125,
                136.20057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1fce39931f600d2b06fa56c27dbfddcd",
        "text": "vocabulary for neural machine translation. In Proc. ACL-IJCNLP http://arxiv.org/\nabs/1412.2007 (2015).\n17.\t Sutskever, I. Vinyals, O. & Le. Q. V. Sequence to sequence learning with neural",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                135.86561584472656,
                294.3454284667969,
                160.50062561035156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "62f95df5fc1644df7ab0d14356daf4bd",
        "text": "networks. In Proc. Advances in Neural Information Processing Systems 27 \n3104–3112 (2014). \n\t\nThis paper showed state-of-the-art machine translation results with the \narchitecture introduced in ref. 72, with a recurrent network trained to read a \nsentence in one language, produce a semantic representation of its meaning, \nand generate a translation in another language.\n18.\t Bottou, L. & Bousquet, O. The tradeoffs of large scale learning. In Proc. Advances",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                160.1656036376953,
                295.9870300292969,
                217.20057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6fc93f3ed2c537eb523f33e434d1a481",
        "text": "in Neural Information Processing Systems 20 161–168 (2007). \n19.\t Duda, R. O. & Hart, P. E. Pattern Classiﬁcation and Scene Analysis (Wiley, 1973). \n20.\t Schölkopf, B. & Smola, A. Learning with Kernels (MIT Press, 2002). \n21.\t Bengio, Y., Delalleau, O. & Le Roux, N. The curse of highly variable functions",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                216.86561584472656,
                289.5211486816406,
                249.60047912597656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "414009412f2d01c7db93062e0dde9b79",
        "text": "for local kernel machines. In Proc. Advances in Neural Information Processing \nSystems 18 107–114 (2005). \n22.\t Selfridge, O. G. Pandemonium: a paradigm for learning in mechanisation of",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                249.2655792236328,
                284.1202392578125,
                273.80059814453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "76d6aba386f09f67deec31ce6acd863d",
        "text": "thought processes. In Proc. Symposium on Mechanisation of Thought Processes \n513–526 (1958). \n23.\t Rosenblatt, F. The Perceptron — A Perceiving and Recognizing Automaton. Tech.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                273.3656005859375,
                292.18218994140625,
                297.80059814453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7b22bacf9795df2072801f0312a7f097",
        "text": "Rep. 85-460-1 (Cornell Aeronautical Laboratory, 1957). \n24.\t Werbos, P. Beyond Regression: New Tools for Prediction and Analysis in the",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                297.3656005859375,
                274.58416748046875,
                313.80059814453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c370521e813c871d65421dea2b045357",
        "text": "Behavioral Sciences. PhD thesis, Harvard Univ. (1974). \n25.\t Parker, D. B. Learning Logic Report TR–47 (MIT Press, 1985). \n26.\t LeCun, Y. Une procédure d’apprentissage pour Réseau à seuil assymétrique",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                313.3656005859375,
                285.2580261230469,
                337.80059814453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "88312019d9c3e91786275beba1aa0e0c",
        "text": "in Cognitiva 85: a la Frontière de l’Intelligence Artiﬁcielle, des Sciences de la \nConnaissance et des Neurosciences [in French] 599–604 (1985). \n27.\t Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                337.3656005859375,
                280.5765075683594,
                361.80059814453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "54ddf536310e03b03e5e4403b4f23900",
        "text": "back-propagating errors. Nature 323, 533–536 (1986). \n28.\t Glorot, X., Bordes, A. & Bengio. Y. Deep sparse rectiﬁer neural networks. In Proc.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                361.3235778808594,
                293.88525390625,
                377.80059814453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "60459da60d70ed997017a0541793572c",
        "text": "14th International Conference on Artificial Intelligence and Statistics 315–323 \n(2011). \n\t\nThis paper showed that supervised training of very deep neural networks is \nmuch faster if the hidden layers are composed of ReLU.\n29.\t Dauphin, Y. et al. Identifying and attacking the saddle point problem in high-",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51959991455078,
                377.3656005859375,
                287.6313171386719,
                417.9006042480469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ad64086fee120a6f0ae40a2a24cf61a6",
        "text": "dimensional non-convex optimization. In Proc. Advances in Neural Information \nProcessing Systems 27 2933–2941 (2014). \n30.\t Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B. & LeCun, Y. The loss",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51959991455078,
                417.56561279296875,
                289.05181884765625,
                442.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "99205ca3e8bd704584a286a08b02ce80",
        "text": "surface of multilayer networks. In Proc. Conference on AI and Statistics http://\narxiv.org/abs/1412.0233 (2014). \n31.\t Hinton, G. E. What kind of graphical model is the brain? In Proc. 19th",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                441.8656005859375,
                285.05859375,
                466.4006042480469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "97aae4487887d89af660a6e9e338841e",
        "text": "International Joint Conference on Artificial intelligence 1765–1775 (2005). \n32.\t Hinton, G. E., Osindero, S. & Teh, Y.-W. A fast learning algorithm for deep belief",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                465.9656066894531,
                289.67303466796875,
                482.4006042480469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0cd3e6a1360987161b4d89ed17c92912",
        "text": "nets. Neural Comp. 18, 1527–1554 (2006).\n\t\nThis paper introduced a novel and effective way of training very deep neural \nnetworks by pre-training one hidden layer at a time using the unsupervised \nlearning procedure for restricted Boltzmann machines. \n33.\t Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. Greedy layer-wise training",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                481.923583984375,
                289.66693115234375,
                522.4005737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3080fc2425f976fbf1747ab5bc5275d5",
        "text": "of deep networks. In Proc. Advances in Neural Information Processing Systems 19 \n153–160 (2006). \n\t\nThis report demonstrated that the unsupervised pre-training method \nintroduced in ref. 32 significantly improves performance on test data and \ngeneralizes the method to other unsupervised representation-learning \ntechniques, such as auto-encoders.\n34.\t Ranzato, M., Poultney, C., Chopra, S. & LeCun, Y. Efﬁcient learning of sparse",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51959991455078,
                521.965576171875,
                295.70513916015625,
                578.5006103515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "707cf570b48f6d2f1ff35028fae02d9e",
        "text": "representations with an energy-based model. In Proc. Advances in Neural \nInformation Processing Systems 19 1137–1144 (2006). \n35.\t Hinton, G. E. & Salakhutdinov, R. Reducing the dimensionality of data with",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                578.1655883789062,
                278.8999938964844,
                602.8005981445312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4a4afe7a9fae6d2a2837d9c50e6fba66",
        "text": "neural networks. Science 313, 504–507 (2006). \n36.\t Sermanet, P., Kavukcuoglu, K., Chintala, S. & LeCun, Y. Pedestrian detection with",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                602.423583984375,
                296.2232971191406,
                619.0006103515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4682c76ae54bb7905a88288c889ef3fd",
        "text": "unsupervised multi-stage feature learning. In Proc. International Conference \non Computer Vision and Pattern Recognition http://arxiv.org/abs/1212.0142 \n(2013). \n37.\t Raina, R., Madhavan, A. & Ng, A. Y. Large-scale deep unsupervised learning",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                618.6655883789062,
                284.9375915527344,
                651.4005737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0040bf701f28165f3cf53f1b43a282e4",
        "text": "using graphics processors. In Proc. 26th Annual International Conference on \nMachine Learning 873–880 (2009). \n38.\t Mohamed, A.-R., Dahl, G. E. & Hinton, G. Acoustic modeling using deep belief",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                651.0656127929688,
                286.760986328125,
                675.7005615234375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "73cdd80357514d70fbbd9239bcd116ec",
        "text": "networks. IEEE Trans. Audio Speech Lang. Process. 20, 14–22 (2012). \n39.\t Dahl, G. E., Yu, D., Deng, L. & Acero, A. Context-dependent pre-trained deep",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                675.3236083984375,
                280.5831298828125,
                691.9005737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2d49a37305426518e2d28ec782fa6c7c",
        "text": "neural networks for large vocabulary speech recognition. IEEE Trans. Audio \nSpeech Lang. Process. 20, 33–42 (2012). \n40.\t Bengio, Y., Courville, A. & Vincent, P. Representation learning: a review and new",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                691.5656127929688,
                292.5863952636719,
                716.2005615234375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f821b375b0585fc56d4c2034ccfec3c7",
        "text": "perspectives. IEEE Trans. Pattern Anal. Machine Intell. 35, 1798–1828 (2013). \n41.\t LeCun, Y. et al. Handwritten digit recognition with a back-propagation network.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                42.51969909667969,
                715.8236083984375,
                292.0967102050781,
                732.4005737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f4864611845e983bad33a88217db018a",
        "text": "for the task of classifying low-resolution images of handwritten digits.\n42.\t LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                55.22357940673828,
                556.36083984375,
                71.80061340332031
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "aa7cd317d997db4235f4d2f5f5416a20",
        "text": "document recognition. Proc. IEEE 86, 2278–2324 (1998). \n\t\nThis overview paper on the principles of end-to-end training of modular \nsystems such as deep neural networks using gradient-based optimization \nshowed how neural networks (and in particular convolutional nets) can be \ncombined with search or inference mechanisms to model complex outputs \nthat are interdependent, such as sequences of characters associated with the \ncontent of a document.\n43.\t Hubel, D. H. & Wiesel, T. N. Receptive ﬁelds, binocular interaction, and functional",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                71.42359161376953,
                560.5097045898438,
                136.60060119628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5b7acdc028f5f897111091ddb2fec235",
        "text": "architecture in the cat’s visual cortex. J. Physiol. 160, 106–154 (1962). \n44.\t Felleman, D. J. & Essen, D. C. V. Distributed hierarchical processing in the",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                136.2235870361328,
                538.5786743164062,
                152.8006134033203
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "24155d5ccfc3517be5d04b475a48c044",
        "text": "primate cerebral cortex. Cereb. Cortex 1, 1–47 (1991). \n45.\t Cadieu, C. F. et al. Deep neural networks rival the representation of primate",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                152.42359924316406,
                544.2899169921875,
                169.00062561035156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d7eaf21dc269e1ff6823f240ad02ef32",
        "text": "it cortex for core visual object recognition. PLoS Comp. Biol. 10, e1003963 \n(2014). \n46.\t Fukushima, K. & Miyake, S. Neocognitron: a new algorithm for pattern",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                168.6236114501953,
                543.1920166015625,
                193.20057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2de8d9528ee928e683a0bac407e3de79",
        "text": "recognition tolerant of deformations and shifts in position. Pattern Recognition \n15, 455–469 (1982). \n47.\t Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K. & Lang, K. Phoneme",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                192.7655792236328,
                554.8522338867188,
                217.20057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b3f61186ac6ffea253a43191b3c9291c",
        "text": "recognition using time-delay neural networks. IEEE Trans. Acoustics Speech \nSignal Process. 37, 328–339 (1989). \n48.\t Bottou, L., Fogelman-Soulié, F., Blanchet, P. & Lienard, J. Experiments with time",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                216.7655792236328,
                556.35498046875,
                241.20057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c0dffe5154a72e9f630eba467cfb2328",
        "text": "delay networks and dynamic time warping for speaker independent isolated \ndigit recognition. In Proc. EuroSpeech 89 537–540 (1989). \n49.\t Simard, D., Steinkraus, P. Y. & Platt, J. C. Best practices for convolutional neural",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                240.7655792236328,
                555.8147583007812,
                265.2005920410156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6ecc86f1d04b557df0b02604559d09dd",
        "text": "networks. In Proc. Document Analysis and Recognition 958–963 (2003). \n50.\t Vaillant, R., Monrocq, C. & LeCun, Y. Original approach for the localisation of",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                264.765625,
                546.8720092773438,
                281.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "993b930a75a04d6ac08b24f29a61a62c",
        "text": "objects in images. In Proc. Vision, Image, and Signal Processing 141, 245–250 \n(1994). \n51.\t Nowlan, S. & Platt, J. in Neural Information Processing Systems 901–908 (1995). \n52.\t Lawrence, S., Giles, C. L., Tsoi, A. C. & Back, A. D. Face recognition: a",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                280.7236022949219,
                557.9851684570312,
                313.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e8913c7f2d69b577d3fcedae8c584d4f",
        "text": "convolutional neural-network approach. IEEE Trans. Neural Networks 8, 98–113 \n(1997). \n53.\t Ciresan, D., Meier, U. Masci, J. & Schmidhuber, J. Multi-column deep neural",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                312.7236022949219,
                556.8164672851562,
                337.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c79a65932a54c6032a062a3d5567a5b9",
        "text": "network for trafﬁc sign classiﬁcation. Neural Networks 32, 333–338 (2012). \n54.\t Ning, F. et al. Toward automatic phenotyping of developing embryos from",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                336.7236022949219,
                545.8372192382812,
                353.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "af0eeb529de200be6102360d2de88e21",
        "text": "videos. IEEE Trans. Image Process. 14, 1360–1371 (2005). \n55.\t Turaga, S. C. et al. Convolutional networks can learn to generate afﬁnity graphs",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                352.7236022949219,
                555.4910888671875,
                369.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3ebeb75d810d81fafc8bcfc00f13ddbd",
        "text": "for image segmentation. Neural Comput. 22, 511–538 (2010). \n56.\t Garcia, C. & Delakis, M. Convolutional face ﬁnder: a neural architecture for",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                368.7236022949219,
                542.6759643554688,
                385.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f74c58fe8a66ca1700f3b385a3f4d658",
        "text": "fast and robust face detection. IEEE Trans. Pattern Anal. Machine Intell. 26, \n1408–1423 (2004). \n57.\t Osadchy, M., LeCun, Y. & Miller, M. Synergistic face detection and pose",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                384.7236022949219,
                539.5701293945312,
                409.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c812ce5cd417a674462957eca73c66b9",
        "text": "estimation with energy-based models. J. Mach. Learn. Res. 8, 1197–1215 \n(2007). \n58.\t Tompson, J., Goroshin, R. R., Jain, A., LeCun, Y. Y. & Bregler, C. C. Efﬁcient object",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                408.7236022949219,
                556.181396484375,
                433.20062255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2e2f8a8683a123a943f8f07c4e69bab5",
        "text": "localization using convolutional networks. In Proc. Conference on Computer \nVision and Pattern Recognition http://arxiv.org/abs/1411.4280 (2014). \n59.\t Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. Deepface: closing the gap to",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                432.765625,
                544.7823486328125,
                457.1006164550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c4e168135671bc36bd8d77a3efa26a48",
        "text": "human-level performance in face veriﬁcation. In Proc. Conference on Computer \nVision and Pattern Recognition 1701–1708 (2014). \n60.\t Hadsell, R. et al. Learning long-range vision for autonomous off-road driving.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                456.56561279296875,
                555.850830078125,
                480.80059814453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c7605ae4d24783a948e974cec7101ba0",
        "text": "J. Field Robot. 26, 120–144 (2009). \n61.\t Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Scene parsing with multiscale",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                480.2236022949219,
                550.5029296875,
                496.6006164550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0a8ed5ea1dbbe9fb13af6ffbaeba6b95",
        "text": "feature learning, purity trees, and optimal covers. In Proc. International \nConference on Machine Learning http://arxiv.org/abs/1202.2160 (2012). \n62.\t Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1416931152344,
                496.06561279296875,
                539.8706665039062,
                520.300537109375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "667d4d6224d8475922d2ad45c7b87948",
        "text": "Dropout: a simple way to prevent neural networks from overﬁtting. J. Machine \nLearning Res. 15, 1929–1958 (2014). \n63.\t Sermanet, P. et al. Overfeat: integrated recognition, localization and detection",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                519.7655639648438,
                553.1929931640625,
                544.0006103515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "68cf197edbf457e04f05f079ff4adb6e",
        "text": "using convolutional networks. In Proc. International Conference on Learning \nRepresentations http://arxiv.org/abs/1312.6229 (2014). \n64.\t Girshick, R., Donahue, J., Darrell, T. & Malik, J. Rich feature hierarchies for",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                543.465576171875,
                543.4066162109375,
                567.7005615234375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e475285e26741cafd9fbbdeb8c4fd9a7",
        "text": "accurate object detection and semantic segmentation. In Proc. Conference on \nComputer Vision and Pattern Recognition 580–587 (2014). \n65.\t Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                567.1655883789062,
                556.4547119140625,
                591.4005737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f24a441dfcc0b1843cd75ef63e414720",
        "text": "image recognition. In Proc. International Conference on Learning Representations \nhttp://arxiv.org/abs/1409.1556 (2014). \n66.\t Boser, B., Sackinger, E., Bromley, J., LeCun, Y. & Jackel, L. An analog neural",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                590.8656005859375,
                558.5075073242188,
                615.1005859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bac915a3525cbc2f6ced6d81bbd2639e",
        "text": "network processor with programmable topology. J. Solid State Circuits 26, \n2017–2025 (1991). \n67.\t Farabet, C. et al. Large-scale FPGA-based convolutional networks. In Scaling",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                614.5236206054688,
                546.4276733398438,
                638.8005981445312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5192ea0ca452b4ad82aa2137b3e69a30",
        "text": "up Machine Learning: Parallel and Distributed Approaches (eds Bekkerman, R., \nBilenko, M. & Langford, J.) 399–419 (Cambridge Univ. Press, 2011). \n68.\t Bengio, Y. Learning Deep Architectures for AI (Now, 2009). \n69.\t Montufar, G. & Morton, J. When does a mixture of products contain a product of",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                638.2655639648438,
                559.1207885742188,
                670.4005737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a0a18d11ea07d3754049423be42b320f",
        "text": "mixtures? J. Discrete Math. 29, 321–347 (2014). \n70.\t Montufar, G. F., Pascanu, R., Cho, K. & Bengio, Y. On the number of linear regions",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                669.8236083984375,
                559.9526977539062,
                686.2005615234375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e3877013118c3eb85abd537ef1864228",
        "text": "of deep neural networks. In Proc. Advances in Neural Information Processing \nSystems 27 2924–2932 (2014). \n71.\t Bengio, Y., Ducharme, R. & Vincent, P. A neural probabilistic language model. In",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                685.6655883789062,
                557.42578125,
                709.9005737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e44e6f47ad9b6898854ed0c7881a5b7a",
        "text": "Proc. Advances in Neural Information Processing Systems 13 932–938 (2001). \n\t\nThis paper introduced neural language models, which learn to convert a word \nsymbol into a word vector or word embedding composed of learned semantic \nfeatures in order to predict the next word in a sequence.\n72.\t Cho, K. et al. Learning phrase representations using RNN encoder-decoder",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                306.1418151855469,
                709.3656005859375,
                558.2567749023438,
                749.4005737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "37209328e87f438f449b6d9f3f81005e",
        "text": "for statistical machine translation. In Proc. Conference on Empirical Methods in \nNatural Language Processing 1724–1734 (2014).  \n73.\t Schwenk, H. Continuous space language models. Computer Speech Lang. 21,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                55.26557922363281,
                284.1629943847656,
                79.63560485839844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "719550b28ecfdfa705be90f533029477",
        "text": "492–518 (2007). \n74.\t Socher, R., Lin, C. C-Y., Manning, C. & Ng, A. Y. Parsing natural scenes and",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                79.16560363769531,
                269.2147521972656,
                95.60060119628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5f10e6cc09170dc69af4294f0f7964a8",
        "text": "natural language with recursive neural networks. In Proc. International \nConference on Machine Learning 129–136 (2011). \n75.\t Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J. Distributed",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                95.16560363769531,
                259.7556457519531,
                119.60060119628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d2f187eadcba988fd32d5c1cada69a34",
        "text": "representations of words and phrases and their compositionality. In Proc. \nAdvances in Neural Information Processing Systems 26 3111–3119 (2013). \n76.\t Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.850399017333984,
                119.16560363769531,
                273.49835205078125,
                143.60060119628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "43ac9ba58d1632a82f094fb79f059eef",
        "text": "learning to align and translate. In Proc. International Conference on Learning \nRepresentations http://arxiv.org/abs/1409.0473 (2015).\n77.\t Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen [in",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                143.1656036376953,
                276.00640869140625,
                167.50062561035156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "93fc0ebe359fc2aa40d0c749533e99c4",
        "text": "German] Diploma thesis, T.U. Münich (1991). \n78.\t Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                167.0656280517578,
                273.63623046875,
                183.60060119628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cbca3f8e2cb8a1b3983050353517b6c3",
        "text": "gradient descent is difﬁcult. IEEE Trans. Neural Networks 5, 157–166 (1994). \n79.\t Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9,",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                183.2235870361328,
                282.7515563964844,
                199.8356170654297
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "83720e3a0561f1dc3e32d99ed4fc6da1",
        "text": "1735–1780 (1997). \n\t\nThis paper introduced LSTM recurrent networks, which have become a crucial \ningredient in recent advances with recurrent networks because they are good \nat learning long-range dependencies. \n80.\t ElHihi, S. & Bengio, Y. Hierarchical recurrent neural networks for long-term",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                199.46559143066406,
                290.58636474609375,
                240.3006134033203
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "66efbeb164292dbd023b95159adc0b27",
        "text": "dependencies. In Proc. Advances in Neural Information Processing Systems 8 \nhttp://papers.nips.cc/paper/1102-hierarchical-recurrent-neural-networks-for-\nlong-term-dependencies (1995). \n81.\t Sutskever, I. Training Recurrent Neural Networks. PhD thesis, Univ. Toronto",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                239.96559143066406,
                285.8284606933594,
                272.6006164550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0b6b66f5a391c6c61454abec15c8f1f7",
        "text": "(2012). \n82.\t Pascanu, R., Mikolov, T. & Bengio, Y. On the difﬁculty of training recurrent neural",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                272.1656188964844,
                289.3774719238281,
                288.6006164550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3f5034773a04e2d53260e4e0383f40ea",
        "text": "networks. In Proc. 30th International Conference on Machine Learning 1310–\n1318 (2013). \n83.\t Sutskever, I., Martens, J. & Hinton, G. E. Generating text with recurrent neural",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                288.1656188964844,
                279.90655517578125,
                312.6006164550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ffd072c0389e7906f5ca547b73eb520a",
        "text": "networks. In Proc. 28th International Conference on Machine Learning 1017–\n1024 (2011). \n84.\t Lakoff, G. & Johnson, M. Metaphors We Live By (Univ. Chicago Press, 2008). \n85.\t Rogers, T. T. & McClelland, J. L. Semantic Cognition: A Parallel Distributed",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                312.1656188964844,
                275.3185119628906,
                344.80059814453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6b44a37d92ec9bb490690b98dd0a23d9",
        "text": "Processing Approach (MIT Press, 2004). \n86.\t Xu, K. et al. Show, attend and tell: Neural image caption generation with visual",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                344.4656066894531,
                284.39935302734375,
                360.9006042480469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1c95dc02f81e22ef74e77e9fb43fe07e",
        "text": "attention. In Proc. International Conference on Learning Representations http://\narxiv.org/abs/1502.03044 (2015). \n87.\t Graves, A., Mohamed, A.-R. & Hinton, G. Speech recognition with deep recurrent",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                360.4656066894531,
                290.4141540527344,
                384.9006042480469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "881376b03fff80dbec9965ac4f28d946",
        "text": "neural networks. In Proc. International Conference on Acoustics, Speech and \nSignal Processing 6645–6649 (2013). \n88.\t Graves, A., Wayne, G. & Danihelka, I. Neural Turing machines. http://arxiv.org/",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                384.4656066894531,
                281.6627502441406,
                408.9006042480469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dbf4c8b97ca6f81c122250ee50d3b60e",
        "text": "abs/1410.5401 (2014). \n89.\t Weston, J. Chopra, S. & Bordes, A. Memory networks. http://arxiv.org/",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                36.85029983520508,
                408.4656066894531,
                258.31463623046875,
                425.0006103515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c9353f26d63c8bcdcd484f16e2ce1806",
        "text": "abs/1410.3916 (2014).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                51.02349853515625,
                424.6656188964844,
                125.01142120361328,
                433.1006164550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ae30455427f705bdbbf6061da5241984",
        "text": "90.\t Weston, J., Bordes, A., Chopra, S. & Mikolov, T. Towards AI-complete question",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                55.26557922363281,
                543.5767211914062,
                63.70058059692383
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b14a44e8f44121bc7c90d068afce7605",
        "text": "answering: a set of prerequisite toy tasks. http://arxiv.org/abs/1502.05698 \n(2015). \n91.\t Hinton, G. E., Dayan, P., Frey, B. J. & Neal, R. M. The wake-sleep algorithm for",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                63.36561584472656,
                542.03515625,
                88.00062561035156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7efb5df1495b74d861ae04b5c8a9d245",
        "text": "unsupervised neural networks. Science 268, 1558–1161 (1995). \n92.\t Salakhutdinov, R. & Hinton, G. Deep Boltzmann machines. In Proc. International",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                87.62360382080078,
                552.1658935546875,
                104.20057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "06d23cd2f4ee4fd1e67b4479b06d5bbb",
        "text": "Conference on Artificial Intelligence and Statistics 448–455 (2009). \n93.\t Vincent, P., Larochelle, H., Bengio, Y. & Manzagol, P.-A. Extracting and composing",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                103.86561584472656,
                554.8151245117188,
                120.10060119628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c7b17ad154869c005d7d0a09c803f68b",
        "text": "robust features with denoising autoencoders. In Proc. 25th International \nConference on Machine Learning 1096–1103 (2008). \n94.\t Kavukcuoglu, K. et al. Learning convolutional feature hierarchies for visual",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                119.46559143066406,
                536.1631469726562,
                143.70057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "44fe2464c8328f4d149928c54873bfcd",
        "text": "recognition. In Proc. Advances in Neural Information Processing Systems 23 \n1090–1098 (2010). \n95.\t Gregor, K. & LeCun, Y. Learning fast approximations of sparse coding. In Proc.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                143.2655792236328,
                546.15234375,
                167.70057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "91941a8744ffd702d20dbb079a0ca697",
        "text": "International Conference on Machine Learning 399–406 (2010). \n96.\t Ranzato, M., Mnih, V., Susskind, J. M. & Hinton, G. E. Modeling natural images",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                167.2655792236328,
                545.0797729492188,
                183.70057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "924921193fff9ff384e71458594971a5",
        "text": "using gated MRFs. IEEE Trans. Pattern Anal. Machine Intell. 35, 2206–2222 \n(2013). \n97.\t Bengio, Y., Thibodeau-Laufer, E., Alain, G. & Yosinski, J. Deep generative",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                183.2235870361328,
                536.2966918945312,
                207.70057678222656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3f408245c3938922c69fe9c417e7e81a",
        "text": "stochastic networks trainable by backprop. In Proc. 31st International \nConference on Machine Learning 226–234 (2014). \n98.\t Kingma, D., Rezende, D., Mohamed, S. & Welling, M. Semi-supervised learning",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                207.2655792236328,
                548.4512329101562,
                231.8006134033203
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b3eeb9d6de79184bf973efad8a8e3867",
        "text": "with deep generative models. In Proc. Advances in Neural Information Processing \nSystems 27 3581–3589 (2014). \n99.\t Ba, J., Mnih, V. & Kavukcuoglu, K. Multiple object recognition with visual",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                231.46559143066406,
                552.9860229492188,
                256.1004943847656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dbc629df9bb093411dc929902c194308",
        "text": "attention. In Proc. International Conference on Learning Representations http://\narxiv.org/abs/1412.7755 (2014). \n100.\tMnih, V. et al. Human-level control through deep reinforcement learning. Nature",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                255.7655792236328,
                554.704345703125,
                280.3355712890625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fc7a71991b0c37a624e0a2973906bea1",
        "text": "518, 529–533 (2015).\n101.\tBottou, L. From machine learning to machine reasoning. Mach. Learn. 94,",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                279.8235778808594,
                535.1813354492188,
                296.4355773925781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "296e0fed4069345a86418843f1549397",
        "text": "133–149 (2014). \n102.\tVinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image",
        "type": "Title",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                296.06561279296875,
                537.1270141601562,
                312.6006164550781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5631dcf65d327fe47746f0093a9ee282",
        "text": "caption generator. In Proc. International Conference on Machine Learning http://\narxiv.org/abs/1502.03044 (2014).\n103.\tvan der Maaten, L. & Hinton, G. E. Visualizing data using t-SNE. J. Mach. Learn.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                312.265625,
                549.48974609375,
                336.9006042480469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6d2e8942efa24698f5a60c852b17f957",
        "text": "Research 9, 2579–2605 (2008).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                314.6455993652344,
                336.5235900878906,
                410.58050537109375,
                345.03558349609375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f81916a8f891a7c13c6ab87d4e7ada56",
        "text": "Acknowledgements The authors would like to thank the Natural Sciences and \nEngineering Research Council of Canada, the Canadian Institute For Advanced \nResearch (CIFAR), the National Science Foundation and Office of Naval Research \nfor support. Y.L. and Y.B. are CIFAR fellows.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                352.5235900878906,
                545.0060424804688,
                385.0006103515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "93061a05c31cad0d3e2a83fe352ff9a3",
        "text": "Author Information Reprints and permissions information is available at \nwww.nature.com/reprints. The authors declare no competing financial \ninterests. Readers are welcome to comment on the online version of this \npaper at go.nature.com/7cjbaa. Correspondence should be addressed to Y.L. \n(yann@cs.nyu.edu).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "nature14539.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                300.472412109375,
                392.5235900878906,
                534.4393920898438,
                433.0006103515625
            ],
            "is_full_width": false
        }
    }
]