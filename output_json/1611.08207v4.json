[
    {
        "element_id": "e71bd525d965e2d4d3ba9d5cb85f7875",
        "text": "Texture Synthesis with Spatial Generative\nAdversarial Networks",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                151.8979949951172,
                99.8338394165039,
                459.537353515625,
                136.97422790527344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "42abe94dfda94dde775c57af44b93528",
        "text": "Nikolay Jetchev∗\nUrs Bergmann∗",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                194.6540069580078,
                176.28836059570312,
                421.4256896972656,
                187.7600860595703
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "56334f326ce25e7f645bdbe4f97a976f",
        "text": "Roland Vollgraf\nZalando Research\n{nikolay.jetchev,urs.bergmann,roland.vollgraf}@zalando.de",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                186.1400146484375,
                206.79547119140625,
                425.8600158691406,
                244.12203979492188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c6b09c50aaa44f7947e667f9cc87c446",
        "text": "arXiv:1611.08207v4  [cs.CV]  8 Sep 2017",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                10.940000534057617,
                216.1400146484375,
                37.619998931884766,
                555.0
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fe6cdfe15b8d8487569ad51e67d4cc22",
        "text": "1\nAbstract",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.00001525878906,
                288.130126953125,
                170.41810607910156,
                300.0853271484375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6f114ddf711ab76d0d9d493e4aa298e9",
        "text": "Generative adversarial networks (GANs) [7] are a recent approach to train generative models of data,\nwhich have been shown to work particularly well on image data. In the current paper we introduce a\nnew model for texture synthesis based on GAN learning. By extending the input noise distribution\nspace from a single vector to a whole spatial tensor, we create an architecture with properties well\nsuited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is\nthe ﬁrst successful completely data-driven texture synthesis method based on GANs.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                313.7674560546875,
                505.24114990234375,
                378.27508544921875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f2659b7d780fe8aacf0dd0d4e09ba779",
        "text": "Our method has the following features which make it a state of the art algorithm for texture synthesis:\nhigh image quality of the generated textures, very high scalability w.r.t. the output texture size, fast\nreal-time forward generation, the ability to fuse multiple diverse source images in complex textures.\nTo illustrate these capabilities we present multiple experiments with different classes of texture\nimages and use cases. We also discuss some limitations of our method with respect to the types of\ntexture images it can synthesize, and compare it to other neural techniques for texture generation.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                384.77740478515625,
                505.7466735839844,
                449.2090759277344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "abbb46d020648db1eab843247cd72a81",
        "text": "2\nIntroduction",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                467.5261535644531,
                190.8136749267578,
                479.4813537597656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0a25c51a58cd1559ffe0d41adc50bc19",
        "text": "2.1\nBackground: texture synthesis",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                493.0735168457031,
                260.16876220703125,
                503.0361022949219
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "639fd184b29cee2394f3ef434c460464",
        "text": "A texture can be deﬁned as an image containing repeating patterns with some amount of randomness.\nMore formally, a texture is a realization of a stationary ergodic stochastic process [6]. The goal of\nvisual texture analysis is to infer a generating process from an example texture, which then allows to\ngenerate arbitrarily many new samples of that texture - hence performing texture synthesis. Success\nin that task is judged primarily by visual quality and closeness to the original texture as estimated by\nhuman observers, but also by other criteria which may be application speciﬁc [17], e.g. the speed\nof analysis and synthesis, ability to generate diverse textures of arbitrary size, the ability to create\nsmoothly morphing textures.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                513.8059692382812,
                505.745849609375,
                600.0680541992188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "986bc4b5ee04ef21d492c531ca63aa13",
        "text": "Approaches to do that fall in two broad categories. Non-parametric techniques resample pixels [3]\nor whole patches [2] from example textures, effectively randomizing the input texture in ways that\npreserve its visual perceptual properties. They can produce high quality images, but have two\ndrawbacks: (i) they do not \"learn\" any models of the textures of interest but just reorder the input\ntexture using local similarity, and (ii) they can be time-consuming when large textures should be\nsynthesized because of all the search routines involved. There are methods to accelerate example-\nbased techniques [17], but this requires complicated algorithms.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                606.4407958984375,
                505.6538391113281,
                681.9110717773438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0ba70156c4321f21dc6061876943af33",
        "text": "∗These authors contributed equally to this work.\nOur source code is available at https://github.com/zalandoresearch/spatial_gan",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 1,
            "languages": [
                "eng"
            ],
            "coordinates": [
                119.8219985961914,
                691.3728637695312,
                452.8109130859375,
                711.9345703125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ceb6abe778b07c285d82228e61356714",
        "text": "Input\nSGAN5\nGatys et al.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                166.94400024414062,
                208.64895629882812,
                462.1638488769531,
                217.6153564453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c62130991dec0f24f63ca21d5dc84e92",
        "text": "Figure 1: Learning a texture from a satellite image of Barcelona of size 1371x647 pixels. We visualize\na 647x647 pixel subset of the training and generated images, and for comparison draw the 125 pixel\nSGAN5 receptive ﬁeld (yellow box, top-left corner of the left image). Our adversarial approach to\ntexture synthesis SGAN (with 5 layers) generates a city texture of higher visual quality (e.g. clearly\nvisible city streets) than the output of the method of Gatys [5].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.7509994506836,
                229.6173858642578,
                504.34423828125,
                283.14007568359375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6cc54b6690484288076072f4dd6a1518",
        "text": "The second category of texture synthesis methods is based on matching statistical properties or\ndescriptors of images. Texture synthesis is then equivalent to ﬁnding an image with similar descriptors,\nusually by solving an optimization problem in the space of image pixels. The work of Portilla and\nSimoncelli [12] is a notable example of this approach, which yields very good image quality for some\ntextures. Carefully designed descriptors over spatial locations, orientations, and scales are used to\nrepresent statistics over target textures.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                308.30029296875,
                505.241455078125,
                372.8830871582031
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "715ed2fffbad8bfe6c736ad0dc780d3e",
        "text": "Gatys et al. [5] present a more data driven parametric approach to allow generation of high quality\ntextures over a variety of natural images. Using ﬁlter correlations in different layers of the convolu-\ntional networks – trained discriminatively on large natural image collections – results in a powerful\ntechnique that nicely captures expressive image statistics. However, creating a single output texture\nrequires solving an optimization problem with iterative backpropagation, which is costly – in time\nand memory.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                379.2567443847656,
                505.6531066894531,
                443.81707763671875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "848d7a9532a0fb87672df210dd3b9976",
        "text": "Recent papers [16, 9] deal with that problem and train feed-forward convolutional networks in order\nto speed up the texture synthesis approach of [5]. Instead of doing the costly optimization of the\noutput image pixels, they utilize powerful deep learning networks that are trained to produce images\nminimizing the loss. A separate network is trained for each texture of interest and can then quickly\ncreate an image with the desired statistics in one forward pass.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                450.24346923828125,
                504.3491516113281,
                503.8420715332031
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d57f5b44486adcaf1fc35e1229830f7f",
        "text": "A generative approach to texture synthesis [15] uses a recurrent neural network to learn the pixel\nprobabilities and statistical dependencies of natural images. They obtain good texture quality on\nmany image types, but their method is computationally expensive and this makes it less practical for\ntexture generation in cases where size and speed matter.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                510.1932678222656,
                504.16552734375,
                552.9580688476562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6bad4081027f835f5f3ce2bc2f6a1ab6",
        "text": "2.2\nAn adversarial approach to texture synthesis",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                570.9095458984375,
                320.6217956542969,
                580.8721313476562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f8544e6b03b1f4d0fdb780f8c8d9eaa2",
        "text": "We will present a novel class of generative parametric models for texture synthesis, using a fully\nconvolutional architecture trained employing an adversarial criterion.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.53199768066406,
                592.3192749023438,
                504.3524169921875,
                613.2660522460938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a860a2893efa86bf3e58a54edd6868e0",
        "text": "As introduced in [7], GANs train a generative model G that captures the data distribution and a\ndiscriminator D that attempts to separate generated from training data. Radford et al. [13] improved\nthe GAN architecture by using deep convolutional layers with (fractional) stride [14] and batch\nnormalization [8]. Overall, GANs are powerful enough to generate natural looking images of high\nquality (but low pixel resolution) that can confuse even human observers [4].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                619.4619140625,
                504.0038757324219,
                673.2910766601562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c5df3d27de06018cb933ef99a59126c3",
        "text": "However, in GANs the size of the output image (e.g. 64x64 pixels) is hard coded into the network\narchitecture. This is a limitation for texture synthesis, where much larger textures and multiple sizes\nmay be required. Laplacian pyramids have been used to generate images of increasing size [4],\ngradually adding more details to the images with stacked conditional GANs. However, that technique",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 2,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                679.6497802734375,
                505.2453918457031,
                722.3828735351562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "502a5a24be16925e2659b78cad5d3ff9",
        "text": "G(Z)\nX\nD(X)",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                273.1092529296875,
                70.42947387695312,
                377.3539123535156,
                81.20892333984375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4ecc7dd4054b67da37a5e0bf7418a1b3",
        "text": "Z",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                198.96517944335938,
                106.66287231445312,
                205.03900146484375,
                116.97508239746094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c6422efcc1d97fc50b32c620bcb3dc14",
        "text": "l",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                158.91555786132812,
                127.35882568359375,
                160.3892364501953,
                133.546142578125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7a2a90ca7003a988385815e2cd862b37",
        "text": "a)",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                144.46995544433594,
                127.87345886230469,
                155.1315155029297,
                140.24810791015625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "178d6bda96e3ee1458ba2f598df4ad7e",
        "text": "m",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                167.8807373046875,
                153.6248321533203,
                173.06256103515625,
                159.81214904785156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "14dfd71f327c1ed1fc21ce7b4e5c6c68",
        "text": "d",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                208.08824157714844,
                162.57533264160156,
                211.46121215820312,
                168.7626495361328
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9d404df06932e7a04381a2c8bf1f1e39",
        "text": "b)",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                144.14263916015625,
                189.12496948242188,
                155.0382843017578,
                201.49961853027344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "68fc15771b6c1aff64834bde37234916",
        "text": "I\nX'\nD(X')",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                183.8979949951172,
                295.21099853515625,
                395.4364929199219,
                309.77447509765625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f686e8a097b83fa4005a60140d33aa7d",
        "text": "Figure 2: Spatial GAN model (SGAN): a generator G transforms a spatial noise array Z ∈Rl×m×d\ninto an RGB image X ∈Rh×w×3 via a stack of fractionally strided convolution layers. The\ndiscriminator D is fed either a generated image (X - case (a)) or a rectangular patch extracted from\nan image I of a database (X′ - case (b)). It uses a stack of convolutional layers to output a 2D\nﬁeld of probabilities for fake/real (X vs. X′) data. The detailed architecture of both generator and\ndiscriminator (the “funnels”) is akin to [13], but varies in having exclusively convolutional layers and\na potentially different number of hidden layers. The subnetwork that projects a single vector of the\narray Z, i.e. zλµ ∈Rd, to the generated output image is equivalent to a standard GAN (see e.g. the\ngreen blocks in the ﬁgure). However, non-overlapping vectors have overlapping projective ﬁelds in\nthe output. In this view, SGAN is a convolutional roll-out of GANs.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                329.8594055175781,
                504.0042419433594,
                440.41107177734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "587a226b61aed00c40f61e7d2f7c9362",
        "text": "is still limited in the output image sizes it can handle because it needs to train GAN models with\nincreasing complexity for each scale in the image pyramid. The scale levels must also be speciﬁed in\nadvance, so the method cannot create output of arbitrary size.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                466.6692810058594,
                503.99951171875,
                498.52508544921875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "07a3ffd59945e9a56bb536b1b50ab5b2",
        "text": "In our work we will input images of textures (possibly of high pixel resolution) as the data distribution\nthat the GAN must learn. However, we will modify the DCGAN architecture [13] to allow for\nscalability and the ability to create any desired output texture size by employing a purely convolutional\narchitecture without any fully connected layers. The SGAN architecture is especially well suited for\ntexture synthesis of arbitrary large size, a novel application of adversarial methods.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                505.0273742675781,
                504.17041015625,
                558.550048828125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "21092fde32239d8052b71404a6a4f36a",
        "text": "In the experiments in Section 4 we will examine these points in detail. In the next section we describe\nthe spatial GAN architecture.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                565.0523681640625,
                504.0014953613281,
                585.8480224609375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8f870ab3b4fbea6b7c5d8605a6d10f1c",
        "text": "3\nThe SGAN Model",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                607.441162109375,
                217.59332275390625,
                619.3963623046875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d90b99c444fb59c8e241339f8ac80afc",
        "text": "The key idea behind Generative Adversarial Networks [7] is to simultaneously learn a generator\nnetwork G and a discriminator network D. The task of G(z) is to map a randomly sampled vector\nz ∈Rd from a prior distribution pz(z) to a sample X ∈Rh×w×3 in the image data space. The\ndiscriminator D(X) outputs a scalar representing the probability that X is from real training data and\nnot from the generator G. Learning is motivated from game theory: the generator G tries to fool the\ndiscriminator into classifying generated data as real one, while the discriminator tries to discriminate\nreal from generated data. As both G and D adapt over time, G generates data that gets close to the\ninput data distribution.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                634.96923828125,
                504.173095703125,
                722.4070434570312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "86991499836ce8f9cfd6b9a38807a191",
        "text": "1",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                436.6585693359375,
                107.67439270019531,
                440.04217529296875,
                113.8617172241211
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "99ba61068dadc96d47f640d4a53cb788",
        "text": "h",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                316.6754150390625,
                111.40936279296875,
                320.0430603027344,
                117.59668731689453
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5329f2cb0eb68a8e5e4d1eb85a7a93bd",
        "text": "m",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                451.713623046875,
                117.52053833007812,
                456.89544677734375,
                123.7078628540039
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "32375ae94730f8bb4d557ebfa0946132",
        "text": "l",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                458.9976806640625,
                141.45465087890625,
                460.47137451171875,
                147.6419677734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1875e9e1cd4fe1b40819de206d351567",
        "text": "w\n3",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                323.2098388671875,
                176.35133361816406,
                331.6364440917969,
                188.89633178710938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "db06c04659ddf9d6db5d725227c7b824",
        "text": "3",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                344.5790100097656,
                189.7842559814453,
                347.9626159667969,
                195.97157287597656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ae4037822f3ef1a61f95e9c2db6c9e06",
        "text": "1",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                436.5462646484375,
                217.7423553466797,
                439.92987060546875,
                223.92967224121094
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "87f54baa6f68d037b5fab078daca94ad",
        "text": "h",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                316.34539794921875,
                221.62086486816406,
                319.7130432128906,
                227.8081817626953
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "62dc7d5831f86db02c99a79e22eafb00",
        "text": "m",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                451.601318359375,
                227.58848571777344,
                456.78314208984375,
                233.7758026123047
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5a45a5438e28b04e2301f647164d82c7",
        "text": "l",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                458.8853759765625,
                251.52259826660156,
                460.35906982421875,
                257.7099304199219
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "196a026a884ca598a966f6acf293c85d",
        "text": "w",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 3,
            "languages": [
                "eng"
            ],
            "coordinates": [
                326.06658935546875,
                285.50457763671875,
                330.41314697265625,
                291.6919250488281
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "650e0d031a1fea896ec1c5c730f16a31",
        "text": "The SGAN generalizes the generator G(Z) to map a tensor Z ∈Rl×m×d to an image X ∈Rh×w×3,\nsee Figure 2. We call l and m the spatial dimensions and d the number of channels. Like in GANs,\nZ is sampled from a (simple) prior distribution: Z ∼pZ(Z). We restricted our experiments to\nhaving each slice of Z at position λ and µ, i.e. zλµ ∈Rd, independently sampled from pz(z), where\nλ, µ ∈N with 1 ≤λ ≤l and 1 ≤µ ≤m. Note that the architecture of the GAN puts a constraint on\nthe dimensions l, m, h, w – if we have a network with k convolution layers with stride 1",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                70.86739349365234,
                505.2480773925781,
                139.9690704345703
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8b38d5614489af657a132be1115ff026",
        "text": "2 and same\nzero padding (see e.g. [1]) then h",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                129.93202209472656,
                504.00372314453125,
                153.14907836914062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "de48de6b4b8c49968b5555ec66b261a9",
        "text": "l = w",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                236.95199584960938,
                141.36878967285156,
                262.5573425292969,
                156.353515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8bc790510d2104b478ec13d4706d6fa7",
        "text": "m = 2k.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                256.2380065917969,
                141.6757354736328,
                290.1606750488281,
                156.353515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0ff6af1439f10bd9587089b12f8016db",
        "text": "Similarly to the way we extended the generator, the discriminator D maps to a two-dimensional ﬁeld\nof size l × m containing probabilities that indicate if an input X (or X′) is real or generated. In order\nto apply the SGAN to a target texture I, we need to use I to deﬁne the true data distribution pdata. To\nthis end we extract rectangular patches X′ ∈Rh×w×3 from the image I at random positions. We\nchose X′ to be of the same size as the samples of the generator X = G(Z) - otherwise GAN training\nfailed in our experiments. For the same reason we chose symmetric architectures for G and D, i.e.\nD(G(Z)) has the same spatial dimensions as Z.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                159.3439178466797,
                505.7387390136719,
                234.99209594726562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1c99f615cbb150d003121495f44aae3c",
        "text": "Both the generator G(Z) and the discriminator D(X) are derived from the architecture of [13]. In\ncontrast to the original architecture however, spatial GANs forgo any fully connected layers - the\nnetworks are purely convolutional. This allows for manipulation of the spatial dimensions (i.e. l and\nm) without any changes in the weights. Hence, a network trained to generate small images is able to\ngenerate a much larger image during deployment, which matches the local statistics of the training\ndata.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                241.1869354248047,
                504.00177001953125,
                305.92608642578125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a839e63aa6a83bd0a52a5c868552df05",
        "text": "We optimize the discriminator (and the generator) simultaneously over all spatial dimensions:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.53199768066406,
                312.3514709472656,
                480.8205871582031,
                322.3140869140625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c7c64e3bbaae5647b06c74ffbe6dd10d",
        "text": "l\nX",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                272.0039978027344,
                342.07476806640625,
                286.39996337890625,
                359.8539733886719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9de693fae04a3747b2b0f46ef143696b",
        "text": "m\nX",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                288.5370178222656,
                342.07476806640625,
                302.9329833984375,
                359.8539733886719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "56c2955f6fcc19479f942a257ce0aff8",
        "text": "min\nG max\nD V (D, G) = 1",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                170.28298950195312,
                345.45294189453125,
                265.4703063964844,
                368.5815124511719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6eca250e867c2f9d01d18a315376a884",
        "text": "µ=1\nEZ∼pZ(Z) [log (1 −Dλµ(G(Z)))]",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                288.2660217285156,
                350.0164489746094,
                441.72296142578125,
                373.1365966796875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "27e6fb1345981b2372868b7011aed67a",
        "text": "lm",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                257.02099609375,
                359.02593994140625,
                268.9362487792969,
                368.988525390625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3d978515d7cf9fccceba828aff23dd89",
        "text": "λ=1",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                271.7929992675781,
                366.5107727050781,
                286.610107421875,
                373.4845886230469
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6f9703764b884b11808150431340dafc",
        "text": "l\nX",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                272.0039978027344,
                378.3387451171875,
                286.39996337890625,
                396.1179504394531
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c8e14059e2bb7f5c4f394cadbabc7d50",
        "text": "m\nX",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                288.5370178222656,
                378.3387451171875,
                302.9329833984375,
                396.1179504394531
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "36b682507be81b72d79bb28a46461655",
        "text": "+ 1",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                248.07699584960938,
                381.71697998046875,
                265.47027587890625,
                398.4195556640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c55d9ab2fc1d3b627748539425823b23",
        "text": "µ=1\nEX′∼pdata(X) [log Dλµ(X′)]\n(1)",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                288.2660217285156,
                386.2804260253906,
                504.0003967285156,
                409.40057373046875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c40452fb33b0007b4d4751e6bfc30a9a",
        "text": "lm",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                257.02099609375,
                395.2909240722656,
                268.9362487792969,
                405.2535095214844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c1ad04ae0054622800f3113c135bfdfe",
        "text": "λ=1",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                271.7929992675781,
                402.7757568359375,
                286.610107421875,
                409.74957275390625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4ebcb53e25c380e58eab6fdae653aa7c",
        "text": "In this formula the ﬁrst row corresponds to Figure 2(a), and the second row to Figure 2(b). In practice,\nit is helpful to apply the trick of [7] and minimize −log(D(G(Z))) instead of log(1 −D(G(Z))).",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                420.3744201660156,
                505.2413330078125,
                441.1700744628906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c852a2811da3ce7a80d675e586bb4fee",
        "text": "Note that the model describes a stochastic process over the image space. In particular, as the generator\nG is purely convolutional and each zλµ is identically distributed and independent of its location λ\nand µ, the generated data is translation-invariant. Hence the process is stationary with respect to\ntranslations.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                447.6714172363281,
                504.1674499511719,
                490.2850646972656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3db1c0458ae3dbdfd4f16b9ab0404211",
        "text": "To show that the process is also strong mixing, we ﬁrst need to deﬁne the projective ﬁeld (PF) of\na spatial patch ˆZ of Z as the smallest patch ˆX of the image X which contains all affected pixels\nof X = G(Z) under all possible changes of ˆZ. In full analogy, we refer to the receptive ﬁeld\n(RF) of a patch in D(X) as the corresponding minimal patch in X which affects it. Assume then\ntwo non-overlapping patches from the generated data, A and B. Additionally, take their respective\nprojective ﬁelds ZA, ZB to be non-overlapping - this can be always achieved as projective ﬁelds\nare ﬁnite, but the array Z can be made arbitrarily large. The generated data in A and B is then\nindependently generated. The process is hence strong mixing (with the length scale of the projective\nﬁeld), which implies it is also ergodic.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.6709976196289,
                496.63629150390625,
                504.003662109375,
                597.3080444335938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "70bc0c91731c057453d26b6126aff766",
        "text": "4\nExperiments",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99998474121094,
                613.6551513671875,
                191.0168914794922,
                625.6103515625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c3cfc8fcba72f4286c6dff917c4cdeb8",
        "text": "4.1\nArchitectural details and speed",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99998474121094,
                637.83251953125,
                262.7889404296875,
                647.7951049804688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d70a18d6711bebb9d2f926ceb82feb29",
        "text": "For the following experiments, we used an architecture close to the DCGAN setup [13]: convolutional\nlayers with stride 1",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                657.8994750976562,
                503.9955139160156,
                678.7794799804688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "70f46778d88cc578018d13f14c0d2f71",
        "text": "2 in the generator, convolutional layers with stride 2 in the discriminator, kernels\nof size 5x5 and zero padding. We used a uniform distribution for pz(z) with support in [−1, 1].\nDepending on the size and structure of the texture to be learned, we used networks of different\ncomplexity, see Table 1. We used networks with identical depths in G and D. The sizes of ﬁlter\nbanks of D were chosen to be in reverse to those of G, yielding more channels for smaller spatial",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 4,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                668.7821044921875,
                505.74346923828125,
                722.4310302734375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8a487a95a49d568d5c4017cfe478887b",
        "text": "SGAN4\nSGAN5\nSGAN6\nZ channel number d\n20\n50\n100\nr = h/l = w/m =\n16\n32\n64\nPF/RF size\n61\n125\n253\ngenerator G ﬁlters\n256-128-64\n512-256-128-64\n1024-512-256-128-64\ndiscriminator D ﬁlters\n64-128-256\n64-128-256-512\n64-128-256-512-1024\nTable 1: Details of the SGAN architecture with 4,5 or 6 layers. The calculation of the projective and\nreceptive ﬁeld (PF and RF) sizes is examined in detail in Appendix I.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                74.38844299316406,
                503.99493408203125,
                177.67410278320312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "000aae652d7a2c87729119274df938fe",
        "text": "SGAN4\nSGAN5\nTextureNet\nGatys\n256x256 px\n.005s\n.006s\n0.020s\n10s\n512x512 px\n.013s\n.019s\n-\n-\n1024x1024 px\n.047s\n.07s\n-\n-\n2048x2048 px\n.178s\n.269s\n-\n-\nTable 2: Time required for generating an output texture of certain size. The SGAN architecture is\nfaster than both TextureNet [16] and Gatys [5]. The time costs per calculated pixel scale sublinearly.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                198.59144592285156,
                505.6678161621094,
                287.8420715332031
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2d4802877e6ec40b3e05b3c9ff7b5e15",
        "text": "representations. We denote with SGANx that we have x layers depth in G and D. We applied batch\nnormalization on all layers, except the output layer of G, the input and output layers of D. All\nnetwork weights were initialized as 0-mean Gaussians with σ = 0.02.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                316.366943359375,
                503.9986877441406,
                348.3780822753906
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e41cce1e5aa396ac807a16058a77a9e9",
        "text": "We tried different sizes for the image patches X. Note that the spatial dimensions of Z and X are\ndependent, h = rl and w = rm. Both setting h = w = 640 or l = m = 4 and adjusting for the\nrespective depending variables worked similarly well, despite different relative impact of the zero\npadded boundaries.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.53199768066406,
                354.57293701171875,
                504.00103759765625,
                397.49407958984375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "aa059c8068de9d02c0c56d9dba0704e2",
        "text": "Our code was implemented in Theano and tested on an Nvidia Tesla K80 GPU. The texture gener-\nation speeds of a trained SGAN with different architectures and image sizes are shown in Table 2.\nGeneration with G is very fast, as is expected for a single forward pass. Forward pass generation\nin TextureNet [16] is signiﬁcantly slower (20ms, according to their publication) than SGAN (5ms)\nfor the 256 pixel resolution, despite the fact that TextureNet uses fewer ﬁlters (8 to 40 channels per\nconvolution layer) than we do (64 to 512 ﬁlters). The simpler SGAN architecture avoids the multiple\nscales and join operations of TextureNet, rendering it more computationally efﬁcient. As expected,\nthe method of Gatys [5] is orders of magnitude slower due to the iterative optimization required.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                403.8592834472656,
                505.7453308105469,
                490.2460632324219
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "61e0226b7b488ac93d046ce9fec06936",
        "text": "There are initial time costs for training the SGAN on the target textures. For optimization we used\nADAM [10] with parameters as in [13] and 32 samples per minibatch. For the simple textures from\nSection 4.2.1, subjective assessment indicates that generated images are close to their ﬁnal quality\nafter roughly 10 minutes of training. The more complex textures of Sections 4.2.2 required around\n30 minutes of training.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                496.6150207519531,
                504.3522033691406,
                550.2710571289062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9aae1d497f53c6959923265249a18978",
        "text": "Training times TextureNet [16] requires a few hours per texture. We could not compare this directly\nwith SGAN on the same machine and textures, but we assume that SGAN trains more efﬁciently than\nTextureNet because of its simpler architecture.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                556.6964721679688,
                504.3504333496094,
                588.47705078125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ad82c3648b98f645bc56f47d42eb842a",
        "text": "The exact time and number of iterations required for training SGANs depends on the structure of the\ntarget texture. A general problem in GAN training is that it is often required to monitor the results\nand stop training – as occasionally overtraining may lead to degeneracy or image quality degradation.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                594.9526977539062,
                505.5588684082031,
                626.6610717773438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a0687bb287936ae594f60ec56e00bdab",
        "text": "4.2\nExamples of generated textures",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                641.021484375,
                263.8748474121094,
                650.9840698242188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c0d21300d3514c526255f4c54374e510",
        "text": "4.2.1\nSingle small image",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                660.9965209960938,
                217.04063415527344,
                670.9591064453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0b6816ed8d6f76a15ecbd5a2401d864d",
        "text": "A common way to measure quality of texture synthesis is to visually evaluate the generated samples.\nA setup with small input textures allows a direct comparison between SGAN and the method of\nGatys [5]. For these examples, we used an SGAN with 4 layers. Figure 3 shows our results for\ntextures coming from a stationary and ergodic process. The textures of radishes and stones (top rows)",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 5,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                679.7477416992188,
                505.74346923828125,
                722.3828735351562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6365652a982cb6db41259ce12bbd7d1a",
        "text": "Input\nSGAN4\nGatys et al.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                200.90098571777344,
                349.4289855957031,
                424.4708251953125,
                358.3953857421875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "46f6178510eccb0b716eb03c60f3766e",
        "text": "Figure 3: Texture synthesis of small texture images of different size - the results of SGAN are\ncomparable to Gatys [5]. In the top left corner of the images we indicate the receptive ﬁeld of size 61\npixels of SGAN4 to visually illustrate their size relative to the texture image sizes.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99996948242188,
                370.24530029296875,
                504.7425537109375,
                402.10107421875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "3a342e366b641e3bc88e80503e35f513",
        "text": "are also mixing, while the letters (bottom row) are mixing horizontally, but not vertically. The texture\nsynthesis of both SGAN and Gatys fail to preserve the row regularity of the source image.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99996948242188,
                425.09442138671875,
                504.00140380859375,
                445.89007568359375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "774e456fef0c8e69f5f5201768061b53",
        "text": "4.2.2\nSingle large image",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99996948242188,
                459.8895263671875,
                215.82516479492188,
                469.85211181640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "23e1283da49a6b1f2741727c60cc6259",
        "text": "Satellite images provide interesting examples of macroscopic structures with texture-like properties.\nCity blocks in particular resemble realizations from a stationary stochastic process: different city\nblocks have similar visual properties (color, size). Mixing occurs on a characteristic length scale\ngiven by the major streets.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99996948242188,
                478.997802734375,
                505.74566650390625,
                521.6770629882812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e1f0f5677fc63b667474bf6998ab3454",
        "text": "Figure 1 shows that our method works better than [5] on satellite images, here concretely a single\nimage of Barcelona.2 SGAN creates a city-like structure, whereas Gatys’ method generates less\nstructure and detail. We interpret this in the following way: the SGAN is trained speciﬁcally on the\ninput image and utilizes all its model power to learn the statistics of this particular texture. Gatys [5]\nrelies on pretrained ﬁlters learned on the ImageNet dataset, which generalize well to a large set of\nimages (but apparently not well to satellite imagery) and fails to model salient features of this city\nimage, in particular the street grid orientation.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99996948242188,
                528.0272216796875,
                504.3504943847656,
                603.5200805664062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a19285408adde1cfd16f144f6fc4c47b",
        "text": "To indicate the superior quality of SGAN for that texture the spatial auto-correlation (AC) of the\noriginal and synthesized textures from Figure 1 are shown on Figure 4. We calculate the AC on whole\nimages of size 1371x647 pixels. The AC of the original and the SGAN5 generation are similar to one\nanother and show clearly the directions of the street grid. In contrast, the AC of Gatys’ texture looks\nmore isotropic than the original, indicating loss of visually important information.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69097137451172,
                609.8702392578125,
                504.0035400390625,
                663.5450439453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c580a90799a0855b2070cf30257fee84",
        "text": "Figure 5 illustrates the effects of different network depths on the SGAN generated outputs. More\nlayers and larger receptive ﬁelds as in SGAN6 allow larger structures to be learned and longer streets\nemerge, i.e., there is less mixing and more regularity at a given scale.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99996948242188,
                669.895263671875,
                503.997802734375,
                701.7510375976562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "42f6c5cf16e810fdf5475f462acb383c",
        "text": "2www.google.com/maps/@41.4033724,2.1551133,292m/data=!3m1!1e3",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 6,
            "languages": [
                "eng"
            ],
            "coordinates": [
                120.65296936035156,
                711.6576538085938,
                407.55926513671875,
                722.3895874023438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0030960aa5356aceb6d755692b6d87d7",
        "text": "Input\nSGAN5\nGatys et al.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                166.60899353027344,
                143.45297241210938,
                459.5098571777344,
                152.41937255859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f42162925895a98b0abc53021ed5e8b5",
        "text": "Figure 4: Spatial autocorrelation (AC) of the Barcelona city example from Figure 1: the preferred\ndirections of the city streets are clearly visible in the centre of the AC of the input texture. The AC of\nthe SGAN texture reﬂects the structure much better than the result of Gatys et al.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                164.26931762695312,
                503.9974670410156,
                196.12509155273438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "b188394c14f37bb524bf0c04d4aba7ff",
        "text": "SGAN4\nSGAN5\nSGAN6",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                159.79400634765625,
                350.0599670410156,
                454.37176513671875,
                359.0263671875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "41d2facef43ccec9cd619c80daae8d05",
        "text": "Figure 5: SGAN with G, D layers of depth 4,5,6 changes the resulting output texture (cropped to\nsize 647x647 pixels). Bigger models exhibit less mixing and generate longer streets.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                368.9229431152344,
                504.0028076171875,
                390.02508544921875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "dae7e229f67f0928571252022cbe6afa",
        "text": "4.2.3\nComposite textures from multiple images",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                416.99951171875,
                312.9505920410156,
                426.96209716796875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "bdb67ad62983fdf515d207537808c4b0",
        "text": "The GAN approach to texture synthesis can combine multiple example texture images in a natural\nway. We experimented with the ﬂowers dataset3 containing 8189 images of various ﬂowers, see\nFigure 6 (a) for examples. We resized each image to 160 pixels in the h-dimension, while rescaling\nthe w-dimension to preserve the aspect ratio of the original image. Then we trained an SGAN with 5\nlayers; each minibatch for training contained 128x128 pixel patches extracted at random positions\nfrom randomly selected input ﬂower images. This is an example of a non-ergodic stochastic process\nsince the input images are quite different from one another.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                437.9032897949219,
                504.00311279296875,
                513.3810424804688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cc85b891c0f18ccdd5732ee148e01669",
        "text": "A sample from the generated composite texture is shown in Figure 6 (b). The algorithm generates a\nvariety of natural looking ﬂowers but cannot blend them smoothly since it was trained on a dataset of\nsingle ﬂower images. Still, the ﬁnal result looks aesthetic and such fusion of large image datasets for\ntexture learning has great potential for photo mosaic applications.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                519.8026733398438,
                504.1651916503906,
                562.4960327148438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "16f5df8d386e825be6b07eacd255a1f4",
        "text": "In another experiment we learned a texture representing the 5 satellite images shown on Figure 6 (c).\nThe input images depict areas in the Old City of Amsterdam with different prevailing orientations.\nFigure 6 (d) demonstrates how the generated texture has orientations from all 5 inputs. Although we\ndid not use any data augmentation for training, the angled segments join smoothly and the model\nlearns spatial transitions between the different input textures. Overall, the Amsterdam city segments\ncome from a more ergodic process than the ﬂowers example, but less ergodic than the Barcelona\nexample.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                568.9603881835938,
                505.7452697753906,
                644.3390502929688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0a2846f44ad4bd0759be685627dd2b7b",
        "text": "GAN based methods can fuse several images naturally, as they are generative models that capture\nthe statistics of the image patches they’re trained with. In contrast, methods with speciﬁed image\nstatistical descriptors [12, 5] generate textures that match a single target image closely in these\ndescriptors. Extending these methods to several images is not straight-forward.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                650.6902465820312,
                504.0030822753906,
                693.4550170898438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2054e3eefcdc23f69db660edb516cefb",
        "text": "3www.robots.ox.ac.uk/~vgg/data/flowers/",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 7,
            "languages": [
                "eng"
            ],
            "coordinates": [
                120.65299987792969,
                711.6576538085938,
                303.00994873046875,
                722.3895874023438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "ef7fe4d43c30f5dfadd6cfe229412bb0",
        "text": "a) Input\nb) SGAN5\nc) Input\nd) SGAN6",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                133.13600158691406,
                208.5670928955078,
                451.92578125,
                217.6153564453125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7667edfee62a0bb42f36fda951663b2f",
        "text": "Figure 6: Learning textures from multiple images. (a) and (c) show the training data textures rescaled\nto ﬁt the screen. (b) and (d) show examples of generated 1000x1000 pixel composite textures.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                229.6173858642578,
                504.0013732910156,
                250.41305541992188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1ed85d56c0e0345cf82b33b8cc211ba1",
        "text": "4.3\nExtension using properties of the spatial GAN",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                272.6365051269531,
                325.623046875,
                282.5990905761719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "38fae0e48ec1a809f01841d785fb70a6",
        "text": "The spatial dimensions of Z are locally independent – output image pixels depend only on a subset of\nthe input noise tensor Z. This property of the SGAN allows two practical tricks for creation of output\ntextures with special properties. Below we illustrate these tricks brieﬂy, see Appendix I for details.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                292.5769348144531,
                504.00408935546875,
                324.58807373046875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "55583d86234d0258ee2abe2c798548d7",
        "text": "4.3.1\nSeamless textures",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                337.9095153808594,
                212.68702697753906,
                347.8721008300781
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4051134e7be55b2a35dd4ff2f43642d9",
        "text": "Seamless textures are important in computer graphics, because arbitrarily large surfaces can be\ncovered by them. Suppose we want to synthesize a seamless texture of desired size ˆh, ˆw and generating\nit would require ˆl, ˆm spatial dimensions of Z for a given SGAN model. Let r = ˆh/ˆl = ˆw/ ˆm be the\nratio of the spatial dimensions of G(Z) to Z. For notation we use Python slicing notation, where\nZ[−i,4:] indicates i indices before the end of the array in the 1st dimension, the ‘4:’ indicates all\nelements but the ﬁrst 4 along the second dimension, and all elements along the last, not explicitly\nindexed, dimension. We should sample a slightly bigger noise tensor Z ∈R(ˆl+4)×( ˆm+4)×d and set\nits edges to repeat: Z[:,−4:] = Z[:,:4] and Z[−4:,:] = Z[:4,:]. Then we can calculate G(Z) and crop 2r\npixels from each border, resulting in an image I = G(Z)[2r:−2r,2r:−2r] of size ˆh, ˆw that can be tiled\nin a rectangular grid as shown on Figure 7.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                356.6602783203125,
                504.34283447265625,
                474.1830749511719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "82bb6666d91b221cd0a2bae819a1d3a0",
        "text": "4.3.2\nMemory efﬁcient generation",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                487.5045166015625,
                258.51495361328125,
                497.46710205078125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7f33c55178102d434bd3fb146c9cecec",
        "text": "In addition, we can use the SGAN generator to create textures in a memory efﬁcient way by splitting\nthe calculation of I = G(Z) into independent chunks that use less memory. This approach allows for\nstraightforward parallelization. A potential application would be real-time 3D engines, where the\nSGANs could produce the currently visible parts of arbitrary large textures.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                506.3873291015625,
                504.1653137207031,
                549.0200805664062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "58a8f1cb2443dfa5ed8f35ee6e32275c",
        "text": "Suppose again that we have Z ∈Rˆl× ˆm×d. Let us split Z in two along dimension ˆl. We can call\nthe generator twice, producing I1 = G(Z[:ˆl/2+2]) and I2 = G(Z[ˆl/2−2:]), with each call using\napproximately half the memory than a call to the whole G(Z). To create the desired large output, we\nconcatenate the two partially generated images, cropping their edges: I = [I1\n[:−2r], I2\n[2r:]]. With this\napproach, the only limitation is the number of pixels that can be stored, while the memory footprint\nin the GPU is constant. Appendix I has precise analysis of the procedure.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                554.6377563476562,
                504.0032043457031,
                626.8450317382812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "5225b7649d05477e9c63aee0c06297a9",
        "text": "5\nDiscussion",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                643.9171752929688,
                179.7431640625,
                655.8723754882812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a6ce31111ab0297908ab3fe04983e4f0",
        "text": "Our SGAN synthesizes textures by learning to generate locally consistent image patches, thus making\nuse of the repeating structures present in most textures. The mixing length scale of the generation\ndepends on the projective ﬁeld sizes. Choosing the best architecture depends on the speciﬁc texture\nimage. This holds also for the algorithm of Gatys et al. [5], where the parametric expressiveness of\nthe model depends on the set of network layers used for the descriptive statistics calculation.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 8,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                668.8843994140625,
                504.0030517578125,
                722.4070434570312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "76955813cc4f4f9d10171eb72d2ce5e5",
        "text": "Figure 7: A 320x320 pixels texture, tiled 2 times vertically and 6 times horizontally. It is straight-\nforward to create seamless textures in the SGAN framework by enforcing periodic boundary condi-\ntions on Z.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                209.81130981445312,
                505.65374755859375,
                241.66708374023438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a1cf59bc934226324e21fa3be2abe7d1",
        "text": "Rather than using handcrafted features or other priors to specify the desired properties of the output,\nthe adversarial framework learns the relevant statistics only from information contained in the training\ntexture. In contrast, parametric methods specify statistical descriptors a priori, before seeing the\nimage I. This includes models using the wavelet transform [12] or the properties of the ﬁlters of a\nneural network trained on a large image dataset [5]. Such models can generalize to many textures,\nbut are not universal – sometimes it is better to train a generative model for a single texture, as our\nexamples from Section 4.2.2 show.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                262.8223876953125,
                505.24658203125,
                338.2200622558594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d0f6e0ba27d890515acaa85eceb0a032",
        "text": "[16] is an interesting case because it describes a generative model that takes as inputs noise vectors\nand produces images with desired statistics. This is analogous to the generator G in the GAN.\nHowever, features extracted from pre-trained discriminative networks (as in [5]) play the role of a\ndiscriminator function, in contrast to learned discriminators in adversarial frameworks.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                344.62384033203125,
                505.74737548828125,
                387.3360595703125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "276276603784a3a24f0b498167f47210",
        "text": "The examples in Sections 4.2.1 and 4.2.2 show that our method deals well with texture images,\ncorresponding to realizations of stationary ergodic stochastic processes that mix. It is not possible to\nlearn statistical dependencies that exceed the projective ﬁeld size with SGANs.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                393.6872863769531,
                505.2431640625,
                425.5430603027344
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2604c3cdfc5fa9b16ece756f85a39676",
        "text": "Synthesizing regular non-mixing textures (e.g. a chess grid pattern) is problematic for some meth-\nods [11]. The SGAN cannot learn such cases - distant pixels in the output image are independent from\none another because of the strong mixing property of the SGAN generation, as discussed in Section\n3. For example, the model learns basic letter shapes from the text image in Figure 3 (bottom row),\nbut fails to align the \"letters\" into globally straight rows of text. The same example shows that the\napproach of Gatys [5] has similar problems synthesizing regular textures. In contrast, the parametric\nmethod of [12] and non-parametric instance-based methods work well with regular patterns.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                431.9017639160156,
                505.6548767089844,
                507.3860778808594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "17e5ceade91bc9f380316688a898a203",
        "text": "To summarize the capabilities of our method:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                513.8125,
                288.4125671386719,
                523.7750854492188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e78f2f16fdf155fa9cdbe86f3bb13500",
        "text": "• real-time generation of high quality textures with a single forward pass",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                133.9029998779297,
                534.3817749023438,
                426.3952941894531,
                544.7040405273438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f17665e2062d19c4f75f1287e470627e",
        "text": "• generation of images of any desired size",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                133.9029998779297,
                550.3867797851562,
                304.18414306640625,
                560.7090454101562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "d854c85102a97fa3ee325797a378ae79",
        "text": "• processing time requirements scale linearly with the number of output pixels",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                133.9029998779297,
                566.3927612304688,
                449.19970703125,
                576.7150268554688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "37766cfa65d00fb32e47aad276466e1f",
        "text": "• combination of separate source images into complex textures",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                133.9029998779297,
                582.3977661132812,
                387.3320007324219,
                592.7200317382812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "42aeed52201abb423ef4ce51ffe09e57",
        "text": "• seamless texture tiles",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                133.9029998779297,
                598.40380859375,
                228.38876342773438,
                608.72607421875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1b266eb17dea3ca1ee85ffa4acdb7305",
        "text": "As next steps, we would like to examine modiﬁcations allowing the learning of images with longer\nspatial correlations and strong regularity. Conditional GAN [13] architectures can help with that, as\nwell as allow for more precise control over the generated content. Conditioning can be used to train a\nsingle network for a variety of textures simultaneously, and then blend them in novel textures.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                619.6622924804688,
                504.17095947265625,
                662.3820190429688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "828893aeda5bb6c76d8f4ce68c0074ee",
        "text": "In future work, we plan to examine further applications of the SGAN such as style transfer, mosaic\nrendering, image completion, in-painting and modeling 3D data. In particular, 3D data offers\nintriguing possibilities in combination with texture modeling – e.g. in machine generated fashion\ndesigns. The SGAN approach can also be applied to audio data represented as time series, and we\ncan experiment with novel methods for audio waveform generation.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 9,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                668.7858276367188,
                504.0041809082031,
                722.4070434570312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "67997ce78bdbec0051f068d97de6509f",
        "text": "Acknowledgements",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                72.78716278076172,
                206.83363342285156,
                84.74236297607422
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "13c22f56ad40cb5309cc0d1694f33ae3",
        "text": "We would like to thank Christian Bracher, Sebastian Heinz and Calvin Seward for their valuable\nfeedback on the manuscript.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.53199768066406,
                97.44528198242188,
                503.9968566894531,
                118.39205932617188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "e74291838ad83e795e14aeda3edc2b2b",
        "text": "References",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                135.203125,
                163.54383850097656,
                147.1583251953125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4d340bfcaab1751a115277c6f30e7042",
        "text": "[1] Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning.\narXiv:1603.07285, 2016.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98100280761719,
                154.39622497558594,
                505.7428894042969,
                175.32809448242188
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2b72a8cc200da222705a766148ba54cd",
        "text": "[2] Alexei A. Efros and William T. Freeman. Image quilting for texture synthesis and transfer. In\nProceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques,\nSIGGRAPH, 2001.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98098754882812,
                184.23040771484375,
                505.2457580566406,
                216.02609252929688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "a161c33ee7a097be9b9d930d306b4f0c",
        "text": "[3] Alexei A. Efros and Thomas K. Leung. Texture synthesis by non-parametric sampling. In\nProceedings of the International Conference on Computer Vision, 1999.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98099517822266,
                224.86727905273438,
                504.0027770996094,
                245.81405639648438
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "02fcfcc000282bb4d361179296a0bbc3",
        "text": "[4] Arthur Szlam Emily Denton, Soumith Chintala and Rob Fergus. Deep generative image\nmodels using a Laplacian pyramid of adversarial networks. In Advances in Neural Information\nProcessing Systems 28, 2015.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98098754882812,
                254.65530395507812,
                504.0027770996094,
                286.5110778808594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "538b5509d8feb65184338792903e4f43",
        "text": "[5] Leon Gatys, Alexander Ecker, and Matthias Bethge. Texture synthesis using convolutional\nneural networks. In Advances in Neural Information Processing Systems 28, 2015.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98100280761719,
                295.3533020019531,
                504.002685546875,
                316.3000793457031
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "01a61987fb779d9ffb7239312a57e691",
        "text": "[6] G. Georgiadis, A. Chiuso, and S. Soatto. Texture compression. In Data Compression Conference,\nMarch 2013.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98098754882812,
                325.1177062988281,
                505.24481201171875,
                346.08807373046875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "9ef4882140a1d383c146620bd931ae0b",
        "text": "[7] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems 27, 2014.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98099517822266,
                354.9405212402344,
                503.99627685546875,
                386.7850646972656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "01e60c10b64a4678527c2aa5a2ef11f5",
        "text": "[8] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. In Proceedings of the 32nd International Conference on\nMachine Learning, 2015.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98100280761719,
                395.6986999511719,
                504.0042419433594,
                427.4830627441406
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "019e70e5dc8ce851decbaefc98a7ddf1",
        "text": "[9] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer\nand super-resolution. In European Conference on Computer Vision, 2016.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                112.98099517822266,
                436.3768310546875,
                504.1653747558594,
                457.2710876464844
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "fb51bb73a524099792ba6c993c78f74b",
        "text": "[10] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,\nabs/1412.6980, 2014.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                465.93408203125,
                505.2454528808594,
                487.05908203125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8eac1648d0e5184a392dbe5ce25c6ea1",
        "text": "[11] Yanxi Liu, Wen-Chieh Lin, and James Hays. Near-regular texture analysis and manipulation. In\nACM SIGGRAPH Papers, 2004.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99999237060547,
                495.9764709472656,
                504.0034484863281,
                516.8480834960938
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "f9c83085ead61af6a26f011e2181c378",
        "text": "[12] Javier Portilla and Eero P. Simoncelli. A parametric texture model based on joint statistics of\ncomplex wavelet coefﬁcients. Int. J. Comput. Vision, 40(1), October 2000.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                525.7192993164062,
                503.99969482421875,
                546.6360473632812
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c66fc2cc9923bb3705c21ce4545b199c",
        "text": "[13] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with\ndeep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99998474121094,
                555.4960327148438,
                503.997802734375,
                576.4240112304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "cba5b9ee08a81206f1eaac6c631defbe",
        "text": "[14] Jost T. Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for\nsimplicity: The all convolutional net. In ICLR (workshop track), 2015.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99996948242188,
                585.29150390625,
                504.16790771484375,
                606.2130737304688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1d2b4d683b408a924025626914a2ef0d",
        "text": "[15] Lucas Theis and Matthias Bethge. Generative image modeling using spatial LSTMs. In\nAdvances in Neural Information Processing Systems 28, 2015.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                615.0542602539062,
                504.0026550292969,
                636.0010375976562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0d14955ab847e74a343e7d1aa1ce91ff",
        "text": "[16] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky. Texture networks:\nFeed-forward synthesis of textures and stylized images. In International Conference on Machine\nLearning, 2016.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.00001525878906,
                644.84228515625,
                505.3847961425781,
                676.6980590820312
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "df391e59075a5a4da5021b90e9a1c3a2",
        "text": "[17] Li-Yi Wie, Sylvain Lefebvre, Vivek Kwatra, and Greg Turk. State of the Art in Example-based\nTexture Synthesis. In M. Pauly and G. Greiner, editors, Eurographics 2009 - State of the Art\nReports. The Eurographics Association, 2009.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 10,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.00000762939453,
                685.6144409179688,
                504.0031433105469,
                717.3960571289062
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0ba8da843d8abedfde97482aef6aee2a",
        "text": "Appendix I",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                72.78716278076172,
                165.16976928710938,
                84.74236297607422
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6abc9a931f7b8fa72ad5bffc0b18353a",
        "text": "We examine in detail the projective ﬁelds (PF) and which inputs from Z map to which output pixels\nin G(Z). Let [a, b) indicate a range starting at index a inclusive and ending at index b exclusive,\nwhich we will also call left and right border. For convenience of notation, we will write only 1D\nindices for the square PFs, e.g. [0, 1) will refer to the square ﬁeld Z[:1,:1] in python slicing notation.\nFor simplicity, we express the formulas valid only for the architecture we usually used for SGAN,\n5x5 kernels and k convolutional layers with stride 1",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.53199768066406,
                97.28990173339844,
                505.7472839355469,
                162.02804565429688
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "8b8b8c4af5ed59da14a5fddf046033ca",
        "text": "We start by examining the recursive relation between input and output of a fractionally strided\nconvolutional layer.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.53199768066406,
                168.37930297851562,
                503.9967346191406,
                189.32608032226562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "728e69dc6af0705156e9341561f60ea2",
        "text": "Proposition 1 An input [a, b) has as its PF an output [a′, b′) after applying one convolutional layer.\nIt holds that a′ = 2a −2 and b′ = 2b + 1.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.83100128173828,
                199.13235473632812,
                505.7411804199219,
                221.42860412597656
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "68d8b6579d9f8eb0176f602cafa66b1b",
        "text": "This relation holds because of the way we implement a convolutional layer with stride 1",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                231.1947784423828,
                458.9120788574219,
                242.9762725830078
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "6f018449bc5de38034ab4893f07b33a6",
        "text": "2 in Theano,\njust like DCGAN [13] does. Note that b′ −a′ = 2(b −a) + 3 which is exactly relationship 13\nfrom [1] between input and output sizes of a transposed convolution.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                233.00868225097656,
                505.2475891113281,
                265.8050842285156
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "4466e9b23f841b8b908a5c26586579a6",
        "text": "We can rewrite the recursive relations as a function of the initial size and count of layers k:",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.53199768066406,
                272.00091552734375,
                469.32159423828125,
                282.1940612792969
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7986378d82b5d924056b5a2694a8da17",
        "text": "Proposition 2 An input [a, b) has as its PF an output [a′, b′) after k convolutional. layers. It holds\nthat a′ = a2k −2k+1 + 2 and b′ = b2k + 2k −1.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                292.0003967285156,
                504.00189208984375,
                315.33258056640625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "718a4e7fad674fda05a40c020202497e",
        "text": "In particular, we get the PF size of a single zλµ for b = a + 1 as PF(k) = 2k+2 −3, which we\ndenote in Table 1 as PF/RF.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                322.9418029785156,
                504.0041198730469,
                347.7880859375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "0cc637bc784a04633913aabad1c4fdd3",
        "text": "With these relations, we can show why we can split without any loss of information the calculation of\na big array Z in two smaller volumes as in Section 4.3.2.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.53199768066406,
                354.2904052734375,
                504.0020751953125,
                375.0860595703125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "59b765be63a9206c509e36fe15726b50",
        "text": "Proposition 3 Let G(Z) = I with Z ∈Rˆl× ˆm×d. We can deﬁne I1 = G(Z[:ˆl/2+2]) and I2 =\nG(Z[ˆl/2−2:]). Then it holds that I = [I1\n[:−2r], I2\n[2r:]] where r = 2k.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                384.19293212890625,
                504.0019226074219,
                414.819580078125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "1365bf0801aa3d8df722dd5b83853b91",
        "text": "Since we split only on one of the spatial dimensions, it is enough to reason only for intervals in that\ndimension and not the whole 2D ﬁeld. Image I1\n[:−2r] in the Python slicing notation is the same as",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.99993896484375,
                424.699462890625,
                504.003173828125,
                448.8040466308594
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c4ad7b781b75dd3e163338a0634d9471",
        "text": "image I1\n[:rˆl/2], and its rightmost pixel has index xr = 2kˆl/2 −1. The PF of Z[ˆl/2+2] has a left border",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                448.24493408203125,
                504.173095703125,
                464.8455810546875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c6e50ce70f79431a26c58b1732031610",
        "text": "(ˆl/2 + 2)2k −2k+1 + 2 = 2k−1ˆl + 2 > xr. The calculation of the pixel xr is not inﬂuenced by any\nfurther elements from Z, i.e. for any I1+δ = G(Z[:ˆl/2+2+δ]), δ ≥0 it holds that I1\n[:rˆl/2] = I1+δ\n[:rˆl/2].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                106.83399963378906,
                464.0788269042969,
                505.74346923828125,
                495.0395202636719
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "7eeda8045d915d98b7d81ca1f13ea3fc",
        "text": "This would mean that I1\n[:rˆl/2] is exactly equal to I[:rˆl/2], the left half of the desired image I. By a",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                494.83074951171875,
                504.0024108886719,
                510.9890441894531
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "14b76c16074a014f96c18bf96c2e92df",
        "text": "similar argument, the left-most pixel of I2\n[2r:] is not inﬂuenced by Z[:ˆl/2−2], and I2\n[2r:] is equal to\nI[rˆl/2:], the right half of the desired image I. This proves that I = [I1\n[:−2r], I2\n[2r:]].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                510.34173583984375,
                503.9957275390625,
                538.2835693359375
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "36690fc78af24b657f01f988ccd427db",
        "text": "This proof shows why an overlap of 2 spatial dimensions of Z is sufﬁcient for splitting. Is it also\nnecessary? The answer is yes, since Z[:ˆl/2+2] is the smallest set required to calculate I1: the PF",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.69100189208984,
                544.0009155273438,
                504.28216552734375,
                568.3790283203125
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "abe0985cf6d340f0f7b80ff059856b7e",
        "text": "of Z[ˆl/2+1] has left border (ˆl/2 + 1)2k −2k+1 + 2 = 2k−1ˆl −2k + 2 < xr and right border",
        "type": "Title",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                108.0,
                566.7047729492188,
                504.1700744628906,
                583.9180297851562
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "c78113e23cdf67a44569444aaeb1094e",
        "text": "(ˆl/2 + 2)2k + 2k −1 > xr, so the pixel xr is inside the PF of Z[ˆl/2+1].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                106.83399963378906,
                582.2437744140625,
                392.668701171875,
                598.549560546875
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "71e2b96aefcc8483d553b767663b8973",
        "text": "A similar proof can be made for the seamless, i.e. periodic, texture case from Section 4.3.1, and\nwe sketch it here. Making the Z periodic in its spacial dimensions makes the output I periodic as\nwell. As in the previous case, we need for each border an overlap of 2, hence we need to make 4\nelements along each dimension to be identical, i.e. we set Z[:,−4:] = Z[:,:4] and Z[−4:,:] = Z[:4,:].\nMore of the periodic structure in Z is not needed as it would be redundant. The output image is\nI = G(Z)[2r:−2r,2r:−2r]. It is easily shown that the leftmost and rightmost pixels I[0,0] and I[0,−1] ﬁt\ntogether as required for a seamless texture. These pixels use information from elements of Z that\nare equal numerically, Z[:4,:4] = Z[:4,−4:]. The relative positions of the pixel I[0,0] inside the PF of\nthe volume Z[:4,:4] is offset exactly by 1 pixel compared to the position of I[0,−1] inside the PF of\nZ[:4,−4:].",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                107.64099884033203,
                604.3682861328125,
                505.74346923828125,
                713.5035400390625
            ],
            "is_full_width": false
        }
    },
    {
        "element_id": "2353c049e9262a9d977a76f71faf985a",
        "text": "2.",
        "type": "NarrativeText",
        "metadata": {
            "source_doc": "1611.08207v4.pdf",
            "page_number": 11,
            "languages": [
                "eng"
            ],
            "coordinates": [
                310.4599914550781,
                152.0655059814453,
                318.11663818359375,
                164.58056640625
            ],
            "is_full_width": false
        }
    }
]